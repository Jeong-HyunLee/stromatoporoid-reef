{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeong-HyunLee/stromatoporoid-reef/blob/main/stromatoporoid_reef_size_Camb_Dev_Suppv16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiczYjViaSKp",
        "outputId": "357d3cf7-388d-416e-8729-c906e28d06ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STROMATOPOROID TURNOVER AND REEF MORPHOLOGY ANALYSIS\n",
            "WITH PEARSON AND SPEARMAN CORRELATIONS\n",
            "STAGE-LEVEL AND 5-MYR BIN ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Libraries loaded successfully!\n",
            "Output directory: ./output\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "#@title CELL 1: SETUP AND IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "# Install required packages (uncomment if needed)\n",
        "# !pip install openpyxl geopandas shapely requests\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib.lines import Line2D\n",
        "from scipy import stats\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import resample\n",
        "import statsmodels.api as sm\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up matplotlib for publication-quality vector figures\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.labelsize'] = 11\n",
        "plt.rcParams['axes.titlesize'] = 12\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['pdf.fonttype'] = 42  # TrueType fonts in PDF\n",
        "plt.rcParams['ps.fonttype'] = 42\n",
        "plt.rcParams['svg.fonttype'] = 'none'  # Text as text in SVG\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STROMATOPOROID TURNOVER AND REEF MORPHOLOGY ANALYSIS\")\n",
        "print(\"WITH PEARSON AND SPEARMAN CORRELATIONS\")\n",
        "print(\"STAGE-LEVEL AND 5-MYR BIN ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nLibraries loaded successfully!\")\n",
        "\n",
        "# Output directory\n",
        "import os\n",
        "OUTPUT_DIR = \"./output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vx2CXdOaivD",
        "outputId": "b1ec0d2c-5905-4138-ae09-642bc7ca1ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "GENERATING MACROSTRAT DATA\n",
            "======================================================================\n",
            "Generating Macrostrat data from API...\n",
            "  Fetching data for Cambrian...\n",
            "    Retrieved 2480 geological units\n",
            "  Fetching data for Ordovician...\n",
            "    Retrieved 2943 geological units\n",
            "  Fetching data for Silurian...\n",
            "    Retrieved 1715 geological units\n",
            "  Fetching data for Devonian...\n",
            "    Retrieved 2793 geological units\n",
            "  Combined dataset: 9931 geological units\n",
            "  ✓ Saved paleozoic_stage_data.csv\n",
            "  ✓ Saved paleozoic_5myr_data.csv\n",
            "✓ Macrostrat data ready\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "#@title CELL 2: GENERATE MACROSTRAT DATA (paleozoic_stage_data.csv, paleozoic_5myr_data.csv)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"GENERATING MACROSTRAT DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if files already exist\n",
        "macrostrat_stage_file = 'paleozoic_stage_data.csv'\n",
        "macrostrat_5myr_file = 'paleozoic_5myr_data.csv'\n",
        "\n",
        "if os.path.exists(macrostrat_stage_file) and os.path.exists(macrostrat_5myr_file):\n",
        "    print(f\"✓ {macrostrat_stage_file} already exists\")\n",
        "    print(f\"✓ {macrostrat_5myr_file} already exists\")\n",
        "    print(\"Skipping Macrostrat data generation...\")\n",
        "else:\n",
        "    print(\"Generating Macrostrat data from API...\")\n",
        "\n",
        "    import requests\n",
        "    try:\n",
        "        import geopandas as gpd\n",
        "    except ImportError:\n",
        "        print(\"Installing geopandas...\")\n",
        "        import subprocess\n",
        "        subprocess.run(['pip', 'install', 'geopandas', '-q'])\n",
        "        import geopandas as gpd\n",
        "\n",
        "    # Define Paleozoic Period Age Ranges\n",
        "    periods = {\n",
        "        \"Cambrian\": {\"start\": 538.8, \"end\": 485.4, \"color\": \"#7FA056\"},\n",
        "        \"Ordovician\": {\"start\": 485.4, \"end\": 443.8, \"color\": \"#00a9ce\"},\n",
        "        \"Silurian\": {\"start\": 443.8, \"end\": 419.2, \"color\": \"#b3e1af\"},\n",
        "        \"Devonian\": {\"start\": 419.2, \"end\": 358.9, \"color\": \"#cb8c37\"}\n",
        "    }\n",
        "    # Define stages (ICS 2023/04; Cambrian stage boundary ages are approximate in the ICS chart)\n",
        "    cambrian_stages = {\n",
        "        \"Stage 10\": (485.4, 489.5), \"Jiangshanian\": (489.5, 494.0), \"Paibian\": (494.0, 497.0),\n",
        "        \"Guzhangian\": (497.0, 500.5), \"Drumian\": (500.5, 504.5), \"Wuliuan\": (504.5, 509.0),\n",
        "        \"Stage 4\": (509.0, 514.0), \"Stage 3\": (514.0, 521.0), \"Stage 2\": (521.0, 529.0),\n",
        "        \"Fortunian\": (529.0, 538.8)\n",
        "    }\n",
        "    ordovician_stages = {\n",
        "        \"Tremadocian\": (477.7, 485.4), \"Floian\": (470.0, 477.7),\n",
        "        \"Dapingian\": (467.3, 470.0), \"Darriwilian\": (458.4, 467.3),\n",
        "        \"Sandbian\": (453.0, 458.4), \"Katian\": (445.2, 453.0),\n",
        "        \"Hirnantian\": (443.8, 445.2)\n",
        "    }\n",
        "    silurian_stages = {\n",
        "        \"Rhuddanian\": (440.8, 443.8), \"Aeronian\": (438.5, 440.8),\n",
        "        \"Telychian\": (433.4, 438.5), \"Sheinwoodian\": (430.5, 433.4),\n",
        "        \"Homerian\": (427.4, 430.5), \"Gorstian\": (425.6, 427.4),\n",
        "        \"Ludfordian\": (423.0, 425.6), \"Pridoli\": (419.2, 423.0)\n",
        "    }\n",
        "    devonian_stages = {\n",
        "        \"Lochkovian\": (410.8, 419.2), \"Pragian\": (407.6, 410.8),\n",
        "        \"Emsian\": (393.3, 407.6), \"Eifelian\": (387.7, 393.3),\n",
        "        \"Givetian\": (382.7, 387.7), \"Frasnian\": (372.2, 382.7),\n",
        "        \"Famennian\": (358.9, 372.2)\n",
        "    }\n",
        "\n",
        "    # Create stages dataframe\n",
        "    stages_data = []\n",
        "    for stage, (end_age, start_age) in ordovician_stages.items():\n",
        "        stages_data.append({\"stage\": stage, \"start_age\": start_age, \"end_age\": end_age,\n",
        "                           \"mid_age\": (start_age + end_age) / 2, \"period\": \"Ordovician\"})\n",
        "    for stage, (end_age, start_age) in silurian_stages.items():\n",
        "        stages_data.append({\"stage\": stage, \"start_age\": start_age, \"end_age\": end_age,\n",
        "                           \"mid_age\": (start_age + end_age) / 2, \"period\": \"Silurian\"})\n",
        "    for stage, (end_age, start_age) in devonian_stages.items():\n",
        "        stages_data.append({\"stage\": stage, \"start_age\": start_age, \"end_age\": end_age,\n",
        "                           \"mid_age\": (start_age + end_age) / 2, \"period\": \"Devonian\"})\n",
        "    stages_df = pd.DataFrame(stages_data)\n",
        "\n",
        "    # Retrieve Macrostrat Data\n",
        "    periods_to_fetch = [\"Cambrian\", \"Ordovician\", \"Silurian\", \"Devonian\"]\n",
        "    all_units_list = []\n",
        "\n",
        "    for period in periods_to_fetch:\n",
        "        url = f\"https://macrostrat.org/api/units?interval_name={period}&format=geojson&response=long\"\n",
        "        print(f\"  Fetching data for {period}...\")\n",
        "        try:\n",
        "            response = requests.get(url, timeout=60)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                features = data.get(\"success\", {}).get(\"data\", [])\n",
        "                if features:\n",
        "                    period_units = gpd.GeoDataFrame.from_features(features)\n",
        "                    print(f\"    Retrieved {len(period_units)} geological units\")\n",
        "                    period_units['source_period'] = period\n",
        "                    all_units_list.append(period_units)\n",
        "        except Exception as e:\n",
        "            print(f\"    Error fetching {period}: {e}\")\n",
        "\n",
        "    if all_units_list:\n",
        "        units = pd.concat(all_units_list, ignore_index=True)\n",
        "        print(f\"  Combined dataset: {len(units)} geological units\")\n",
        "\n",
        "        # Process units\n",
        "        try:\n",
        "            if units.crs is None:\n",
        "                units.set_crs(epsg=4326, inplace=True)\n",
        "            units = units.to_crs(epsg=3857)\n",
        "            if 'col_area' in units.columns:\n",
        "                units['area_km2'] = pd.to_numeric(units['col_area'], errors='coerce')\n",
        "            else:\n",
        "                units['area_km2'] = units.geometry.area / 1e6\n",
        "        except:\n",
        "            units['area_km2'] = 100  # Default\n",
        "\n",
        "        units['t_age'] = pd.to_numeric(units['t_age'], errors='coerce')\n",
        "        units['b_age'] = pd.to_numeric(units['b_age'], errors='coerce')\n",
        "        units['mid_age'] = (units['t_age'] + units['b_age']) / 2.0\n",
        "        units.dropna(subset=['mid_age'], inplace=True)\n",
        "\n",
        "        # Identify carbonates\n",
        "        def check_if_carbonate(lithologies):\n",
        "            if isinstance(lithologies, list):\n",
        "                for lith in lithologies:\n",
        "                    if isinstance(lith, dict) and 'type' in lith and 'carbonate' in str(lith['type']).lower():\n",
        "                        return True\n",
        "            elif isinstance(lithologies, str):\n",
        "                return 'carbonate' in lithologies.lower()\n",
        "            return False\n",
        "\n",
        "        units['is_carbonate'] = units['lith'].apply(check_if_carbonate)\n",
        "        carbonate_units = units[units['is_carbonate']].copy()\n",
        "\n",
        "        # Assign stages\n",
        "        all_stages = {**cambrian_stages, **ordovician_stages, **silurian_stages, **devonian_stages}\n",
        "        def assign_stage(age):\n",
        "            for stage, (end, start) in all_stages.items():\n",
        "                if start >= age >= end:\n",
        "                    return stage\n",
        "            return None\n",
        "\n",
        "        units['stage'] = units['mid_age'].apply(assign_stage)\n",
        "        carbonate_units['stage'] = carbonate_units['mid_age'].apply(assign_stage)\n",
        "\n",
        "        # Aggregate by stage\n",
        "        stage_totals = units.groupby('stage')['area_km2'].sum().reset_index()\n",
        "        stage_totals.rename(columns={'area_km2': 'total_area_km2'}, inplace=True)\n",
        "        stage_carbonates = carbonate_units.groupby('stage')['area_km2'].sum().reset_index()\n",
        "        stage_carbonates.rename(columns={'area_km2': 'carbonate_area_km2'}, inplace=True)\n",
        "\n",
        "        stage_summary = pd.merge(stage_totals, stage_carbonates, on='stage', how='left')\n",
        "        stage_summary['carbonate_area_km2'] = stage_summary['carbonate_area_km2'].fillna(0)\n",
        "        stage_summary['carbonate_percentage'] = (stage_summary['carbonate_area_km2'] / stage_summary['total_area_km2']) * 100\n",
        "\n",
        "        macrostrat_data = pd.merge(stages_df, stage_summary, on='stage', how='left')\n",
        "        macrostrat_data = macrostrat_data.sort_values('start_age', ascending=False).reset_index(drop=True)\n",
        "        macrostrat_data.to_csv(macrostrat_stage_file, index=False)\n",
        "        print(f\"  ✓ Saved {macrostrat_stage_file}\")\n",
        "\n",
        "        # 5 Myr bins\n",
        "        max_age = 490\n",
        "        min_age = 355\n",
        "        manual_bins = np.arange(min_age, max_age + 5, 5)\n",
        "\n",
        "        units['time_bin'] = pd.cut(units['mid_age'], bins=manual_bins, include_lowest=True, right=False)\n",
        "        carbonate_units['time_bin'] = pd.cut(carbonate_units['mid_age'], bins=manual_bins, include_lowest=True, right=False)\n",
        "\n",
        "        macro_all_5myr = units.groupby('time_bin')['area_km2'].sum().reset_index()\n",
        "        macro_all_5myr.rename(columns={'area_km2': 'total_area_km2'}, inplace=True)\n",
        "        macro_carb_5myr = carbonate_units.groupby('time_bin')['area_km2'].sum().reset_index()\n",
        "        macro_carb_5myr.rename(columns={'area_km2': 'carbonate_area_km2'}, inplace=True)\n",
        "\n",
        "        macrostrat_5myr = pd.merge(macro_all_5myr, macro_carb_5myr, on='time_bin', how='left')\n",
        "        macrostrat_5myr['carbonate_area_km2'] = macrostrat_5myr['carbonate_area_km2'].fillna(0)\n",
        "        macrostrat_5myr['carbonate_percentage'] = (macrostrat_5myr['carbonate_area_km2'] / macrostrat_5myr['total_area_km2']) * 100\n",
        "        macrostrat_5myr['bin_mid'] = macrostrat_5myr['time_bin'].apply(lambda x: (x.left + x.right) / 2 if pd.notna(x) else np.nan)\n",
        "        macrostrat_5myr.to_csv(macrostrat_5myr_file, index=False)\n",
        "        print(f\"  ✓ Saved {macrostrat_5myr_file}\")\n",
        "    else:\n",
        "        print(\"  WARNING: Could not fetch Macrostrat data. Creating placeholder files...\")\n",
        "        # Create placeholder files\n",
        "        pd.DataFrame(columns=['stage', 'total_area_km2', 'carbonate_area_km2', 'carbonate_percentage']).to_csv(macrostrat_stage_file, index=False)\n",
        "        pd.DataFrame(columns=['bin_mid', 'total_area_km2', 'carbonate_area_km2', 'carbonate_percentage']).to_csv(macrostrat_5myr_file, index=False)\n",
        "\n",
        "print(\"✓ Macrostrat data ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Fk40AhaealAV",
        "outputId": "de288ccc-170b-41a3-de5b-12a9131c9576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "GENERATING PARED REEF DATA\n",
            "======================================================================\n",
            "Source file 'PARED_reef_All_numerical.csv' not found.\n",
            "Please upload it now:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ec7ff221-51ba-4114-b7d6-0fa885ec7801\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ec7ff221-51ba-4114-b7d6-0fa885ec7801\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving PARED_reef_All_numerical.csv to PARED_reef_All_numerical.csv\n",
            "Processing PARED_reef_All_numerical.csv...\n",
            "  ✓ Generated cambrian_devonian_reef_data_stage_for_analysis.csv\n",
            "  ✓ Generated cambrian_devonian_reef_data_5myr_for_analysis.csv\n",
            "✓ PARED reef data ready\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "#@title CELL 3: GENERATE PARED REEF DATA (reef stage and 5myr files)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING PARED REEF DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "reef_stage_file = 'cambrian_devonian_reef_data_stage_for_analysis.csv'\n",
        "reef_5myr_file = 'cambrian_devonian_reef_data_5myr_for_analysis.csv'\n",
        "pared_source_file = 'PARED_reef_All_numerical.csv'\n",
        "\n",
        "# Force regeneration to include new variable\n",
        "if os.path.exists(reef_stage_file) and os.path.exists(reef_5myr_file) and False:\n",
        "    print(f\"✓ {reef_stage_file} already exists\")\n",
        "    print(f\"✓ {reef_5myr_file} already exists\")\n",
        "    print(\"Skipping PARED reef data generation...\")\n",
        "else:\n",
        "    # Check if source file exists\n",
        "    if not os.path.exists(pared_source_file):\n",
        "        print(f\"Source file '{pared_source_file}' not found.\")\n",
        "        print(\"Please upload it now:\")\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            uploaded_pared = files.upload()\n",
        "            uploaded_name = list(uploaded_pared.keys())[0]\n",
        "            if uploaded_name != pared_source_file:\n",
        "                os.rename(uploaded_name, pared_source_file)\n",
        "        except ImportError:\n",
        "            raise FileNotFoundError(f\"Please place '{pared_source_file}' in the current directory.\")\n",
        "\n",
        "    print(f\"Processing {pared_source_file}...\")\n",
        "\n",
        "    # Load PARED data\n",
        "    try:\n",
        "        pared_df = pd.read_csv(pared_source_file, encoding='utf-8')\n",
        "    except UnicodeDecodeError:\n",
        "        try:\n",
        "            pared_df = pd.read_csv(pared_source_file, encoding='latin-1')\n",
        "        except:\n",
        "            pared_df = pd.read_csv(pared_source_file, encoding='cp1252')\n",
        "\n",
        "    # --- NEW CODE: Calculate Paired Ratio (Log Difference) ---\n",
        "    # Ensure numeric\n",
        "    pared_df['thickness'] = pd.to_numeric(pared_df['thickness'], errors='coerce')\n",
        "    pared_df['width'] = pd.to_numeric(pared_df['width'], errors='coerce')\n",
        "\n",
        "    # Calculate difference (Log Thickness - Log Width)\n",
        "    # This automatically becomes NaN if either value is missing\n",
        "    pared_df['t_w_log_ratio'] = pared_df['thickness'] - pared_df['width']\n",
        "    # ---------------------------------------------------------\n",
        "\n",
        "    def analyze_pared_data(dataframe, bin_definitions, analysis_type_label):\n",
        "        \"\"\"Calculate statistics using OVERLAP method\"\"\"\n",
        "        results = []\n",
        "        # Added 't_w_log_ratio' to variables\n",
        "        variables = ['thickness', 'width', 'extension', 't_w_log_ratio']\n",
        "\n",
        "        for bin_def in bin_definitions:\n",
        "            s_start = bin_def['start_ma']\n",
        "            s_end = bin_def['end_ma']\n",
        "            name = bin_def['time_identifier']\n",
        "\n",
        "            # OVERLAP LOGIC\n",
        "            mask = (dataframe['min_ma'] < s_end) & (dataframe['max_ma'] > s_start)\n",
        "            subset = dataframe[mask]\n",
        "\n",
        "            row_data = {\n",
        "                'bin_center': (s_start + s_end) / 2.0,\n",
        "                'start_age': s_start,\n",
        "                'end_age': s_end,\n",
        "                'reef_count': len(subset),\n",
        "            }\n",
        "\n",
        "            for var in variables:\n",
        "                data = subset[var].dropna() if var in subset.columns else pd.Series()\n",
        "                if len(data) > 0:\n",
        "                    row_data[f'{var}_mean'] = data.mean()\n",
        "                    row_data[f'{var}_std'] = data.std()\n",
        "                    row_data[f'{var}_stderr'] = data.sem()\n",
        "                    row_data[f'{var}_median'] = data.median()\n",
        "                    row_data[f'{var}_count'] = len(data)\n",
        "                else:\n",
        "                    for suffix in ['mean', 'std', 'stderr', 'median']:\n",
        "                        row_data[f'{var}_{suffix}'] = np.nan\n",
        "                    row_data[f'{var}_count'] = 0\n",
        "\n",
        "            row_data['analysis_type'] = analysis_type_label\n",
        "            row_data['time_identifier'] = name\n",
        "            row_data['name'] = name\n",
        "            row_data['start_ma'] = s_start\n",
        "            row_data['end_ma'] = s_end\n",
        "            row_data['midpoint_ma'] = (s_start + s_end) / 2.0\n",
        "            row_data['duration_myr'] = round(s_end - s_start, 1)\n",
        "\n",
        "            results.append(row_data)\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    # Stage definitions (PRESERVED FROM ORIGINAL)\n",
        "    stages_data = [\n",
        "        # Devonian\n",
        "        {'time_identifier': 'Famennian', 'start_ma': 358.9, 'end_ma': 372.2},\n",
        "        {'time_identifier': 'Frasnian', 'start_ma': 372.2, 'end_ma': 382.7},\n",
        "        {'time_identifier': 'Givetian', 'start_ma': 382.7, 'end_ma': 387.7},\n",
        "        {'time_identifier': 'Eifelian', 'start_ma': 387.7, 'end_ma': 393.3},\n",
        "        {'time_identifier': 'Emsian', 'start_ma': 393.3, 'end_ma': 407.6},\n",
        "        {'time_identifier': 'Pragian', 'start_ma': 407.6, 'end_ma': 410.8},\n",
        "        {'time_identifier': 'Lochkovian', 'start_ma': 410.8, 'end_ma': 419.2},\n",
        "\n",
        "        # Silurian\n",
        "        {'time_identifier': 'Pridoli', 'start_ma': 419.2, 'end_ma': 423.0},\n",
        "        {'time_identifier': 'Ludfordian', 'start_ma': 423.0, 'end_ma': 425.6},\n",
        "        {'time_identifier': 'Gorstian', 'start_ma': 425.6, 'end_ma': 427.4},\n",
        "        {'time_identifier': 'Homerian', 'start_ma': 427.4, 'end_ma': 430.5},\n",
        "        {'time_identifier': 'Sheinwoodian', 'start_ma': 430.5, 'end_ma': 433.4},\n",
        "        {'time_identifier': 'Telychian', 'start_ma': 433.4, 'end_ma': 438.5},\n",
        "        {'time_identifier': 'Aeronian', 'start_ma': 438.5, 'end_ma': 440.8},\n",
        "        {'time_identifier': 'Rhuddanian', 'start_ma': 440.8, 'end_ma': 443.8},\n",
        "\n",
        "        # Ordovician\n",
        "        {'time_identifier': 'Hirnantian', 'start_ma': 443.8, 'end_ma': 445.2},\n",
        "        {'time_identifier': 'Katian', 'start_ma': 445.2, 'end_ma': 453.0},\n",
        "        {'time_identifier': 'Sandbian', 'start_ma': 453.0, 'end_ma': 458.4},\n",
        "        {'time_identifier': 'Darriwilian', 'start_ma': 458.4, 'end_ma': 467.3},\n",
        "        {'time_identifier': 'Dapingian', 'start_ma': 467.3, 'end_ma': 470.0},\n",
        "        {'time_identifier': 'Floian', 'start_ma': 470.0, 'end_ma': 477.7},\n",
        "        {'time_identifier': 'Tremadocian', 'start_ma': 477.7, 'end_ma': 485.4},\n",
        "\n",
        "        # Cambrian (ICS 2023/04; approximate boundaries)\n",
        "        {'time_identifier': 'Stage 10', 'start_ma': 485.4, 'end_ma': 489.5},\n",
        "        {'time_identifier': 'Jiangshanian', 'start_ma': 489.5, 'end_ma': 494.0},\n",
        "        {'time_identifier': 'Paibian', 'start_ma': 494.0, 'end_ma': 497.0},\n",
        "        {'time_identifier': 'Guzhangian', 'start_ma': 497.0, 'end_ma': 500.5},\n",
        "        {'time_identifier': 'Drumian', 'start_ma': 500.5, 'end_ma': 504.5},\n",
        "        {'time_identifier': 'Wuliuan', 'start_ma': 504.5, 'end_ma': 509.0},\n",
        "        {'time_identifier': 'Stage 4', 'start_ma': 509.0, 'end_ma': 514.0},\n",
        "        {'time_identifier': 'Stage 3', 'start_ma': 514.0, 'end_ma': 521.0},\n",
        "        {'time_identifier': 'Stage 2', 'start_ma': 521.0, 'end_ma': 529.0},\n",
        "        {'time_identifier': 'Fortunian', 'start_ma': 529.0, 'end_ma': 538.8},\n",
        "    ]\n",
        "\n",
        "    # 5-Myr bins (Cambrian–Devonian)\n",
        "    # We avoid spilling into pre-Cambrian by using partial bins at both ends:\n",
        "    #   358.9–360.0 and 535.0–538.8.\n",
        "    bins_5myr = []\n",
        "\n",
        "    # bottom partial bin (Devonian top is 358.9)\n",
        "    bins_5myr.append({'time_identifier': \"358.9-360.0 Ma\", 'start_ma': 358.9, 'end_ma': 360.0})\n",
        "\n",
        "    # regular 5 Myr bins\n",
        "    age = 360.0\n",
        "    while age < 535.0:\n",
        "        bins_5myr.append({\n",
        "            'time_identifier': f\"{int(age)}-{int(age+5)} Ma\",\n",
        "            'start_ma': float(age),\n",
        "            'end_ma': float(age + 5.0)\n",
        "        })\n",
        "        age += 5.0\n",
        "\n",
        "    # top partial bin (Cambrian base is 538.8)\n",
        "    bins_5myr.append({'time_identifier': \"535.0-538.8 Ma\", 'start_ma': 535.0, 'end_ma': 538.8})\n",
        "\n",
        "\n",
        "    # Generate stage data\n",
        "    df_stages = analyze_pared_data(pared_df, stages_data, 'Geological_Stages')\n",
        "    df_stages.to_csv(reef_stage_file, index=False)\n",
        "    print(f\"  ✓ Generated {reef_stage_file}\")\n",
        "\n",
        "    # Generate 5myr data\n",
        "    df_5myr = analyze_pared_data(pared_df, bins_5myr, '5_myr_bins')\n",
        "    df_5myr.to_csv(reef_5myr_file, index=False)\n",
        "    print(f\"  ✓ Generated {reef_5myr_file}\")\n",
        "\n",
        "print(\"✓ PARED reef data ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-CR5FkScbhfW",
        "outputId": "b697134c-4b19-4903-9e91-3a7d4c16657b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created Generic 5-Myr bins (Aligned to 540.0):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     bin_label  bin_top  bin_bottom\n",
              "0  540.0-535.0    540.0       535.0\n",
              "1  535.0-530.0    535.0       530.0\n",
              "2  530.0-525.0    530.0       525.0\n",
              "3  525.0-520.0    525.0       520.0\n",
              "4  520.0-515.0    520.0       515.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bcac4e2a-c131-4d44-94c1-e626d13774b1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bin_label</th>\n",
              "      <th>bin_top</th>\n",
              "      <th>bin_bottom</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>540.0-535.0</td>\n",
              "      <td>540.0</td>\n",
              "      <td>535.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>535.0-530.0</td>\n",
              "      <td>535.0</td>\n",
              "      <td>530.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>530.0-525.0</td>\n",
              "      <td>530.0</td>\n",
              "      <td>525.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>525.0-520.0</td>\n",
              "      <td>525.0</td>\n",
              "      <td>520.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>520.0-515.0</td>\n",
              "      <td>520.0</td>\n",
              "      <td>515.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bcac4e2a-c131-4d44-94c1-e626d13774b1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bcac4e2a-c131-4d44-94c1-e626d13774b1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bcac4e2a-c131-4d44-94c1-e626d13774b1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"=\\\"*40)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"bin_label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"535.0-530.0\",\n          \"520.0-515.0\",\n          \"530.0-525.0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bin_top\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.905694150420948,\n        \"min\": 520.0,\n        \"max\": 540.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          535.0,\n          520.0,\n          530.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bin_bottom\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.905694150420948,\n        \"min\": 515.0,\n        \"max\": 535.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          530.0,\n          515.0,\n          525.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      bin_label  bin_top  bin_bottom\n",
              "32  380.0-375.0    380.0       375.0\n",
              "33  375.0-370.0    375.0       370.0\n",
              "34  370.0-365.0    370.0       365.0\n",
              "35  365.0-360.0    365.0       360.0\n",
              "36  360.0-355.0    360.0       355.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d39d8a4b-8873-410b-90b5-36de3780c4c5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bin_label</th>\n",
              "      <th>bin_top</th>\n",
              "      <th>bin_bottom</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>380.0-375.0</td>\n",
              "      <td>380.0</td>\n",
              "      <td>375.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>375.0-370.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>370.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>370.0-365.0</td>\n",
              "      <td>370.0</td>\n",
              "      <td>365.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>365.0-360.0</td>\n",
              "      <td>365.0</td>\n",
              "      <td>360.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>360.0-355.0</td>\n",
              "      <td>360.0</td>\n",
              "      <td>355.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d39d8a4b-8873-410b-90b5-36de3780c4c5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d39d8a4b-8873-410b-90b5-36de3780c4c5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d39d8a4b-8873-410b-90b5-36de3780c4c5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"=\\\"*40)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"bin_label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"375.0-370.0\",\n          \"360.0-355.0\",\n          \"370.0-365.0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bin_top\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.905694150420948,\n        \"min\": 360.0,\n        \"max\": 380.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          375.0,\n          360.0,\n          370.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bin_bottom\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.905694150420948,\n        \"min\": 355.0,\n        \"max\": 375.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          370.0,\n          355.0,\n          365.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- CHECKING FILE SYSTEM ---\n",
            "[WARN] No pbdb_data_*.csv found in ./output or current directory.\n",
            "Please upload your RAW PBDB files now (pbdb_data_*.csv).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-da18f281-fb15-4961-905e-c49e03046293\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-da18f281-fb15-4961-905e-c49e03046293\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pbdb_data_Actinostromatida.csv to pbdb_data_Actinostromatida.csv\n",
            "Saving pbdb_data_Amphiporida.csv to pbdb_data_Amphiporida.csv\n",
            "Saving pbdb_data_Clathrodictyida.csv to pbdb_data_Clathrodictyida.csv\n",
            "Saving pbdb_data_Labechiida.csv to pbdb_data_Labechiida.csv\n",
            "Saving pbdb_data_Rugosa.csv to pbdb_data_Rugosa.csv\n",
            "Saving pbdb_data_Stromatoporellida.csv to pbdb_data_Stromatoporellida.csv\n",
            "Saving pbdb_data_Stromatoporida.csv to pbdb_data_Stromatoporida.csv\n",
            "Saving pbdb_data_Syringostromatida.csv to pbdb_data_Syringostromatida.csv\n",
            "Saving pbdb_data_Tabulata.csv to pbdb_data_Tabulata.csv\n",
            "  Uploaded 9 pbdb_data_*.csv files.\n",
            "Found 9 files:\n",
            "['pbdb_data_Actinostromatida.csv', 'pbdb_data_Amphiporida.csv', 'pbdb_data_Clathrodictyida.csv', 'pbdb_data_Labechiida.csv', 'pbdb_data_Rugosa.csv', 'pbdb_data_Stromatoporellida.csv', 'pbdb_data_Stromatoporida.csv', 'pbdb_data_Syringostromatida.csv', 'pbdb_data_Tabulata.csv']\n",
            "\n",
            "--- STARTING ANALYSIS (Generic 540.0 Bins; Cam–Dev) ---\n",
            "\n",
            "Analyzing Actinostromatida...\n",
            "  -> Created: pbdb_Actinostromatida_midpoint_stages.csv\n",
            "  -> Created: pbdb_Actinostromatida_midpoint_5myr_bins.csv\n",
            "\n",
            "Analyzing Amphiporida...\n",
            "  -> Created: pbdb_Amphiporida_midpoint_stages.csv\n",
            "  -> Created: pbdb_Amphiporida_midpoint_5myr_bins.csv\n",
            "\n",
            "Analyzing Clathrodictyida...\n",
            "  -> Created: pbdb_Clathrodictyida_midpoint_stages.csv\n",
            "  -> Created: pbdb_Clathrodictyida_midpoint_5myr_bins.csv\n",
            "\n",
            "Analyzing Labechiida...\n",
            "  -> Created: pbdb_Labechiida_midpoint_stages.csv\n",
            "  -> Created: pbdb_Labechiida_midpoint_5myr_bins.csv\n",
            "\n",
            "Analyzing Rugosa...\n",
            "  -> Created: pbdb_Rugosa_midpoint_stages.csv\n",
            "  -> Created: pbdb_Rugosa_midpoint_5myr_bins.csv\n",
            "\n",
            "Analyzing Stromatoporellida...\n",
            "  -> Created: pbdb_Stromatoporellida_midpoint_stages.csv\n",
            "  -> Created: pbdb_Stromatoporellida_midpoint_5myr_bins.csv\n",
            "\n",
            "Analyzing Stromatoporida...\n",
            "  -> Created: pbdb_Stromatoporida_midpoint_stages.csv\n",
            "  -> Created: pbdb_Stromatoporida_midpoint_5myr_bins.csv\n",
            "\n",
            "Analyzing Syringostromatida...\n",
            "  -> Created: pbdb_Syringostromatida_midpoint_stages.csv\n",
            "  -> Created: pbdb_Syringostromatida_midpoint_5myr_bins.csv\n",
            "\n",
            "Analyzing Tabulata...\n",
            "  -> Created: pbdb_Tabulata_midpoint_stages.csv\n",
            "  -> Created: pbdb_Tabulata_midpoint_5myr_bins.csv\n",
            "\n",
            "========================================\n",
            "DONE! New 540.0-aligned bin files created (Cambrian–Devonian).\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 4: GENERATE PBDB DIVERSITY AND OCCURRENCE DATA (GENERIC 540.0 BINS; Cam–Dev)\n",
        "#   - Self-contained (defines smart_read_pbdb and all helpers)\n",
        "#   - Works whether PBDB files are in ./output or current directory\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Colab-safe upload\n",
        "try:\n",
        "    from google.colab import files  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP: Create Time References (Cambrian–Devonian)\n",
        "#   NOTE: Cambrian stage boundary ages are approximate in the ICS chart.\n",
        "# ==========================================\n",
        "ics_data = \"\"\"stage,series,period,start_ma,end_ma\n",
        "Fortunian,Lower Cambrian,Cambrian,538.8,529.0\n",
        "Stage 2,Lower Cambrian,Cambrian,529.0,521.0\n",
        "Stage 3,Lower Cambrian,Cambrian,521.0,514.0\n",
        "Stage 4,Lower Cambrian,Cambrian,514.0,509.0\n",
        "Wuliuan,Miaolingian,Cambrian,509.0,504.5\n",
        "Drumian,Miaolingian,Cambrian,504.5,500.5\n",
        "Guzhangian,Miaolingian,Cambrian,500.5,497.0\n",
        "Paibian,Furongian,Cambrian,497.0,494.0\n",
        "Jiangshanian,Furongian,Cambrian,494.0,489.5\n",
        "Stage 10,Furongian,Cambrian,489.5,485.4\n",
        "Tremadocian,Lower Ordovician,Ordovician,485.4,477.7\n",
        "Floian,Lower Ordovician,Ordovician,477.7,470.0\n",
        "Dapingian,Middle Ordovician,Ordovician,470.0,467.3\n",
        "Darriwilian,Middle Ordovician,Ordovician,467.3,458.4\n",
        "Sandbian,Upper Ordovician,Ordovician,458.4,453.0\n",
        "Katian,Upper Ordovician,Ordovician,453.0,445.2\n",
        "Hirnantian,Upper Ordovician,Ordovician,445.2,443.8\n",
        "Rhuddanian,Llandovery,Silurian,443.8,440.8\n",
        "Aeronian,Llandovery,Silurian,440.8,438.5\n",
        "Telychian,Llandovery,Silurian,438.5,433.4\n",
        "Sheinwoodian,Wenlock,Silurian,433.4,430.5\n",
        "Homerian,Wenlock,Silurian,430.5,427.4\n",
        "Gorstian,Ludlow,Silurian,427.4,425.6\n",
        "Ludfordian,Ludlow,Silurian,425.6,423.0\n",
        "Pridoli,Pridoli,Silurian,423.0,419.2\n",
        "Lochkovian,Lower Devonian,Devonian,419.2,410.8\n",
        "Pragian,Lower Devonian,Devonian,410.8,407.6\n",
        "Emsian,Lower Devonian,Devonian,407.6,393.3\n",
        "Eifelian,Middle Devonian,Devonian,393.3,387.7\n",
        "Givetian,Middle Devonian,Devonian,387.7,382.7\n",
        "Frasnian,Upper Devonian,Devonian,382.7,372.2\n",
        "Famennian,Upper Devonian,Devonian,372.2,358.9\n",
        "\"\"\"\n",
        "\n",
        "ICS_CSV = Path(\"ICS_stage_boundaries.csv\")\n",
        "ICS_CSV.write_text(ics_data, encoding=\"utf-8\")\n",
        "\n",
        "# Generic 5-Myr bins aligned to 540.0 Ma to cover full Cambrian–Devonian window.\n",
        "def create_5myr_bins(start_ma=540.0, end_ma=358.9, step=5.0):\n",
        "    bins = []\n",
        "    current = float(start_ma)\n",
        "\n",
        "    # Iterate downward in time (older -> younger).\n",
        "    # We want to include the partial bin that contains end_ma (358.9).\n",
        "    while True:\n",
        "        top = current\n",
        "        bottom = current - step\n",
        "        bins.append({\n",
        "            \"bin_label\": f\"{top:.1f}-{bottom:.1f}\",\n",
        "            \"bin_top\": float(top),\n",
        "            \"bin_bottom\": float(bottom)\n",
        "        })\n",
        "\n",
        "        # Stop once we've created the bin that contains end_ma.\n",
        "        # Example: end_ma=358.9 lies in 360–355, so stop after creating 360–355.\n",
        "        if bottom < end_ma:\n",
        "            break\n",
        "\n",
        "        current = bottom\n",
        "\n",
        "        # Safety: avoid runaway if end_ma is mis-set\n",
        "        if len(bins) > 1000:\n",
        "            raise RuntimeError(\"Too many bins created; check start_ma/end_ma.\")\n",
        "    return pd.DataFrame(bins)\n",
        "\n",
        "bins_5myr_df = create_5myr_bins()\n",
        "stages_df = pd.read_csv(ICS_CSV)\n",
        "\n",
        "print(\"Created Generic 5-Myr bins (Aligned to 540.0):\")\n",
        "display(bins_5myr_df.head())\n",
        "display(bins_5myr_df.tail())\n",
        "\n",
        "# ==========================================\n",
        "# 2. FILE CHECK\n",
        "# ==========================================\n",
        "print(\"\\n--- CHECKING FILE SYSTEM ---\")\n",
        "\n",
        "# Look in ./output first (your pipeline convention), then current directory\n",
        "found_files = []\n",
        "for pattern in [\"./output/pbdb_data_*.csv\", \"./pbdb_data_*.csv\"]:\n",
        "    found_files.extend(sorted([str(p) for p in Path(\".\").glob(pattern.replace(\"./\", \"\"))]))\n",
        "\n",
        "# De-duplicate while preserving order\n",
        "seen = set()\n",
        "found_files = [f for f in found_files if not (f in seen or seen.add(f))]\n",
        "\n",
        "if not found_files:\n",
        "    print(\"[WARN] No pbdb_data_*.csv found in ./output or current directory.\")\n",
        "    if IN_COLAB:\n",
        "        print(\"Please upload your RAW PBDB files now (pbdb_data_*.csv).\")\n",
        "        uploaded = files.upload()\n",
        "        # after upload, they land in current directory\n",
        "        found_files = sorted([f for f in os.listdir(\".\") if f.lower().startswith(\"pbdb_data_\") and f.lower().endswith(\".csv\")])\n",
        "        print(f\"  Uploaded {len(found_files)} pbdb_data_*.csv files.\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No pbdb_data_*.csv found. Place files in ./output or the current directory and re-run Cell 4.\")\n",
        "\n",
        "print(f\"Found {len(found_files)} files:\")\n",
        "print(found_files)\n",
        "\n",
        "# ==========================================\n",
        "# 3. HELPER FUNCTIONS (Self-contained)\n",
        "# ==========================================\n",
        "def smart_read_pbdb(file_path: str):\n",
        "    \"\"\"\n",
        "    PBDB downloads sometimes contain metadata rows before the header.\n",
        "    Detect the header row by locating 'occurrence_no'.\n",
        "    \"\"\"\n",
        "    header_row = None\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "            lines = [f.readline() for _ in range(100)]\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"occurrence_no\" in (line or \"\").lower():\n",
        "                header_row = i\n",
        "                break\n",
        "        if header_row is None:\n",
        "            print(f\"    CRITICAL: Could not find 'occurrence_no' header in {file_path}\")\n",
        "            return None\n",
        "        return pd.read_csv(file_path, header=header_row)\n",
        "    except Exception as e:\n",
        "        print(f\"    Error reading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_genus(accepted_name):\n",
        "    if pd.isna(accepted_name):\n",
        "        return None\n",
        "    return str(accepted_name).split(\" \")[0]\n",
        "\n",
        "def get_stage_from_age(age, stages_df):\n",
        "    # Strict containment: start_ma >= age > end_ma (Ma decreasing through time)\n",
        "    match = stages_df[(stages_df[\"start_ma\"] >= age) & (stages_df[\"end_ma\"] < age)]\n",
        "    if match.empty:\n",
        "        # Snap very youngest boundary if needed\n",
        "        if np.isfinite(age) and abs(age - stages_df[\"end_ma\"].min()) < 1e-6:\n",
        "            return stages_df.iloc[-1][\"stage\"]\n",
        "        return None\n",
        "    return match.iloc[0][\"stage\"]\n",
        "\n",
        "def get_bin_from_age(age, bins_df):\n",
        "    # Strict containment\n",
        "    match = bins_df[(bins_df[\"bin_top\"] >= age) & (bins_df[\"bin_bottom\"] < age)]\n",
        "\n",
        "    # Tolerance snap for slightly older-than-top ages (rare rounding issues)\n",
        "    if match.empty:\n",
        "        max_top = bins_df[\"bin_top\"].max()\n",
        "        if age > max_top and (age - max_top) < 2.0:\n",
        "            return bins_df.iloc[0][\"bin_label\"]\n",
        "\n",
        "    if not match.empty:\n",
        "        return match.iloc[0][\"bin_label\"]\n",
        "    return None\n",
        "\n",
        "def process_midpoint(df, group_name, reference_df, ref_type=\"stage\"):\n",
        "    if \"accepted_name\" not in df.columns:\n",
        "        print(\"    Error: 'accepted_name' column missing.\")\n",
        "        return None\n",
        "\n",
        "    # Required PBDB age columns\n",
        "    if (\"max_ma\" not in df.columns) or (\"min_ma\" not in df.columns):\n",
        "        print(\"    Error: PBDB file missing 'max_ma' and/or 'min_ma' columns.\")\n",
        "        return None\n",
        "\n",
        "    df[\"genus_name\"] = df[\"accepted_name\"].apply(extract_genus)\n",
        "    df = df.dropna(subset=[\"genus_name\"])\n",
        "\n",
        "    # Midpoint Logic\n",
        "    df[\"midpoint\"] = (pd.to_numeric(df[\"max_ma\"], errors=\"coerce\") + pd.to_numeric(df[\"min_ma\"], errors=\"coerce\")) / 2.0\n",
        "    df = df.dropna(subset=[\"midpoint\"])\n",
        "\n",
        "    if ref_type == \"stage\":\n",
        "        df[\"assigned_interval\"] = df[\"midpoint\"].apply(lambda x: get_stage_from_age(x, reference_df))\n",
        "        merge_col = \"stage\"\n",
        "    else:\n",
        "        df[\"assigned_interval\"] = df[\"midpoint\"].apply(lambda x: get_bin_from_age(x, reference_df))\n",
        "        merge_col = \"bin_label\"\n",
        "\n",
        "    df = df.dropna(subset=[\"assigned_interval\"])\n",
        "\n",
        "    # Aggregation\n",
        "    genus_counts = df.groupby(\"assigned_interval\")[\"genus_name\"].nunique()\n",
        "    genus_counts.name = f\"{group_name}_genus\"\n",
        "\n",
        "    occ_counts = df.groupby(\"assigned_interval\")[\"occurrence_no\"].nunique()\n",
        "    occ_counts.name = f\"{group_name}_occ\"\n",
        "\n",
        "    # Merging\n",
        "    final_df = reference_df.copy()\n",
        "    final_df = final_df.merge(genus_counts, left_on=merge_col, right_index=True, how=\"left\")\n",
        "    final_df = final_df.merge(occ_counts, left_on=merge_col, right_index=True, how=\"left\")\n",
        "\n",
        "    # Cleanup\n",
        "    cols_to_fix = [f\"{group_name}_genus\", f\"{group_name}_occ\"]\n",
        "    for c in cols_to_fix:\n",
        "        if c in final_df.columns:\n",
        "            final_df[c] = final_df[c].fillna(0).astype(int)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# ==========================================\n",
        "# 4. MAIN EXECUTION LOOP\n",
        "# ==========================================\n",
        "print(\"\\n--- STARTING ANALYSIS (Generic 540.0 Bins; Cam–Dev) ---\")\n",
        "\n",
        "for file_path in found_files:\n",
        "    filename = os.path.basename(file_path)\n",
        "    group_name = filename.replace(\"pbdb_data_\", \"\").replace(\".csv\", \"\")\n",
        "    print(f\"\\nAnalyzing {group_name}...\")\n",
        "\n",
        "    df_pbdb = smart_read_pbdb(file_path)\n",
        "    if df_pbdb is None:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # A. Stages\n",
        "        stage_df = process_midpoint(df_pbdb.copy(), group_name, stages_df, \"stage\")\n",
        "        if stage_df is not None:\n",
        "            out_stage = f\"pbdb_{group_name}_midpoint_stages.csv\"\n",
        "            stage_df.to_csv(out_stage, index=False)\n",
        "            print(f\"  -> Created: {out_stage}\")\n",
        "\n",
        "        # B. 5-Myr Bins\n",
        "        bin_df = process_midpoint(df_pbdb.copy(), group_name, bins_5myr_df, \"bin\")\n",
        "        if bin_df is not None:\n",
        "            out_bin = f\"pbdb_{group_name}_midpoint_5myr_bins.csv\"\n",
        "            bin_df.to_csv(out_bin, index=False)\n",
        "            print(f\"  -> Created: {out_bin}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"DONE! New 540.0-aligned bin files created (Cambrian–Devonian).\")\n",
        "print(\"=\"*40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "OHVXM-Zdbn9n",
        "outputId": "6241861e-7886-47e8-cfb7-ae2eed3709d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following files are missing and need to be uploaded:\n",
            "  1. temperature.csv\n",
            "  2. DO.csv\n",
            "  3. oxygen.csv\n",
            "  4. sealevel.csv\n",
            "  5. d13C_5Myr_Cam-Dev.csv\n",
            "  6. d13C_stage_binned_Cam-Dev.csv\n",
            "\n",
            "Click 'Choose Files' and select the missing files...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-caae9f99-e0d5-44cb-859a-70201d7df5b1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-caae9f99-e0d5-44cb-859a-70201d7df5b1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving d13C_5Myr_Cam-Dev.csv to d13C_5Myr_Cam-Dev.csv\n",
            "Saving d13C_stage_binned_Cam-Dev.csv to d13C_stage_binned_Cam-Dev.csv\n",
            "Saving DO.csv to DO.csv\n",
            "Saving oxygen.csv to oxygen.csv\n",
            "Saving sealevel.csv to sealevel.csv\n",
            "Saving temperature.csv to temperature.csv\n",
            "\n",
            "✓ Uploaded 6 files:\n",
            "  - d13C_5Myr_Cam-Dev.csv\n",
            "  - d13C_stage_binned_Cam-Dev.csv\n",
            "  - DO.csv\n",
            "  - oxygen.csv\n",
            "  - sealevel.csv\n",
            "  - temperature.csv\n",
            "\n",
            "✓ 6 files ready for analysis\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "#@title CELL 5: UPLOAD ENVIRONMENT DATA FILES (Google Colab) - CONDITIONAL\n",
        "# =============================================================================\n",
        "\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Define required files\n",
        "required_files = [\n",
        "    'temperature.csv',\n",
        "    'DO.csv',\n",
        "    'oxygen.csv',\n",
        "    'sealevel.csv',\n",
        "    'd13C_5Myr_Cam-Dev.csv',\n",
        "    'd13C_stage_binned_Cam-Dev.csv'\n",
        "]\n",
        "# Check which files are missing\n",
        "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "\n",
        "if missing_files:\n",
        "    print(\"The following files are missing and need to be uploaded:\")\n",
        "    for i, f in enumerate(missing_files, 1):\n",
        "        print(f\"  {i}. {f}\")\n",
        "    print(\"\\nClick 'Choose Files' and select the missing files...\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    print(f\"\\n✓ Uploaded {len(uploaded)} files:\")\n",
        "    for fn in uploaded.keys():\n",
        "        print(f\"  - {fn}\")\n",
        "else:\n",
        "    print(\"✓ All required files already exist in the folder\")\n",
        "    uploaded = {}\n",
        "    # Load existing files into uploaded dict for compatibility\n",
        "    for f in required_files:\n",
        "        with open(f, 'rb') as file:\n",
        "            uploaded[f] = file.read()\n",
        "\n",
        "print(f\"\\n✓ {len(required_files)} files ready for analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J9_ZRvYbrYT",
        "outputId": "66aa3c1c-947c-4efa-c434-380e95c6a889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Stromatoporoid (Stage)...\n",
            "  ✓ Found: ./pbdb_Stromatoporida_midpoint_stages.csv\n",
            "  ✓ Found: ./pbdb_Labechiida_midpoint_stages.csv\n",
            "  ✓ Found: ./pbdb_Actinostromatida_midpoint_stages.csv\n",
            "  ✓ Found: ./pbdb_Clathrodictyida_midpoint_stages.csv\n",
            "  ✓ Found: ./pbdb_Syringostromatida_midpoint_stages.csv\n",
            "  ✓ Found: ./pbdb_Stromatoporellida_midpoint_stages.csv\n",
            "  ✓ Found: ./pbdb_Amphiporida_midpoint_stages.csv\n",
            "  -> Dropped 11 Stromatoporoid stage(s) with zero occurrences.\n",
            "  -> Merged Stromatoporoid Stages. Rows: 21\n",
            "\n",
            "Processing Coral (Stage)...\n",
            "  ✓ Found: ./pbdb_Tabulata_midpoint_stages.csv\n",
            "  ✓ Found: ./pbdb_Rugosa_midpoint_stages.csv\n",
            "  -> Dropped 11 Coral stage(s) with zero occurrences.\n",
            "  -> Merged Coral Stages. Rows: 21\n",
            "\n",
            "Processing Stromatoporoid (5-Myr)...\n",
            "  ✓ Found: ./pbdb_Stromatoporida_midpoint_5myr_bins.csv\n",
            "  ✓ Found: ./pbdb_Labechiida_midpoint_5myr_bins.csv\n",
            "  ✓ Found: ./pbdb_Actinostromatida_midpoint_5myr_bins.csv\n",
            "  ✓ Found: ./pbdb_Clathrodictyida_midpoint_5myr_bins.csv\n",
            "  ✓ Found: ./pbdb_Syringostromatida_midpoint_5myr_bins.csv\n",
            "  ✓ Found: ./pbdb_Stromatoporellida_midpoint_5myr_bins.csv\n",
            "  ✓ Found: ./pbdb_Amphiporida_midpoint_5myr_bins.csv\n",
            "  -> Dropped 13 Stromatoporoid 5-Myr bin(s) with zero occurrences.\n",
            "  -> Merged Stromatoporoid 5-Myr Bins. Rows: 24\n",
            "\n",
            "Processing Coral (5-Myr)...\n",
            "  ✓ Found: ./pbdb_Tabulata_midpoint_5myr_bins.csv\n",
            "  ✓ Found: ./pbdb_Rugosa_midpoint_5myr_bins.csv\n",
            "  -> Dropped 12 Coral 5-Myr bin(s) with zero occurrences.\n",
            "  -> Merged Coral 5-Myr Bins. Rows: 25\n",
            "\n",
            "Loading Contextual Data...\n",
            "\n",
            "✓ Data loading complete.\n",
            "\n",
            "Saving intermediate merged datasets...\n",
            "✓ Intermediate files saved to ./output\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 6: LOAD AND PROCESS DATA (FROM CONTENT FOLDER)\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Helper: Find file path by keywords in the current directory\n",
        "def get_file_path_robust(keywords, search_dir='.'):\n",
        "    \"\"\"\n",
        "    Finds a filename in the search_dir that contains ALL keywords (case-insensitive).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        files = os.listdir(search_dir)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  ! Error: Directory '{search_dir}' not found.\")\n",
        "        return None\n",
        "\n",
        "    for filename in files:\n",
        "        if not filename.endswith(('.csv', '.xlsx', '.xls')):\n",
        "            continue\n",
        "        # Check if ALL keywords are present in this filename\n",
        "        if all(str(k).lower() in filename.lower() for k in keywords):\n",
        "            return os.path.join(search_dir, filename)\n",
        "    return None\n",
        "\n",
        "def load_and_merge_from_disk(target_list, merge_cols, dataset_name):\n",
        "    \"\"\"\n",
        "    Iterates through a list of target filenames, finds them on disk, and merges them.\n",
        "    \"\"\"\n",
        "    merged_df = None\n",
        "    print(f\"\\nProcessing {dataset_name}...\")\n",
        "\n",
        "    for target in target_list:\n",
        "        # Extract keywords from the target filename\n",
        "        clean_name = target.replace('.csv', '').replace('.xlsx', '')\n",
        "        keywords = [k for k in clean_name.split('_') if k and k.lower() != 'pbdb']\n",
        "\n",
        "        # Search for the file\n",
        "        filepath = get_file_path_robust(keywords)\n",
        "\n",
        "        if filepath:\n",
        "            print(f\"  ✓ Found: {filepath}\")\n",
        "            try:\n",
        "                df = pd.read_csv(filepath)\n",
        "                df.columns = df.columns.str.strip()\n",
        "\n",
        "                if merged_df is None:\n",
        "                    merged_df = df\n",
        "                else:\n",
        "                    merged_df = pd.merge(merged_df, df, on=merge_cols, how='outer')\n",
        "            except Exception as e:\n",
        "                print(f\"  ! Error reading {filepath}: {e}\")\n",
        "        else:\n",
        "            # Retry with minimal keywords (Taxon + Resolution)\n",
        "            # This handles cases where the user filename might differ slightly from the instruction\n",
        "            taxon = next((k for k in keywords if k.lower() not in ['midpoint', 'stages', '5myr', 'bins']), None)\n",
        "            resolution = '5myr' if '5myr' in target.lower() else 'stages'\n",
        "\n",
        "            if taxon:\n",
        "                filepath_retry = get_file_path_robust([taxon, resolution])\n",
        "                if filepath_retry:\n",
        "                    print(f\"  ✓ Found (fallback): {filepath_retry}\")\n",
        "                    try:\n",
        "                        df = pd.read_csv(filepath_retry)\n",
        "                        df.columns = df.columns.str.strip()\n",
        "                        if merged_df is None: merged_df = df\n",
        "                        else: merged_df = pd.merge(merged_df, df, on=merge_cols, how='outer')\n",
        "                    except Exception as e:\n",
        "                        print(f\"  ! Error reading {filepath_retry}: {e}\")\n",
        "                else:\n",
        "                     print(f\"  x Could not find file for: {taxon} ({resolution})\")\n",
        "            else:\n",
        "                 print(f\"  x Could not find file matching: {keywords}\")\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "# Common merge columns\n",
        "stage_merge_cols = ['stage', 'series', 'period', 'start_ma', 'end_ma']\n",
        "bin_merge_cols = ['bin_label', 'bin_top', 'bin_bottom']\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Substitute Stromatoporoid Data (Stage)\n",
        "# =============================================================================\n",
        "strom_stage_targets = [\n",
        "    \"pbdb_Stromatoporida_midpoint_stages.csv\",\n",
        "    \"pbdb_Labechiida_midpoint_stages.csv\",\n",
        "    \"pbdb_Actinostromatida_midpoint_stages.csv\",\n",
        "    \"pbdb_clathrodictyida_midpoint_stages.csv\",\n",
        "    \"pbdb_Syringostromatida_midpoint_stages.csv\",\n",
        "    \"pbdb_Stromatoporellida_midpoint_stages.csv\",\n",
        "    \"pbdb_Amphiporida_midpoint_stages.csv\"\n",
        "]\n",
        "\n",
        "strom_df = load_and_merge_from_disk(strom_stage_targets, stage_merge_cols, \"Stromatoporoid (Stage)\")\n",
        "\n",
        "if strom_df is not None:\n",
        "    strom_df = strom_df.fillna(0)\n",
        "    # Calculate Totals\n",
        "    genus_cols = [c for c in strom_df.columns if c.endswith('_genus')]\n",
        "    occ_cols = [c for c in strom_df.columns if c.endswith('_occ')]\n",
        "    strom_df['Total_genus'] = strom_df[genus_cols].sum(axis=1)\n",
        "    strom_df['Total_occ'] = strom_df[occ_cols].sum(axis=1)\n",
        "\n",
        "    # === FILTER: Remove empty stages ===\n",
        "    dropped_s = len(strom_df[strom_df['Total_occ'] == 0])\n",
        "    strom_df = strom_df[strom_df['Total_occ'] > 0].copy()\n",
        "    if dropped_s > 0:\n",
        "        print(f\"  -> Dropped {dropped_s} Stromatoporoid stage(s) with zero occurrences.\")\n",
        "    # ===================================\n",
        "\n",
        "    print(f\"  -> Merged Stromatoporoid Stages. Rows: {len(strom_df)}\")\n",
        "else:\n",
        "    print(\"  ! Error: Stromatoporoid dataframe is empty.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 2. Substitute Coral Data (Stage)\n",
        "# =============================================================================\n",
        "coral_stage_targets = [\n",
        "    \"pbdb_tabulata_midpoint_stages.csv\",\n",
        "    \"pbdb_Rugosa_midpoint_stages.csv\"\n",
        "]\n",
        "\n",
        "coral_df = load_and_merge_from_disk(coral_stage_targets, stage_merge_cols, \"Coral (Stage)\")\n",
        "\n",
        "if coral_df is not None:\n",
        "    coral_df = coral_df.fillna(0)\n",
        "\n",
        "    # Calculate Totals (Added for filtering)\n",
        "    c_genus_cols = [c for c in coral_df.columns if c.endswith('_genus')]\n",
        "    c_occ_cols = [c for c in coral_df.columns if c.endswith('_occ')]\n",
        "    coral_df['Total_genus'] = coral_df[c_genus_cols].sum(axis=1)\n",
        "    coral_df['Total_occ'] = coral_df[c_occ_cols].sum(axis=1)\n",
        "\n",
        "    # === FILTER: Remove empty stages ===\n",
        "    dropped_c = len(coral_df[coral_df['Total_occ'] == 0])\n",
        "    coral_df = coral_df[coral_df['Total_occ'] > 0].copy()\n",
        "    if dropped_c > 0:\n",
        "        print(f\"  -> Dropped {dropped_c} Coral stage(s) with zero occurrences.\")\n",
        "    # ===================================\n",
        "\n",
        "    print(f\"  -> Merged Coral Stages. Rows: {len(coral_df)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 3. Create New Dataset for 5-Myr Bins\n",
        "# =============================================================================\n",
        "# Stromatoporoids\n",
        "strom_bin_targets = [\n",
        "    \"pbdb_Stromatoporida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Labechiida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Actinostromatida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_clathrodictyida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Syringostromatida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Stromatoporellida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Amphiporida_midpoint_5myr_bins.csv\"\n",
        "]\n",
        "\n",
        "strom_5myr_df = load_and_merge_from_disk(strom_bin_targets, bin_merge_cols, \"Stromatoporoid (5-Myr)\")\n",
        "\n",
        "if strom_5myr_df is not None:\n",
        "    strom_5myr_df = strom_5myr_df.fillna(0)\n",
        "    genus_cols = [c for c in strom_5myr_df.columns if c.endswith('_genus')]\n",
        "    occ_cols = [c for c in strom_5myr_df.columns if c.endswith('_occ')]\n",
        "    strom_5myr_df['Total_genus'] = strom_5myr_df[genus_cols].sum(axis=1)\n",
        "    strom_5myr_df['Total_occ'] = strom_5myr_df[occ_cols].sum(axis=1)\n",
        "\n",
        "    # === FILTER: Remove empty bins ===\n",
        "    dropped_s5 = len(strom_5myr_df[strom_5myr_df['Total_occ'] == 0])\n",
        "    strom_5myr_df = strom_5myr_df[strom_5myr_df['Total_occ'] > 0].copy()\n",
        "    if dropped_s5 > 0:\n",
        "        print(f\"  -> Dropped {dropped_s5} Stromatoporoid 5-Myr bin(s) with zero occurrences.\")\n",
        "    # =================================\n",
        "\n",
        "    print(f\"  -> Merged Stromatoporoid 5-Myr Bins. Rows: {len(strom_5myr_df)}\")\n",
        "\n",
        "# Corals\n",
        "coral_bin_targets = [\n",
        "    \"pbdb_tabulata_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Rugosa_midpoint_5myr_bins.csv\"\n",
        "]\n",
        "coral_5myr_df = load_and_merge_from_disk(coral_bin_targets, bin_merge_cols, \"Coral (5-Myr)\")\n",
        "\n",
        "if coral_5myr_df is not None:\n",
        "    coral_5myr_df = coral_5myr_df.fillna(0)\n",
        "\n",
        "    # Calculate Totals (Added for filtering)\n",
        "    c5_genus_cols = [c for c in coral_5myr_df.columns if c.endswith('_genus')]\n",
        "    c5_occ_cols = [c for c in coral_5myr_df.columns if c.endswith('_occ')]\n",
        "    coral_5myr_df['Total_genus'] = coral_5myr_df[c5_genus_cols].sum(axis=1)\n",
        "    coral_5myr_df['Total_occ'] = coral_5myr_df[c5_occ_cols].sum(axis=1)\n",
        "\n",
        "    # === FILTER: Remove empty bins ===\n",
        "    dropped_c5 = len(coral_5myr_df[coral_5myr_df['Total_occ'] == 0])\n",
        "    coral_5myr_df = coral_5myr_df[coral_5myr_df['Total_occ'] > 0].copy()\n",
        "    if dropped_c5 > 0:\n",
        "        print(f\"  -> Dropped {dropped_c5} Coral 5-Myr bin(s) with zero occurrences.\")\n",
        "    # =================================\n",
        "\n",
        "    print(f\"  -> Merged Coral 5-Myr Bins. Rows: {len(coral_5myr_df)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 4. Load Remaining Contextual Data\n",
        "# =============================================================================\n",
        "print(\"\\nLoading Contextual Data...\")\n",
        "\n",
        "def quick_load(keywords):\n",
        "    path = get_file_path_robust(keywords)\n",
        "    return pd.read_csv(path) if path else pd.DataFrame()\n",
        "\n",
        "reef_df = quick_load([\"reef_data\", \"stage\"])\n",
        "reef_5myr_df = quick_load([\"reef_data\", \"5myr\"])\n",
        "\n",
        "macro_stage = quick_load([\"paleozoic_stage_data\"])\n",
        "if 'stage' in macro_stage.columns: macro_stage['stage'] = macro_stage['stage'].replace('Pridolian', 'Pridoli')\n",
        "\n",
        "macro_5myr = quick_load([\"paleozoic_5myr_data\"])\n",
        "\n",
        "# Environmental\n",
        "env_files = {\n",
        "    'temperature': 'temperature',\n",
        "    'do': 'DO',\n",
        "    'oxygen': 'oxygen',\n",
        "    'sealevel': 'sealevel',\n",
        "    # δ13C files (already binned; DO NOT re-bin/interpolate these)\n",
        "    'd13c_5myr': 'd13C_5Myr_Cam-Dev',\n",
        "    'd13c_stage': 'd13C_stage_binned_Cam-Dev'\n",
        "}\n",
        "env_dfs = {}\n",
        "for var, key in env_files.items():\n",
        "    df = quick_load([key])\n",
        "    if not df.empty: df.columns = df.columns.str.strip().str.replace('\\ufeff', '')\n",
        "    env_dfs[var] = df\n",
        "\n",
        "temp_df = env_dfs.get('temperature', pd.DataFrame())\n",
        "do_df = env_dfs.get('do', pd.DataFrame())\n",
        "oxygen_df = env_dfs.get('oxygen', pd.DataFrame())\n",
        "sealevel_df = env_dfs.get('sealevel', pd.DataFrame())\n",
        "d13c_5myr_df = env_dfs.get('d13c_5myr', pd.DataFrame())\n",
        "d13c_stage_df = env_dfs.get('d13c_stage', pd.DataFrame())\n",
        "print(\"\\n✓ Data loading complete.\")\n",
        "\n",
        "print(\"\\nSaving intermediate merged datasets...\")\n",
        "\n",
        "if 'strom_df' in locals() and strom_df is not None:\n",
        "    strom_df.to_csv(f\"{OUTPUT_DIR}/intermediate_strom_stage.csv\", index=False)\n",
        "if 'coral_df' in locals() and coral_df is not None:\n",
        "    coral_df.to_csv(f\"{OUTPUT_DIR}/intermediate_coral_stage.csv\", index=False)\n",
        "\n",
        "if 'strom_5myr_df' in locals() and strom_5myr_df is not None:\n",
        "    strom_5myr_df.to_csv(f\"{OUTPUT_DIR}/intermediate_strom_5myr.csv\", index=False)\n",
        "if 'coral_5myr_df' in locals() and coral_5myr_df is not None:\n",
        "    coral_5myr_df.to_csv(f\"{OUTPUT_DIR}/intermediate_coral_5myr.csv\", index=False)\n",
        "\n",
        "print(\"✓ Intermediate files saved to ./output\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqhKwRrKbtoq",
        "outputId": "d16b5baf-88bd-45f3-e386-9329680c17ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Constants defined and DataFrames normalized.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 7: DEFINE CONSTANTS AND STAGE INFORMATION\n",
        "# =============================================================================\n",
        "\n",
        "# 1. Stage definitions (ICS 2023/04)\n",
        "# Note: Cambrian stage boundary numerical ages are approximate (~) in the ICS chart.\n",
        "STAGES = {\n",
        "    # Cambrian\n",
        "    'Fortunian':    {'start': 538.8, 'end': 529.0, 'mid': (538.8 + 529.0) / 2, 'period': 'Cambrian'},\n",
        "    'Stage 2':      {'start': 529.0, 'end': 521.0, 'mid': (529.0 + 521.0) / 2, 'period': 'Cambrian'},\n",
        "    'Stage 3':      {'start': 521.0, 'end': 514.0, 'mid': (521.0 + 514.0) / 2, 'period': 'Cambrian'},\n",
        "    'Stage 4':      {'start': 514.0, 'end': 509.0, 'mid': (514.0 + 509.0) / 2, 'period': 'Cambrian'},\n",
        "    'Wuliuan':      {'start': 509.0, 'end': 504.5, 'mid': (509.0 + 504.5) / 2, 'period': 'Cambrian'},\n",
        "    'Drumian':      {'start': 504.5, 'end': 500.5, 'mid': (504.5 + 500.5) / 2, 'period': 'Cambrian'},\n",
        "    'Guzhangian':   {'start': 500.5, 'end': 497.0, 'mid': (500.5 + 497.0) / 2, 'period': 'Cambrian'},\n",
        "    'Paibian':      {'start': 497.0, 'end': 494.0, 'mid': (497.0 + 494.0) / 2, 'period': 'Cambrian'},\n",
        "    'Jiangshanian': {'start': 494.0, 'end': 489.5, 'mid': (494.0 + 489.5) / 2, 'period': 'Cambrian'},\n",
        "    'Stage 10':     {'start': 489.5, 'end': 485.4, 'mid': (489.5 + 485.4) / 2, 'period': 'Cambrian'},\n",
        "\n",
        "    # Ordovician–Devonian\n",
        "    'Tremadocian': {'start': 485.4, 'end': 477.7, 'mid': 481.55, 'period': 'Ordovician'},\n",
        "    'Floian': {'start': 477.7, 'end': 470.0, 'mid': 473.85, 'period': 'Ordovician'},\n",
        "    'Dapingian': {'start': 470.0, 'end': 467.3, 'mid': 468.65, 'period': 'Ordovician'},\n",
        "    'Darriwilian': {'start': 467.3, 'end': 458.4, 'mid': 462.85, 'period': 'Ordovician'},\n",
        "    'Sandbian': {'start': 458.4, 'end': 453.0, 'mid': 455.7, 'period': 'Ordovician'},\n",
        "    'Katian': {'start': 453.0, 'end': 445.2, 'mid': 449.1, 'period': 'Ordovician'},\n",
        "    'Hirnantian': {'start': 445.2, 'end': 443.8, 'mid': 444.5, 'period': 'Ordovician'},\n",
        "    'Rhuddanian': {'start': 443.8, 'end': 440.8, 'mid': 442.3, 'period': 'Silurian'},\n",
        "    'Aeronian': {'start': 440.8, 'end': 438.5, 'mid': 439.65, 'period': 'Silurian'},\n",
        "    'Telychian': {'start': 438.5, 'end': 433.4, 'mid': 435.95, 'period': 'Silurian'},\n",
        "    'Sheinwoodian': {'start': 433.4, 'end': 430.5, 'mid': 431.95, 'period': 'Silurian'},\n",
        "    'Homerian': {'start': 430.5, 'end': 427.4, 'mid': 428.95, 'period': 'Silurian'},\n",
        "    'Gorstian': {'start': 427.4, 'end': 425.6, 'mid': 426.5, 'period': 'Silurian'},\n",
        "    'Ludfordian': {'start': 425.6, 'end': 423.0, 'mid': 424.3, 'period': 'Silurian'},\n",
        "    'Pridoli': {'start': 423.0, 'end': 419.2, 'mid': 421.1, 'period': 'Silurian'},\n",
        "    'Lochkovian': {'start': 419.2, 'end': 410.8, 'mid': 415.0, 'period': 'Devonian'},\n",
        "    'Pragian': {'start': 410.8, 'end': 407.6, 'mid': 409.2, 'period': 'Devonian'},\n",
        "    'Emsian': {'start': 407.6, 'end': 393.3, 'mid': 400.45, 'period': 'Devonian'},\n",
        "    'Eifelian': {'start': 393.3, 'end': 387.7, 'mid': 390.5, 'period': 'Devonian'},\n",
        "    'Givetian': {'start': 387.7, 'end': 382.7, 'mid': 385.2, 'period': 'Devonian'},\n",
        "    'Frasnian': {'start': 382.7, 'end': 372.2, 'mid': 377.45, 'period': 'Devonian'},\n",
        "    'Famennian': {'start': 372.2, 'end': 358.9, 'mid': 365.55, 'period': 'Devonian'}\n",
        "}\n",
        "STAGE_ORDER = list(STAGES.keys())\n",
        "\n",
        "# 2. Period colors (ICS standard)\n",
        "PERIOD_COLORS = {\n",
        "    'Cambrian': '#7FA056',\n",
        "    'Ordovician': '#009270',\n",
        "    'Silurian': '#B3E1B6',\n",
        "    'Devonian': '#CB8C37'\n",
        "}\n",
        "\n",
        "# 3. Stromatoporoid order colors (phylogenetically informed)\n",
        "STROM_COLORS = {\n",
        "    'Labechiida': '#8B0000',       # Dark red - basal\n",
        "    'Clathrodictyida': '#CD5C5C',  # Indian red - early-diverging\n",
        "    'Actinostromatida': '#FF8C00', # Dark orange - derived\n",
        "    'Stromatoporida': '#FFD700',   # Gold - derived\n",
        "    'Stromatoporellida': '#32CD32',# Lime green - derived\n",
        "    'Syringostromatida': '#4169E1',# Royal blue - derived reef builders\n",
        "    'Amphiporida': '#9370DB'       # Medium purple - derived\n",
        "}\n",
        "STROM_ORDERS = ['Labechiida', 'Clathrodictyida', 'Actinostromatida',\n",
        "                'Stromatoporida', 'Stromatoporellida', 'Syringostromatida', 'Amphiporida']\n",
        "\n",
        "# 4. Coral colors (New definitions for Rugosa/Tabulata)\n",
        "CORAL_COLORS = {\n",
        "    'Rugosa': '#800080',    # Purple\n",
        "    'Tabulata': '#D2691E'   # Chocolate/Orange-Brown\n",
        "}\n",
        "CORAL_GROUPS = ['Rugosa', 'Tabulata']\n",
        "\n",
        "# =============================================================================\n",
        "# DATA NORMALIZATION: Ensure DataFrame columns match capitalized constants\n",
        "# =============================================================================\n",
        "# Some files were lowercase (e.g., 'clathrodictyida'), but constants are TitleCase.\n",
        "# We fix this here to prevent KeyErrors in future plotting cells.\n",
        "\n",
        "def normalize_columns(df, target_orders):\n",
        "    if df is None: return df\n",
        "\n",
        "    # Get current columns\n",
        "    cols = df.columns.tolist()\n",
        "    rename_map = {}\n",
        "\n",
        "    for order in target_orders:\n",
        "        # Check if TitleCase version exists (e.g., 'Clathrodictyida_genus')\n",
        "        title_genus = f\"{order}_genus\"\n",
        "        title_occ = f\"{order}_occ\"\n",
        "\n",
        "        # Check if LowerCase version exists (e.g., 'clathrodictyida_genus')\n",
        "        lower_genus = f\"{order.lower()}_genus\"\n",
        "        lower_occ = f\"{order.lower()}_occ\"\n",
        "\n",
        "        # If TitleCase missing but LowerCase present, map Lower -> Title\n",
        "        if title_genus not in cols and lower_genus in cols:\n",
        "            rename_map[lower_genus] = title_genus\n",
        "        if title_occ not in cols and lower_occ in cols:\n",
        "            rename_map[lower_occ] = title_occ\n",
        "\n",
        "        # Also handle \"tabulata\" (lowercase t)\n",
        "        if order == 'Tabulata' and 'tabulata_genus' in cols:\n",
        "             rename_map['tabulata_genus'] = 'Tabulata_genus'\n",
        "             rename_map['tabulata_occ'] = 'Tabulata_occ'\n",
        "\n",
        "    if rename_map:\n",
        "        print(f\"  Note: Renaming columns to match standard capitalization: {list(rename_map.keys())}\")\n",
        "        df = df.rename(columns=rename_map)\n",
        "    return df\n",
        "\n",
        "# Apply normalization to the datasets loaded in Cell 5\n",
        "if 'strom_df' in locals(): strom_df = normalize_columns(strom_df, STROM_ORDERS)\n",
        "if 'strom_5myr_df' in locals(): strom_5myr_df = normalize_columns(strom_5myr_df, STROM_ORDERS)\n",
        "if 'coral_df' in locals(): coral_df = normalize_columns(coral_df, CORAL_GROUPS)\n",
        "if 'coral_5myr_df' in locals(): coral_5myr_df = normalize_columns(coral_5myr_df, CORAL_GROUPS)\n",
        "\n",
        "print(\"✓ Constants defined and DataFrames normalized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mRadY5BbvJm",
        "outputId": "faf93269-c510-4738-df99-cf6eec018f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading environmental datasets...\n",
            "  ✓ Loaded temperature.csv (387 rows)\n",
            "  ✓ Loaded DO.csv (499 rows)\n",
            "  ✓ Loaded oxygen.csv (58 rows)\n",
            "  ✓ Loaded sealevel.csv (101 rows)\n",
            "\n",
            "Interpolating environmental proxies to stage midpoints...\n",
            "  -> δ13C (stage-binned) merged: 32/32 values\n",
            "\n",
            "Interpolating environmental proxies to 5-Myr bin midpoints...\n",
            "  -> δ13C (5-Myr binned) merged: 37/37 values\n",
            "✓ Environmental proxies interpolated (Stage & 5-Myr).\n",
            "✓ Interpolated environmental data saved to ./output\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 8: RELOAD ENV DATA & INTERPOLATE (ROBUST FIX)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.interpolate import interp1d\n",
        "import os\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. RELOAD ENVIRONMENTAL DATA (To ensure it is not empty)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"Reloading environmental datasets...\")\n",
        "\n",
        "def load_env_file(filenames, target_cols):\n",
        "    \"\"\"Try to load a file from a list of possible names\"\"\"\n",
        "    for fname in filenames:\n",
        "        if os.path.exists(fname):\n",
        "            try:\n",
        "                df = pd.read_csv(fname)\n",
        "                df.columns = df.columns.str.strip() # clean whitespace\n",
        "                print(f\"  ✓ Loaded {fname} ({len(df)} rows)\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                print(f\"  ! Error loading {fname}: {e}\")\n",
        "    print(f\"  ! WARNING: Could not find any of {filenames}\")\n",
        "    return pd.DataFrame() # Return empty if not found\n",
        "\n",
        "# Load with specific fallbacks\n",
        "temp_df     = load_env_file(['temperature.csv', 'Temperature.csv'], ['Age', 'SST'])\n",
        "do_df       = load_env_file(['DO.csv', 'do.csv'], ['Age', 'DO'])\n",
        "oxygen_df   = load_env_file(['oxygen.csv', 'Oxygen.csv'], ['Age', 'O2'])\n",
        "sealevel_df = load_env_file(['sealevel.csv', 'Sealevel.csv'], ['Age', 'Eustatic'])\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. STANDARDIZE COLUMNS\n",
        "# -----------------------------------------------------------------------------\n",
        "def standardize_env_columns(df, name, target_age='Age', target_val=None):\n",
        "    if df.empty: return df\n",
        "\n",
        "    # Fix Age column\n",
        "    if target_age not in df.columns:\n",
        "        for candidate in ['age', 'AGE', 'Time', 'Ma', 'time']:\n",
        "            if candidate in df.columns:\n",
        "                df = df.rename(columns={candidate: target_age})\n",
        "                break\n",
        "\n",
        "    # Fix Value column\n",
        "    if target_val and target_val not in df.columns:\n",
        "        # Check case-insensitive match\n",
        "        for col in df.columns:\n",
        "            if col.lower() == target_val.lower():\n",
        "                df = df.rename(columns={col: target_val})\n",
        "                break\n",
        "\n",
        "    return df\n",
        "\n",
        "do_df = standardize_env_columns(do_df, 'Dissolved Oxygen', target_val='DO')\n",
        "temp_df = standardize_env_columns(temp_df, 'Temperature', target_val='SST')\n",
        "oxygen_df = standardize_env_columns(oxygen_df, 'Atmosphere', target_age='Age')\n",
        "sealevel_df = standardize_env_columns(sealevel_df, 'Sea Level', target_val='Eustatic Sea Level')\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. INTERPOLATION\n",
        "# -----------------------------------------------------------------------------\n",
        "def interpolate_to_ages(source_df, age_col, value_col, target_ages):\n",
        "    \"\"\"Interpolate values to target ages, skipping if columns missing.\"\"\"\n",
        "    # Check if empty or missing columns\n",
        "    if source_df.empty or age_col not in source_df.columns or value_col not in source_df.columns:\n",
        "        return np.full_like(target_ages, np.nan)\n",
        "\n",
        "    # Drop NaNs in source\n",
        "    source_df = source_df.dropna(subset=[age_col, value_col])\n",
        "    # Sort by Age (Crucial for interp1d)\n",
        "    source_df = source_df.sort_values(age_col)\n",
        "\n",
        "    if len(source_df) < 2:\n",
        "        return np.full_like(target_ages, np.nan)\n",
        "\n",
        "    # Interpolate\n",
        "    f = interp1d(source_df[age_col], source_df[value_col],\n",
        "                 kind='linear', fill_value='extrapolate', bounds_error=False)\n",
        "    return f(target_ages)\n",
        "\n",
        "# Get Stage Midpoints from Cell 7 constants\n",
        "stage_midpoints = np.array([STAGES[s]['mid'] for s in STAGE_ORDER])\n",
        "\n",
        "print(\"\\nInterpolating environmental proxies to stage midpoints...\")\n",
        "env_data = pd.DataFrame({'stage': STAGE_ORDER, 'midpoint_ma': stage_midpoints})\n",
        "\n",
        "env_data['temperature']     = interpolate_to_ages(temp_df, 'Age', 'SST', stage_midpoints)\n",
        "env_data['dissolved_O2']    = interpolate_to_ages(do_df, 'Age', 'DO', stage_midpoints)\n",
        "env_data['atm_O2']          = interpolate_to_ages(oxygen_df, 'Age', 'O2', stage_midpoints)\n",
        "env_data['atm_CO2']         = interpolate_to_ages(oxygen_df, 'Age', 'CO2', stage_midpoints)\n",
        "env_data['sea_level']       = interpolate_to_ages(sealevel_df, 'Age', 'Eustatic Sea Level', stage_midpoints)\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# MERGE δ13C (STAGE-BINNED) — prefer Stage-name join; fallback to age-nearest\n",
        "# Uses attached d13C_stage_binned_Cam-Dev.csv WITHOUT re-binning/conversion.\n",
        "# -----------------------------------------------------------------------------\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "def _norm_stage(s):\n",
        "    if pd.isna(s):\n",
        "        return np.nan\n",
        "    s = str(s).strip().lower()\n",
        "    # remove spaces/punct to survive minor naming differences\n",
        "    return re.sub(r'[^a-z0-9]+', '', s)\n",
        "\n",
        "try:\n",
        "    # Load attached stage-binned δ13C table\n",
        "    cand = [\n",
        "        Path(\"d13C_stage_binned_Cam-Dev.csv\"),\n",
        "        Path(\"/mnt/data/d13C_stage_binned_Cam-Dev.csv\")\n",
        "    ]\n",
        "    f = next((p for p in cand if p.exists()), None)\n",
        "    if f is None:\n",
        "        raise FileNotFoundError(\"d13C_stage_binned_Cam-Dev.csv not found in working dir or /mnt/data\")\n",
        "\n",
        "    d13_stage = pd.read_csv(f)\n",
        "    d13_stage.columns = d13_stage.columns.str.strip().str.replace('\\ufeff', '')\n",
        "\n",
        "    # Required columns in attached file\n",
        "    if \"Stage\" not in d13_stage.columns or \"Mid_Ma\" not in d13_stage.columns or \"d13C_mean\" not in d13_stage.columns:\n",
        "        raise ValueError(\"δ13C stage file must have columns: Stage, Mid_Ma, d13C_mean\")\n",
        "\n",
        "    # Clean numeric + stage keys\n",
        "    d13_stage[\"Mid_Ma\"] = pd.to_numeric(d13_stage[\"Mid_Ma\"], errors=\"coerce\")\n",
        "    d13_stage[\"d13C_mean\"] = pd.to_numeric(d13_stage[\"d13C_mean\"], errors=\"coerce\")\n",
        "    d13_stage[\"stage_key\"] = d13_stage[\"Stage\"].apply(_norm_stage)\n",
        "\n",
        "    env_data[\"midpoint_ma\"] = pd.to_numeric(env_data[\"midpoint_ma\"], errors=\"coerce\")\n",
        "\n",
        "    # --- 1) Stage-name merge (best) ---\n",
        "    if \"stage\" in env_data.columns:\n",
        "        env_data[\"stage_key\"] = env_data[\"stage\"].apply(_norm_stage)\n",
        "\n",
        "        _m1 = env_data.merge(\n",
        "            d13_stage[[\"stage_key\", \"d13C_mean\", \"Mid_Ma\"]],\n",
        "            on=\"stage_key\",\n",
        "            how=\"left\",\n",
        "            suffixes=(\"\", \"_d13\")\n",
        "        )\n",
        "\n",
        "        # Keep δ13C as a single downstream name\n",
        "        _m1[\"d13C\"] = _m1[\"d13C_mean\"]\n",
        "\n",
        "        # --- 2) Fallback: for unmatched stages, fill by age-nearest ---\n",
        "        missing = _m1[\"d13C\"].isna() & _m1[\"midpoint_ma\"].notna()\n",
        "        if missing.any():\n",
        "            _left = _m1.loc[missing, [\"midpoint_ma\"]].copy().sort_values(\"midpoint_ma\")\n",
        "            _right = d13_stage[[\"Mid_Ma\", \"d13C_mean\"]].dropna().sort_values(\"Mid_Ma\")\n",
        "\n",
        "            _fill = pd.merge_asof(\n",
        "                _left,\n",
        "                _right,\n",
        "                left_on=\"midpoint_ma\",\n",
        "                right_on=\"Mid_Ma\",\n",
        "                direction=\"nearest\",\n",
        "                tolerance=2.0  # stage midpoints can differ by >0.25; allow reasonable slack\n",
        "            )\n",
        "            _m1.loc[missing, \"d13C\"] = _fill[\"d13C_mean\"].values\n",
        "\n",
        "        # Cleanup\n",
        "        env_data = _m1.drop(columns=[c for c in [\"d13C_mean\", \"Mid_Ma\", \"stage_key\"] if c in _m1.columns])\n",
        "        env_data = env_data.sort_values(\"midpoint_ma\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    else:\n",
        "        # If env_data has no stage column, do age-nearest only (more tolerant)\n",
        "        _left = env_data.dropna(subset=[\"midpoint_ma\"]).sort_values(\"midpoint_ma\")\n",
        "        _right = d13_stage[[\"Mid_Ma\", \"d13C_mean\"]].dropna().sort_values(\"Mid_Ma\")\n",
        "        _m = pd.merge_asof(_left, _right, left_on=\"midpoint_ma\", right_on=\"Mid_Ma\", direction=\"nearest\", tolerance=2.0)\n",
        "        env_data = _m.drop(columns=[\"Mid_Ma\"]).rename(columns={\"d13C_mean\": \"d13C\"})\n",
        "        env_data = env_data.sort_values(\"midpoint_ma\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    n_valid = int(env_data[\"d13C\"].notna().sum()) if \"d13C\" in env_data.columns else 0\n",
        "    print(f\"  -> δ13C (stage-binned) merged: {n_valid}/{len(env_data)} values\")\n",
        "\n",
        "except Exception as e:\n",
        "    env_data[\"d13C\"] = np.nan\n",
        "    print(\"  ! Warning: δ13C stage merge failed:\", e)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. INTERPOLATE TO 5-MYR BINS\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nInterpolating environmental proxies to 5-Myr bin midpoints...\")\n",
        "\n",
        "# Define bins (Logic from previous step)\n",
        "if 'reef_5myr_df' in locals() and not reef_5myr_df.empty:\n",
        "    bin_midpoints_5myr = reef_5myr_df['midpoint_ma'].values\n",
        "    bin_ids = reef_5myr_df['time_identifier']\n",
        "elif 'strom_5myr_df' in locals() and not strom_5myr_df.empty:\n",
        "    # Recalculate if column missing\n",
        "    if 'midpoint_ma' not in strom_5myr_df.columns:\n",
        "        strom_5myr_df['midpoint_ma'] = (strom_5myr_df['bin_top'] + strom_5myr_df['bin_bottom']) / 2\n",
        "    bin_midpoints_5myr = strom_5myr_df['midpoint_ma'].values\n",
        "    bin_ids = strom_5myr_df['bin_label']\n",
        "else:\n",
        "    # Fallback\n",
        "    bin_midpoints_5myr = np.arange(482.5, 359, -5)\n",
        "    bin_ids = [f\"{x}\" for x in bin_midpoints_5myr]\n",
        "\n",
        "env_data_5myr = pd.DataFrame({'bin_id': bin_ids, 'midpoint_ma': bin_midpoints_5myr})\n",
        "\n",
        "env_data_5myr['temperature']     = interpolate_to_ages(temp_df, 'Age', 'SST', bin_midpoints_5myr)\n",
        "env_data_5myr['dissolved_O2']    = interpolate_to_ages(do_df, 'Age', 'DO', bin_midpoints_5myr)\n",
        "env_data_5myr['atm_O2']          = interpolate_to_ages(oxygen_df, 'Age', 'O2', bin_midpoints_5myr)\n",
        "env_data_5myr['atm_CO2']         = interpolate_to_ages(oxygen_df, 'Age', 'CO2', bin_midpoints_5myr)\n",
        "env_data_5myr['sea_level']       = interpolate_to_ages(sealevel_df, 'Age', 'Eustatic Sea Level', bin_midpoints_5myr)\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4b. MERGE δ13C (5-MYR BINNED) WITHOUT INTERPOLATION / RE-BINNING\n",
        "#   Float bin midpoints often differ by tiny rounding; if exact merge yields\n",
        "#   few/zero matches, fall back to merge_asof (requires ascending sort).\n",
        "# -----------------------------------------------------------------------------\n",
        "try:\n",
        "    if 'd13c_5myr_df' in globals() and isinstance(d13c_5myr_df, pd.DataFrame) and (not d13c_5myr_df.empty):\n",
        "        _d13_5 = d13c_5myr_df.copy()\n",
        "    else:\n",
        "        cand = [\n",
        "            'd13C_5Myr_Cam-Dev.csv',\n",
        "            './output/d13C_5Myr_Cam-Dev.csv',\n",
        "            'd13C_5Myr.csv',\n",
        "            './output/d13C_5Myr.csv'\n",
        "        ]\n",
        "        _d13_5 = None\n",
        "        for p in cand:\n",
        "            if os.path.exists(p):\n",
        "                _d13_5 = pd.read_csv(p)\n",
        "                break\n",
        "        if _d13_5 is None:\n",
        "            raise FileNotFoundError(\"No 5-Myr δ13C file found (tried: \" + \", \".join(cand) + \").\")\n",
        "\n",
        "    _d13_5.columns = _d13_5.columns.str.strip().str.replace('\\ufeff', '')\n",
        "\n",
        "    # Standardize columns\n",
        "    if 'age_Ma' in _d13_5.columns and 'midpoint_ma' not in _d13_5.columns:\n",
        "        _d13_5 = _d13_5.rename(columns={'age_Ma': 'midpoint_ma'})\n",
        "    if 'Mid_Ma' in _d13_5.columns and 'midpoint_ma' not in _d13_5.columns:\n",
        "        _d13_5 = _d13_5.rename(columns={'Mid_Ma': 'midpoint_ma'})\n",
        "    if 'd13Ccarb_permille' in _d13_5.columns and 'd13C' not in _d13_5.columns:\n",
        "        _d13_5 = _d13_5.rename(columns={'d13Ccarb_permille': 'd13C'})\n",
        "    if 'd13C_mean' in _d13_5.columns and 'd13C' not in _d13_5.columns:\n",
        "        _d13_5 = _d13_5.rename(columns={'d13C_mean': 'd13C'})\n",
        "\n",
        "    _d13_5['midpoint_ma'] = pd.to_numeric(_d13_5['midpoint_ma'], errors='coerce')\n",
        "    _d13_5['d13C'] = pd.to_numeric(_d13_5['d13C'], errors='coerce')\n",
        "    _d13_5 = _d13_5.dropna(subset=['midpoint_ma', 'd13C']).copy()\n",
        "\n",
        "    # --- Attempt exact merge after rounding to reduce floating mismatch\n",
        "    env_data_5myr['midpoint_ma'] = pd.to_numeric(env_data_5myr['midpoint_ma'], errors='coerce')\n",
        "    _left = env_data_5myr.copy()\n",
        "    _left['midpoint_ma_round'] = _left['midpoint_ma'].round(3)\n",
        "    _right = _d13_5[['midpoint_ma', 'd13C']].copy()\n",
        "    _right['midpoint_ma_round'] = _right['midpoint_ma'].round(3)\n",
        "\n",
        "    env_data_5myr = _left.merge(_right[['midpoint_ma_round', 'd13C']], on='midpoint_ma_round', how='left')\n",
        "    env_data_5myr = env_data_5myr.drop(columns=['midpoint_ma_round'])\n",
        "\n",
        "    n_valid = env_data_5myr['d13C'].notna().sum()\n",
        "\n",
        "    # --- Fallback: nearest-age join if exact merge fails\n",
        "    if n_valid == 0 and len(_d13_5) > 0:\n",
        "        _left_sorted = env_data_5myr.drop(columns=['d13C'], errors='ignore').copy()\n",
        "        _left_sorted = _left_sorted.dropna(subset=['midpoint_ma']).sort_values('midpoint_ma', ascending=True).reset_index(drop=True)\n",
        "\n",
        "        _right_sorted = _d13_5[['midpoint_ma', 'd13C']].sort_values('midpoint_ma', ascending=True).reset_index(drop=True)\n",
        "\n",
        "        _m = pd.merge_asof(\n",
        "            _left_sorted,\n",
        "            _right_sorted,\n",
        "            on='midpoint_ma',\n",
        "            direction='nearest',\n",
        "            tolerance=2.6  # ~half of 5-Myr bin width\n",
        "        )\n",
        "        env_data_5myr = _m.sort_values('midpoint_ma', ascending=False).reset_index(drop=True)\n",
        "        n_valid = env_data_5myr['d13C'].notna().sum()\n",
        "\n",
        "    print(f\"  -> δ13C (5-Myr binned) merged: {n_valid}/{len(env_data_5myr)} values\")\n",
        "\n",
        "except Exception as e:\n",
        "    env_data_5myr['d13C'] = np.nan\n",
        "    print(\"  ! Warning: δ13C 5-Myr merge failed:\", e)\n",
        "\n",
        "print(\"✓ Environmental proxies interpolated (Stage & 5-Myr).\")\n",
        "\n",
        "# =============================================================================\n",
        "# [ADDED] SAVE INTERPOLATED ENV DATA\n",
        "# =============================================================================\n",
        "if 'env_data' in locals() and not env_data.empty:\n",
        "    env_data.to_csv(f\"{OUTPUT_DIR}/intermediate_env_data_stage.csv\", index=False)\n",
        "\n",
        "if 'env_data_5myr' in locals() and not env_data_5myr.empty:\n",
        "    env_data_5myr.to_csv(f\"{OUTPUT_DIR}/intermediate_env_data_5myr.csv\", index=False)\n",
        "\n",
        "print(f\"✓ Interpolated environmental data saved to {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2BtkuBab2Kp",
        "outputId": "058ec8c4-5411-45f8-a0fc-68bb19135ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Master Stage Dataset...\n",
            "✓ Master dataset (STAGES) created: 32 stages, 56 variables\n",
            "\n",
            "======================================================================\n",
            "CREATING 5-MYR BIN MASTER DATASET\n",
            "======================================================================\n",
            "Using Reef Data as primary 5-Myr bin source.\n",
            "✓ Master dataset (5-MYR BINS) created: 37 bins, 39 variables\n",
            "✓ Saved: ./output/MASTER_dataset_stage.csv\n",
            "✓ Saved: ./output/MASTER_dataset_5myr.csv\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 9: CREATE MASTER DATASET (STAGES AND 5-MYR BINS)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. BUILD MASTER STAGE DATASET\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"Building Master Stage Dataset...\")\n",
        "data = []\n",
        "\n",
        "for stage, info in STAGES.items():\n",
        "    row = {\n",
        "        'stage': stage,\n",
        "        'midpoint_ma': info['mid'],\n",
        "        'start_ma': info['start'],\n",
        "        'end_ma': info['end'],\n",
        "        'period': info['period']\n",
        "    }\n",
        "\n",
        "# A. Reef data (if available)\n",
        "    if 'reef_df' in locals() and not reef_df.empty:\n",
        "        reef_row = reef_df[reef_df['name'] == stage]\n",
        "        if len(reef_row) > 0:\n",
        "            for col in [\n",
        "                'thickness_mean', 'thickness_std', 'thickness_stderr', 'thickness_median',\n",
        "                'thickness_min', 'thickness_max', 'thickness_q25', 'thickness_q75', 'thickness_count',\n",
        "                'width_mean', 'width_std', 'width_median', 'width_min', 'width_max',\n",
        "                'reef_count'\n",
        "            ]:\n",
        "                if col in reef_row.columns:\n",
        "                    row[col] = reef_row[col].values[0]\n",
        "\n",
        "    # B. Stromatoporoid data (from strom_df)\n",
        "    if 'strom_df' in locals() and not strom_df.empty:\n",
        "        # Check column name (case insensitive)\n",
        "        strom_row = strom_df[strom_df['stage'].str.lower() == stage.lower()]\n",
        "        if len(strom_row) > 0:\n",
        "            for order in STROM_ORDERS:\n",
        "                col_occ = f'{order}_occ'\n",
        "                col_gen = f'{order}_genus'\n",
        "                # Check actual columns (constants are TitleCase, data is normalized in Cell 7)\n",
        "                if col_occ in strom_row.columns:\n",
        "                    row[col_occ] = strom_row[col_occ].values[0]\n",
        "                if col_gen in strom_row.columns:\n",
        "                    row[col_gen] = strom_row[col_gen].values[0]\n",
        "\n",
        "            if 'Total_occ' in strom_row.columns:\n",
        "                row['strom_total_occ'] = strom_row['Total_occ'].values[0]\n",
        "            if 'Total_genus' in strom_row.columns:\n",
        "                row['strom_total_gen'] = strom_row['Total_genus'].values[0]\n",
        "\n",
        "    # C. Coral data (from coral_df)\n",
        "    if 'coral_df' in locals() and not coral_df.empty:\n",
        "        coral_row = coral_df[coral_df['stage'].str.lower() == stage.lower()]\n",
        "        if len(coral_row) > 0:\n",
        "            # Rugosa\n",
        "            if 'Rugosa_genus' in coral_row.columns: row['rugose_div'] = coral_row['Rugosa_genus'].values[0]\n",
        "            if 'Rugosa_occ' in coral_row.columns:   row['rugose_occ'] = coral_row['Rugosa_occ'].values[0]\n",
        "\n",
        "            # Tabulata\n",
        "            if 'Tabulata_genus' in coral_row.columns: row['tabulate_div'] = coral_row['Tabulata_genus'].values[0]\n",
        "            if 'Tabulata_occ' in coral_row.columns:   row['tabulate_occ'] = coral_row['Tabulata_occ'].values[0]\n",
        "\n",
        "    # D. Macrostrat data\n",
        "    if 'macro_stage' in locals() and not macro_stage.empty:\n",
        "        macro_row = macro_stage[macro_stage['stage'] == stage]\n",
        "        if len(macro_row) > 0:\n",
        "            row['total_area_km2'] = macro_row['total_area_km2'].values[0]\n",
        "            row['carbonate_area_km2'] = macro_row['carbonate_area_km2'].values[0]\n",
        "            row['carbonate_percentage'] = macro_row['carbonate_percentage'].values[0]\n",
        "\n",
        "    # E. Environmental proxies\n",
        "    if 'env_data' in locals() and not env_data.empty:\n",
        "        env_row = env_data[env_data['stage'] == stage]\n",
        "        if len(env_row) > 0:\n",
        "            row['temperature'] = env_row['temperature'].values[0]\n",
        "            row['dissolved_O2'] = env_row['dissolved_O2'].values[0]\n",
        "            row['atm_O2'] = env_row['atm_O2'].values[0]\n",
        "            row['atm_CO2'] = env_row['atm_CO2'].values[0]\n",
        "            row['sea_level'] = env_row['sea_level'].values[0]\n",
        "            if 'd13C' in env_row.columns: row['d13C'] = env_row['d13C'].values[0]\n",
        "\n",
        "    data.append(row)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate Stromatoporoid Proportions\n",
        "if 'strom_total_occ' in df.columns:\n",
        "    for order in STROM_ORDERS:\n",
        "        col_occ = f'{order}_occ'\n",
        "        if col_occ in df.columns:\n",
        "            df[f'{order}_prop'] = np.where(\n",
        "                df['strom_total_occ'] > 0,\n",
        "                df[col_occ].fillna(0) / df['strom_total_occ'],\n",
        "                0.0\n",
        "            )\n",
        "\n",
        "    # Derived vs Basal\n",
        "    derived_orders = ['Actinostromatida', 'Stromatoporida', 'Stromatoporellida',\n",
        "                      'Syringostromatida', 'Amphiporida']\n",
        "    basal_orders = ['Labechiida', 'Clathrodictyida']\n",
        "\n",
        "    # Occurrences\n",
        "    df['derived_strom_occ'] = sum(df[f'{o}_occ'].fillna(0) for o in derived_orders if f'{o}_occ' in df.columns)\n",
        "    df['basal_strom_occ'] = sum(df[f'{o}_occ'].fillna(0) for o in basal_orders if f'{o}_occ' in df.columns)\n",
        "\n",
        "    # Diversity\n",
        "    df['derived_strom_div'] = sum(df[f'{o}_genus'].fillna(0) for o in derived_orders if f'{o}_genus' in df.columns)\n",
        "    df['basal_strom_div'] = sum(df[f'{o}_genus'].fillna(0) for o in basal_orders if f'{o}_genus' in df.columns)\n",
        "\n",
        "    # Proportions\n",
        "    df['derived_strom_prop'] = np.where(df['strom_total_occ'] > 0, df['derived_strom_occ'] / df['strom_total_occ'], 0.0)\n",
        "    df['basal_strom_prop'] = np.where(df['strom_total_occ'] > 0, df['basal_strom_occ'] / df['strom_total_occ'], 0.0)\n",
        "\n",
        "# Sort\n",
        "df = df.sort_values('midpoint_ma', ascending=False).reset_index(drop=True)\n",
        "print(f\"✓ Master dataset (STAGES) created: {len(df)} stages, {len(df.columns)} variables\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. BUILD MASTER 5-MYR DATASET\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING 5-MYR BIN MASTER DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Determine the primary source for 5-Myr bins\n",
        "# We prefer reef data if available, otherwise we use the biological data\n",
        "if 'reef_5myr_df' in locals() and not reef_5myr_df.empty:\n",
        "    primary_bins = reef_5myr_df\n",
        "    print(\"Using Reef Data as primary 5-Myr bin source.\")\n",
        "elif 'strom_5myr_df' in locals() and not strom_5myr_df.empty:\n",
        "    primary_bins = strom_5myr_df\n",
        "    # Add midpoint if missing\n",
        "    if 'midpoint_ma' not in primary_bins.columns:\n",
        "        primary_bins['midpoint_ma'] = (primary_bins['bin_top'] + primary_bins['bin_bottom']) / 2\n",
        "    print(\"Using Stromatoporoid Data as primary 5-Myr bin source.\")\n",
        "else:\n",
        "    # Fallback to env_data_5myr if bio data missing\n",
        "    primary_bins = env_data_5myr\n",
        "    print(\"Using Environmental Data as primary 5-Myr bin source.\")\n",
        "\n",
        "data_5myr = []\n",
        "\n",
        "# Iterate through the chosen primary bins\n",
        "for idx, row_ref in primary_bins.iterrows():\n",
        "    # Identify bin\n",
        "    if 'time_identifier' in row_ref: bin_id = row_ref['time_identifier']\n",
        "    elif 'bin_label' in row_ref: bin_id = row_ref['bin_label']\n",
        "    elif 'bin_id' in row_ref: bin_id = row_ref['bin_id']\n",
        "    else: bin_id = idx # fallback\n",
        "\n",
        "    midpoint = row_ref['midpoint_ma']\n",
        "\n",
        "    row = {\n",
        "        'bin_id': bin_id,\n",
        "        'midpoint_ma': midpoint\n",
        "    }\n",
        "\n",
        "    # A. Reef Data (if available)\n",
        "    if 'reef_5myr_df' in locals() and not reef_5myr_df.empty:\n",
        "        # Find matching reef row (if not already iterating it)\n",
        "        if primary_bins is not reef_5myr_df:\n",
        "            # Match by midpoint proximity (float comparison)\n",
        "            reef_match = reef_5myr_df[abs(reef_5myr_df['midpoint_ma'] - midpoint) < 0.1]\n",
        "            if len(reef_match) > 0:\n",
        "                row_ref_for_reef = reef_match.iloc[0]\n",
        "            else:\n",
        "                row_ref_for_reef = pd.Series()\n",
        "        else:\n",
        "            row_ref_for_reef = row_ref\n",
        "\n",
        "        for col in ['thickness_mean', 'thickness_std', 'thickness_stderr', 'thickness_median',\n",
        "                    'thickness_count', 'width_mean', 'width_std', 'reef_count']:\n",
        "            if col in row_ref_for_reef.index:\n",
        "                row[col] = row_ref_for_reef[col]\n",
        "\n",
        "    # B. Stromatoporoid Data (5-Myr) - NOW INCLUDED\n",
        "    if 'strom_5myr_df' in locals() and not strom_5myr_df.empty:\n",
        "        # Match by midpoint\n",
        "        strom_match = strom_5myr_df[abs(((strom_5myr_df['bin_top'] + strom_5myr_df['bin_bottom'])/2) - midpoint) < 0.1]\n",
        "        if len(strom_match) > 0:\n",
        "            s_row = strom_match.iloc[0]\n",
        "            for order in STROM_ORDERS:\n",
        "                if f'{order}_occ' in s_row: row[f'{order}_occ'] = s_row[f'{order}_occ']\n",
        "                if f'{order}_genus' in s_row: row[f'{order}_genus'] = s_row[f'{order}_genus']\n",
        "            if 'Total_occ' in s_row: row['strom_total_occ'] = s_row['Total_occ']\n",
        "            if 'Total_genus' in s_row: row['strom_total_gen'] = s_row['Total_genus']\n",
        "\n",
        "    # C. Coral Data (5-Myr) - NOW INCLUDED\n",
        "    if 'coral_5myr_df' in locals() and not coral_5myr_df.empty:\n",
        "        # Match by midpoint\n",
        "        coral_match = coral_5myr_df[abs(((coral_5myr_df['bin_top'] + coral_5myr_df['bin_bottom'])/2) - midpoint) < 0.1]\n",
        "        if len(coral_match) > 0:\n",
        "            c_row = coral_match.iloc[0]\n",
        "            if 'Rugosa_occ' in c_row: row['rugose_occ'] = c_row['Rugosa_occ']\n",
        "            if 'Rugosa_genus' in c_row: row['rugose_div'] = c_row['Rugosa_genus']\n",
        "            if 'Tabulata_occ' in c_row: row['tabulate_occ'] = c_row['Tabulata_occ']\n",
        "            if 'Tabulata_genus' in c_row: row['tabulate_div'] = c_row['Tabulata_genus']\n",
        "\n",
        "    # D. Macrostrat Data\n",
        "    if 'macro_5myr' in locals() and not macro_5myr.empty and 'bin_mid' in macro_5myr.columns:\n",
        "        macro_match = macro_5myr[abs(macro_5myr['bin_mid'] - midpoint) < 2.5]\n",
        "        if len(macro_match) > 0:\n",
        "            row['total_area_km2'] = macro_match['total_area_km2'].values[0]\n",
        "            row['carbonate_area_km2'] = macro_match['carbonate_area_km2'].values[0]\n",
        "            row['carbonate_percentage'] = macro_match['carbonate_percentage'].values[0]\n",
        "\n",
        "    # E. Environmental Proxies\n",
        "    if 'env_data_5myr' in locals() and not env_data_5myr.empty:\n",
        "        # Match by midpoint\n",
        "        env_match = env_data_5myr[abs(env_data_5myr['midpoint_ma'] - midpoint) < 0.1]\n",
        "        if len(env_match) > 0:\n",
        "            env_val = env_match.iloc[0]\n",
        "            row['temperature'] = env_val['temperature']\n",
        "            row['dissolved_O2'] = env_val['dissolved_O2']\n",
        "            row['atm_O2'] = env_val['atm_O2']\n",
        "            row['atm_CO2'] = env_val['atm_CO2']\n",
        "            row['sea_level'] = env_val['sea_level']\n",
        "            if 'd13C' in env_val.index: row['d13C'] = env_val['d13C']\n",
        "\n",
        "    data_5myr.append(row)\n",
        "\n",
        "df_5myr = pd.DataFrame(data_5myr)\n",
        "df_5myr = df_5myr.sort_values('midpoint_ma', ascending=False).reset_index(drop=True)\n",
        "print(f\"✓ Master dataset (5-MYR BINS) created: {len(df_5myr)} bins, {len(df_5myr.columns)} variables\")\n",
        "\n",
        "# =============================================================================\n",
        "# [ADDED] SAVE MASTER DATASETS\n",
        "# =============================================================================\n",
        "# Save the master datasets immediately after creation\n",
        "if 'df' in locals() and not df.empty:\n",
        "    df.to_csv(f\"{OUTPUT_DIR}/MASTER_dataset_stage.csv\", index=False)\n",
        "    print(f\"✓ Saved: {OUTPUT_DIR}/MASTER_dataset_stage.csv\")\n",
        "\n",
        "if 'df_5myr' in locals() and not df_5myr.empty:\n",
        "    df_5myr.to_csv(f\"{OUTPUT_DIR}/MASTER_dataset_5myr.csv\", index=False)\n",
        "    print(f\"✓ Saved: {OUTPUT_DIR}/MASTER_dataset_5myr.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpj1BjT5d8eO",
        "outputId": "92c365c8-e5fd-4de3-944c-ddfd262949a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "PRE-PROCESSING: CLR TRANSFORMATION\n",
            "Transforming closed compositional data (proportions) to open log-ratios\n",
            "==========================================================================================\n",
            "  Stage-Level: Created 'log_derived_basal_ratio'\n",
            "  Stage-Level: Generated 7 CLR variables.\n",
            "  5-Myr Bins: Created 'log_derived_basal_ratio'\n",
            "  5-Myr Bins: Generated 7 CLR variables.\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "CLR CORRELATION ANALYSIS\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Stage-Level ---\n",
            "  Taxon/Group                  |   Orig ρ |    CLR ρ |    Diff |      CLR p\n",
            "  ---------------------------------------------------------------------------\n",
            "  Labechiida                   |   +0.075 |   -0.364 |  -0.439 |     0.0406 *\n",
            "  Clathrodictyida              |   +0.520 |   +0.416 |  -0.104 |     0.0178 *\n",
            "  Actinostromatida             |   +0.712 |   +0.405 |  -0.307 |     0.0213 *\n",
            "  Stromatoporida               |   +0.679 |   +0.359 |  -0.320 |     0.0433 *\n",
            "  Stromatoporellida            |   +0.622 |   +0.107 |  -0.516 |     0.5602 \n",
            "  Syringostromatida            |   +0.811 |   +0.506 |  -0.305 |     0.0031 *\n",
            "  Amphiporida                  |   +0.675 |   +0.158 |  -0.517 |     0.3881 \n",
            "  ---------------------------------------------------------------------------\n",
            "  GROUP COMPARISONS:\n",
            "  Basal (Labech+Clathr) CLR    |      N/A |   -0.233 |     N/A |     0.1997 \n",
            "  Derived (5 taxa) CLR         |      N/A |   +0.233 |     N/A |     0.1997 \n",
            "  Log(Derived/Basal) Ratio     |      N/A |   +0.530 |     N/A |     0.0018 *\n",
            "\n",
            "--- 5-Myr Bins ---\n",
            "  Taxon/Group                  |   Orig ρ |    CLR ρ |    Diff |      CLR p\n",
            "  ---------------------------------------------------------------------------\n",
            "  Labechiida                   |   -0.083 |   -0.610 |  -0.527 |     0.0001 *\n",
            "  Clathrodictyida              |   +0.541 |   +0.463 |  -0.078 |     0.0039 *\n",
            "  Actinostromatida             |   +0.747 |   +0.355 |  -0.393 |     0.0313 *\n",
            "  Stromatoporida               |   +0.714 |   +0.389 |  -0.325 |     0.0172 *\n",
            "  Stromatoporellida            |   +0.809 |   +0.600 |  -0.209 |     0.0001 *\n",
            "  Syringostromatida            |   +0.756 |   +0.516 |  -0.239 |     0.0011 *\n",
            "  Amphiporida                  |   +0.803 |   +0.482 |  -0.321 |     0.0025 *\n",
            "  ---------------------------------------------------------------------------\n",
            "  GROUP COMPARISONS:\n",
            "  Basal (Labech+Clathr) CLR    |      N/A |   -0.498 |     N/A |     0.0017 *\n",
            "  Derived (5 taxa) CLR         |      N/A |   +0.498 |     N/A |     0.0017 *\n",
            "  Log(Derived/Basal) Ratio     |      N/A |   +0.645 |     N/A |     0.0000 *\n",
            "\n",
            "Saved: ./output/results_clr.csv (40 rows)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 10: CLR COMPOSITIONAL TRANSFORMATION (WITH BASAL/DERIVED GROUPS)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import gmean\n",
        "import scipy.stats as stats\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"PRE-PROCESSING: CLR TRANSFORMATION\")\n",
        "print(\"Transforming closed compositional data (proportions) to open log-ratios\")\n",
        "print(\"=\"*90)\n",
        "# Global list for CLR results\n",
        "global_clr_results = []\n",
        "def calculate_missing_props(input_df):\n",
        "    \"\"\"\n",
        "    Helper: If _prop columns are missing, calculate them from _occ columns.\n",
        "    \"\"\"\n",
        "    df_copy = input_df.copy()\n",
        "    orders = ['Labechiida', 'Clathrodictyida', 'Actinostromatida',\n",
        "              'Stromatoporida', 'Stromatoporellida',\n",
        "              'Syringostromatida', 'Amphiporida']\n",
        "    # Check if we have occurrence columns\n",
        "    occ_cols = [f\"{o}_occ\" for o in orders if f\"{o}_occ\" in df_copy.columns]\n",
        "    if not occ_cols:\n",
        "        return df_copy\n",
        "    # Recalculate Total\n",
        "    if 'strom_total_occ' not in df_copy.columns:\n",
        "        df_copy['strom_total_occ'] = df_copy[occ_cols].sum(axis=1)\n",
        "    # Calculate Proportions\n",
        "    for o in orders:\n",
        "        occ_col = f\"{o}_occ\"\n",
        "        prop_col = f\"{o}_prop\"\n",
        "        if occ_col in df_copy.columns:\n",
        "            total = df_copy['strom_total_occ'].replace(0, np.nan)\n",
        "            df_copy[prop_col] = df_copy[occ_col] / total\n",
        "            df_copy[prop_col] = df_copy[prop_col].fillna(0)\n",
        "    return df_copy\n",
        "def apply_clr(input_df, label):\n",
        "    \"\"\"Apply CLR transformation and calculate group-level metrics.\"\"\"\n",
        "\n",
        "    dataset = input_df.copy()\n",
        "\n",
        "    # 1. AUTO-REPAIR: Ensure Proportion Columns Exist\n",
        "    dataset = calculate_missing_props(dataset)\n",
        "    # 2. Identify Proportion Columns\n",
        "    prop_cols = ['Labechiida_prop', 'Clathrodictyida_prop', 'Actinostromatida_prop',\n",
        "                 'Stromatoporida_prop', 'Stromatoporellida_prop',\n",
        "                 'Syringostromatida_prop', 'Amphiporida_prop']\n",
        "    available_cols = [c for c in prop_cols if c in dataset.columns]\n",
        "    # Define group membership\n",
        "    basal_orders = ['Labechiida_prop', 'Clathrodictyida_prop']\n",
        "    derived_orders = ['Actinostromatida_prop', 'Stromatoporida_prop',\n",
        "                      'Stromatoporellida_prop', 'Syringostromatida_prop',\n",
        "                      'Amphiporida_prop']\n",
        "    if len(available_cols) < 2:\n",
        "        print(f\"  {label}: ! Skipped. Found only {len(available_cols)} proportion columns.\")\n",
        "        return dataset\n",
        "    # 3. Extract and Handle Zeros\n",
        "    comp_data = dataset[available_cols].replace(0, 1e-5)\n",
        "    # 4. Geometric Mean per Row\n",
        "    gmeans = gmean(comp_data, axis=1)\n",
        "    # 5. Transform: ln(x / gmean)\n",
        "    clr_data = np.log(comp_data.div(gmeans, axis=0))\n",
        "\n",
        "    # Use CLR_ prefix (avoiding duplicates)\n",
        "    new_col_names = []\n",
        "    for c in available_cols:\n",
        "        new_name = f\"CLR_{c}\"\n",
        "        # Drop existing column if present to avoid duplicates\n",
        "        if new_name in dataset.columns:\n",
        "            dataset = dataset.drop(columns=[new_name])\n",
        "        new_col_names.append(new_name)\n",
        "    clr_data.columns = new_col_names\n",
        "    # 6. Create 'Derived vs Basal' Log-Ratio\n",
        "    basal_in = [c for c in basal_orders if c in dataset.columns]\n",
        "    derived_in = [c for c in derived_orders if c in dataset.columns]\n",
        "    if basal_in and derived_in:\n",
        "        b_sum = dataset[basal_in].sum(axis=1).replace(0, 1e-5)\n",
        "        d_sum = dataset[derived_in].sum(axis=1).replace(0, 1e-5)\n",
        "        dataset['log_derived_basal_ratio'] = np.log(d_sum / b_sum)\n",
        "        print(f\"  {label}: Created 'log_derived_basal_ratio'\")\n",
        "    # 7. Merge CLR columns back to dataset\n",
        "    dataset = pd.concat([dataset.reset_index(drop=True), clr_data.reset_index(drop=True)], axis=1)\n",
        "    print(f\"  {label}: Generated {len(clr_data.columns)} CLR variables.\")\n",
        "\n",
        "    # 8. Calculate GROUP-LEVEL CLR means\n",
        "    clr_basal_cols = [f\"CLR_{c}\" for c in basal_in]\n",
        "    clr_derived_cols = [f\"CLR_{c}\" for c in derived_in]\n",
        "\n",
        "    if clr_basal_cols:\n",
        "        dataset['CLR_basal_mean'] = dataset[clr_basal_cols].mean(axis=1)\n",
        "    if clr_derived_cols:\n",
        "        dataset['CLR_derived_mean'] = dataset[clr_derived_cols].mean(axis=1)\n",
        "\n",
        "    return dataset\n",
        "# Apply to both master datasets\n",
        "df = apply_clr(df, \"Stage-Level\")\n",
        "df_5myr = apply_clr(df_5myr, \"5-Myr Bins\")\n",
        "# =============================================================================\n",
        "# CLR CORRELATION ANALYSIS (Individual Taxa + Basal/Derived Groups)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"-\"*90)\n",
        "print(\"CLR CORRELATION ANALYSIS\")\n",
        "print(\"-\"*90)\n",
        "prop_cols = ['Labechiida_prop', 'Clathrodictyida_prop', 'Actinostromatida_prop',\n",
        "             'Stromatoporida_prop', 'Stromatoporellida_prop',\n",
        "             'Syringostromatida_prop', 'Amphiporida_prop']\n",
        "targets = ['thickness_mean', 'width_mean']\n",
        "def safe_spearman(x, y):\n",
        "    \"\"\"Calculate Spearman correlation safely, returning scalars.\"\"\"\n",
        "    try:\n",
        "        # Ensure 1D numpy arrays\n",
        "        x_arr = np.array(x).flatten()\n",
        "        y_arr = np.array(y).flatten()\n",
        "\n",
        "        # Remove NaN pairs\n",
        "        mask = ~(np.isnan(x_arr) | np.isnan(y_arr))\n",
        "        x_clean = x_arr[mask]\n",
        "        y_clean = y_arr[mask]\n",
        "\n",
        "        if len(x_clean) < 3:\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        result = stats.spearmanr(x_clean, y_clean)\n",
        "        # Handle both old and new scipy return types\n",
        "        if hasattr(result, 'correlation'):\n",
        "            return float(result.correlation), float(result.pvalue)\n",
        "        else:\n",
        "            return float(result[0]), float(result[1])\n",
        "    except:\n",
        "        return np.nan, np.nan\n",
        "for label, data in [('Stage-Level', df), ('5-Myr Bins', df_5myr)]:\n",
        "    print(f\"\\n--- {label} ---\")\n",
        "    print(f\"  {'Taxon/Group':<28s} | {'Orig ρ':>8s} | {'CLR ρ':>8s} | {'Diff':>7s} | {'CLR p':>10s}\")\n",
        "    print(\"  \" + \"-\"*75)\n",
        "\n",
        "    # A. INDIVIDUAL TAXA\n",
        "    for prop in prop_cols:\n",
        "        clr_col = f\"CLR_{prop}\"\n",
        "        if prop not in data.columns or clr_col not in data.columns:\n",
        "            continue\n",
        "\n",
        "        for target in targets:\n",
        "            if target not in data.columns:\n",
        "                continue\n",
        "\n",
        "            # Get data as 1D arrays\n",
        "            x_orig = data[prop].values\n",
        "            x_clr = data[clr_col].values\n",
        "            y = data[target].values\n",
        "\n",
        "            # Calculate correlations\n",
        "            r_orig, p_orig = safe_spearman(x_orig, y)\n",
        "            r_clr, p_clr = safe_spearman(x_clr, y)\n",
        "\n",
        "            if np.isnan(r_orig) or np.isnan(r_clr):\n",
        "                continue\n",
        "\n",
        "            diff = r_clr - r_orig\n",
        "\n",
        "            # Interpretation\n",
        "            if abs(diff) < 0.1:\n",
        "                interp = \"Stable\"\n",
        "            elif diff < -0.2:\n",
        "                interp = \"SUPPRESSED\"\n",
        "            elif diff > 0.2:\n",
        "                interp = \"INFLATED\"\n",
        "            else:\n",
        "                interp = \"Moderate\"\n",
        "\n",
        "            global_clr_results.append({\n",
        "                'Dataset': label, 'Level': 'Individual',\n",
        "                'Predictor': prop.replace('_prop', ''),\n",
        "                'Target': target, 'Original_Rho': r_orig, 'CLR_Rho': r_clr,\n",
        "                'Difference': diff, 'CLR_P': p_clr, 'Interpretation': interp\n",
        "            })\n",
        "\n",
        "            if target == 'thickness_mean':\n",
        "                sig = \"*\" if p_clr < 0.05 else \"\"\n",
        "                print(f\"  {prop.replace('_prop',''):<28s} | {r_orig:>+8.3f} | {r_clr:>+8.3f} | {diff:>+7.3f} | {p_clr:>10.4f} {sig}\")\n",
        "\n",
        "    # B. BASAL vs DERIVED GROUPS\n",
        "    print(\"  \" + \"-\"*75)\n",
        "    print(\"  GROUP COMPARISONS:\")\n",
        "\n",
        "    group_vars = [\n",
        "        ('CLR_basal_mean', 'Basal (Labech+Clathr) CLR'),\n",
        "        ('CLR_derived_mean', 'Derived (5 taxa) CLR'),\n",
        "        ('log_derived_basal_ratio', 'Log(Derived/Basal) Ratio')\n",
        "    ]\n",
        "\n",
        "    for col, name in group_vars:\n",
        "        if col not in data.columns:\n",
        "            continue\n",
        "\n",
        "        for target in targets:\n",
        "            if target not in data.columns:\n",
        "                continue\n",
        "\n",
        "            x = data[col].values\n",
        "            y = data[target].values\n",
        "\n",
        "            r, p = safe_spearman(x, y)\n",
        "\n",
        "            if np.isnan(r):\n",
        "                continue\n",
        "\n",
        "            global_clr_results.append({\n",
        "                'Dataset': label, 'Level': 'Group',\n",
        "                'Predictor': name, 'Target': target,\n",
        "                'Original_Rho': np.nan, 'CLR_Rho': r,\n",
        "                'Difference': np.nan, 'CLR_P': p, 'Interpretation': 'Group-level'\n",
        "            })\n",
        "\n",
        "            if target == 'thickness_mean':\n",
        "                sig = \"*\" if p < 0.05 else \"\"\n",
        "                print(f\"  {name:<28s} |      N/A | {r:>+8.3f} |     N/A | {p:>10.4f} {sig}\")\n",
        "# =============================================================================\n",
        "# SAVE CLR RESULTS\n",
        "# =============================================================================\n",
        "pd.DataFrame(global_clr_results).to_csv(f\"{OUTPUT_DIR}/results_clr.csv\", index=False)\n",
        "print(f\"\\nSaved: {OUTPUT_DIR}/results_clr.csv ({len(global_clr_results)} rows)\")\n",
        "# Update predictor lists\n",
        "if 'all_test_vars' in locals():\n",
        "    new_vars = [\n",
        "        ('log_derived_basal_ratio', 'Log(Derived/Basal)'),\n",
        "        ('CLR_basal_mean', 'CLR Basal Mean'),\n",
        "        ('CLR_derived_mean', 'CLR Derived Mean')\n",
        "    ]\n",
        "    for var in new_vars:\n",
        "        if not any(v[0] == var[0] for v in all_test_vars):\n",
        "            all_test_vars.append(var)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhFo2l0de0YP",
        "outputId": "c1d119a2-0718-4af6-abe9-92f8240f966c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Correlation functions defined (Spearman and Pearson)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 11: CORRELATION FUNCTIONS - SPEARMAN AND PEARSON\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "\"\"\"\n",
        "CORRELATION ANALYSIS METHODS\n",
        "============================\n",
        "This cell defines functions for both Spearman and Pearson correlations.\n",
        "\n",
        "SPEARMAN'S RHO (ρ):\n",
        "- Non-parametric rank correlation\n",
        "- Measures monotonic relationships (not just linear)\n",
        "- Robust to outliers and non-normal distributions\n",
        "- Appropriate for ordinal data or data with outliers\n",
        "- Reference: Spearman, C. (1904). American Journal of Psychology, 15(1), 72-101.\n",
        "\n",
        "PEARSON'S r:\n",
        "- Parametric correlation coefficient\n",
        "- Measures linear relationships specifically\n",
        "- Assumes normally distributed variables\n",
        "- More powerful when assumptions are met\n",
        "- Reference: Pearson, K. (1895). Philosophical Transactions of the Royal Society A, 186, 343-414.\n",
        "\n",
        "For geological time series:\n",
        "- Spearman is often preferred due to non-normal distributions\n",
        "- Pearson provides information on linear relationships\n",
        "- Presenting both allows comparison and assessment of relationship type\n",
        "\"\"\"\n",
        "\n",
        "def calc_spearman(data, v1, v2):\n",
        "    \"\"\"\n",
        "    Calculate Spearman rank correlation with significance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : DataFrame\n",
        "        Data containing the variables\n",
        "    v1, v2 : str\n",
        "        Column names to correlate\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    rho : float\n",
        "        Spearman correlation coefficient\n",
        "    p : float\n",
        "        Two-tailed p-value\n",
        "    n : int\n",
        "        Number of valid pairs\n",
        "    \"\"\"\n",
        "    # Check if columns exist\n",
        "    if v1 not in data.columns or v2 not in data.columns:\n",
        "        return np.nan, np.nan, 0\n",
        "\n",
        "    valid = data[[v1, v2]].dropna()\n",
        "\n",
        "    if len(valid) >= 5:\n",
        "        # Spearmanr returns a Result object or tuple depending on version, generic unpacking is safer\n",
        "        result = stats.spearmanr(valid[v1], valid[v2])\n",
        "        # Handle cases where result might be a struct or tuple\n",
        "        try:\n",
        "            r, p = result.correlation, result.pvalue\n",
        "        except AttributeError:\n",
        "            r, p = result[0], result[1]\n",
        "\n",
        "        return r, p, len(valid)\n",
        "\n",
        "    return np.nan, np.nan, 0\n",
        "\n",
        "def calc_pearson(data, v1, v2):\n",
        "    \"\"\"\n",
        "    Calculate Pearson correlation with significance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : DataFrame\n",
        "        Data containing the variables\n",
        "    v1, v2 : str\n",
        "        Column names to correlate\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    r : float\n",
        "        Pearson correlation coefficient\n",
        "    p : float\n",
        "        Two-tailed p-value\n",
        "    n : int\n",
        "        Number of valid pairs\n",
        "    \"\"\"\n",
        "    if v1 not in data.columns or v2 not in data.columns:\n",
        "        return np.nan, np.nan, 0\n",
        "\n",
        "    valid = data[[v1, v2]].dropna()\n",
        "\n",
        "    if len(valid) >= 5:\n",
        "        r, p = stats.pearsonr(valid[v1], valid[v2])\n",
        "        return r, p, len(valid)\n",
        "\n",
        "    return np.nan, np.nan, 0\n",
        "\n",
        "def calc_both_correlations(data, v1, v2):\n",
        "    \"\"\"\n",
        "    Calculate both Spearman and Pearson correlations\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict with both correlation results\n",
        "    \"\"\"\n",
        "    spearman_r, spearman_p, n = calc_spearman(data, v1, v2)\n",
        "    pearson_r, pearson_p, _ = calc_pearson(data, v1, v2)\n",
        "\n",
        "    return {\n",
        "        'spearman_rho': spearman_r,\n",
        "        'spearman_p': spearman_p,\n",
        "        'pearson_r': pearson_r,\n",
        "        'pearson_p': pearson_p,\n",
        "        'n': n\n",
        "    }\n",
        "\n",
        "def get_significance_stars(p):\n",
        "    \"\"\"Convert p-value to significance stars\"\"\"\n",
        "    if p is None or np.isnan(p):\n",
        "        return ''\n",
        "    if p < 0.001:\n",
        "        return '***'\n",
        "    elif p < 0.01:\n",
        "        return '**'\n",
        "    elif p < 0.05:\n",
        "        return '*'\n",
        "    else:\n",
        "        return 'ns'\n",
        "\n",
        "print(\"✓ Correlation functions defined (Spearman and Pearson)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0WIsgnZGe2v-",
        "outputId": "df8a6e4d-2451-4788-96f9-8f982724f664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "COMPREHENSIVE CORRELATION ANALYSIS\n",
            "Metrics: Thickness, Width\n",
            "Scopes:  Stage-Level AND 5-Myr Bins\n",
            "Notes:   Excluding strom_total_occ<=0 rows; NO T/W ratio; includes δ13C + atm O2/CO2 if present\n",
            "==========================================================================================\n",
            "Pre-processing data...\n",
            "  [OK] [Stage] Calculated Stromatoporoid proportions and groupings\n",
            "  [OK] [5-Myr] Calculated Stromatoporoid proportions and groupings\n",
            "  [OK] [Stage] atmospheric_O2 <- atm_O2\n",
            "  [OK] [Stage] atmospheric_CO2 <- atm_CO2\n",
            "  [OK] [5-Myr] atmospheric_O2 <- atm_O2\n",
            "  [OK] [5-Myr] atmospheric_CO2 <- atm_CO2\n",
            "  - [Stage] Excluding strom_total_occ<=0 rows: removed 11, kept 21.\n",
            "  - [5-Myr] Excluding strom_total_occ<=0 rows: removed 13, kept 24.\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "TARGET: THICKNESS (LOG)  |  rows used (after no-strom filter): 21\n",
            "------------------------------------------------------------------------------------------\n",
            "Variable                                 rho     p(rho)        r       p(r)     n\n",
            "Derived Proportion                      0.87***    2.5e-07     0.85***   8.77e-07    21\n",
            "Basal Proportion                       -0.87***    2.5e-07    -0.85***   8.77e-07    21\n",
            "Labechiida Prop                        -0.78***   3.21e-05    -0.73***   0.000153    21\n",
            "Clathrodictyida Prop                    0.05         0.838    -0.15         0.513    21\n",
            "Actinostromatida Prop                   0.61**      0.0031     0.60**     0.00438    21\n",
            "Stromatoporida Prop                     0.53*       0.0139     0.49*       0.0257    21\n",
            "Stromatoporellida Prop                  0.52*       0.0162     0.51*       0.0173    21\n",
            "Syringostromatida Prop                  0.84***   2.06e-06     0.57**     0.00702    21\n",
            "Amphiporida Prop                        0.65**     0.00135     0.61**     0.00322    21\n",
            "Total Occurrence                        0.35         0.117     0.44*       0.0477    21\n",
            "Derived Occurrence                      0.69***   0.000536     0.52*       0.0155    21\n",
            "Basal Occurrence                       -0.16         0.493    -0.21         0.355    21\n",
            "Labechiida Occ                         -0.48*       0.0283    -0.33         0.141    21\n",
            "Clathrodictyida Occ                     0.22         0.349     0.07          0.77    21\n",
            "Actinostromatida Occ                    0.62**     0.00277     0.56**     0.00857    21\n",
            "Stromatoporida Occ                      0.59**     0.00491     0.51*       0.0173    21\n",
            "Stromatoporellida Occ                   0.51*       0.0193     0.47*       0.0305    21\n",
            "Syringostromatida Occ                   0.77***   4.77e-05     0.47*        0.032    21\n",
            "Amphiporida Occ                         0.64**     0.00178     0.54*       0.0122    21\n",
            "Total Diversity                         0.50*       0.0196     0.43        0.0527    21\n",
            "Derived Diversity                       0.67***   0.000883     0.57**     0.00678    21\n",
            "Basal Diversity                        -0.11         0.631    -0.18         0.438    21\n",
            "Labechiida Div                         -0.58**     0.00595    -0.45*        0.042    21\n",
            "Clathrodictyida Div                     0.34         0.129     0.27         0.245    21\n",
            "Actinostromatida Div                    0.47*       0.0318     0.28         0.217    21\n",
            "Stromatoporida Div                      0.58**     0.00605     0.51*       0.0177    21\n",
            "Stromatoporellida Div                   0.50*       0.0215     0.50*       0.0209    21\n",
            "Syringostromatida Div                   0.72***   0.000249     0.52*       0.0147    21\n",
            "Amphiporida Div                         0.63**     0.00202     0.63**     0.00231    21\n",
            "Rugose Diversity                        0.48*       0.0294     0.40        0.0702    21\n",
            "Tabulate Diversity                      0.36         0.113     0.33         0.145    21\n",
            "Rugose Occurrence                       0.40        0.0755     0.36         0.105    21\n",
            "Tabulate Occurrence                     0.36         0.109     0.24         0.299    21\n",
            "Total Area                              0.11          0.63     0.09         0.689    21\n",
            "Carb Area                               0.03         0.902    -0.07         0.768    21\n",
            "Carb %                                 -0.36         0.113    -0.41         0.067    21\n",
            "SST                                    -0.14         0.537    -0.31         0.169    21\n",
            "Sea Level                              -0.37         0.103    -0.40        0.0698    21\n",
            "Atm O2                                  0.37        0.0949     0.38        0.0921    21\n",
            "Atm CO2                                -0.37         0.101    -0.39        0.0802    21\n",
            "Dissolved O2                            0.48*       0.0294     0.43        0.0536    21\n",
            "δ¹³C                                    0.23         0.313     0.25         0.266    21\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "TARGET: WIDTH (LOG)  |  rows used (after no-strom filter): 21\n",
            "------------------------------------------------------------------------------------------\n",
            "Variable                                 rho     p(rho)        r       p(r)     n\n",
            "Derived Proportion                      0.54*       0.0119     0.51*       0.0177    21\n",
            "Basal Proportion                       -0.54*       0.0119    -0.51*       0.0177    21\n",
            "Labechiida Prop                        -0.36         0.107    -0.15         0.518    21\n",
            "Clathrodictyida Prop                   -0.48*       0.0279    -0.50*       0.0206    21\n",
            "Actinostromatida Prop                   0.17          0.46     0.17         0.473    21\n",
            "Stromatoporida Prop                     0.09         0.691     0.06         0.813    21\n",
            "Stromatoporellida Prop                  0.13         0.583     0.29         0.195    21\n",
            "Syringostromatida Prop                  0.55**     0.00969     0.50*       0.0207    21\n",
            "Amphiporida Prop                        0.28         0.213     0.36         0.107    21\n",
            "Total Occurrence                        0.08         0.738     0.33         0.139    21\n",
            "Derived Occurrence                      0.24         0.289     0.40        0.0734    21\n",
            "Basal Occurrence                       -0.31         0.174    -0.17         0.472    21\n",
            "Labechiida Occ                         -0.44*       0.0432     0.01         0.949    21\n",
            "Clathrodictyida Occ                    -0.31         0.176    -0.29         0.202    21\n",
            "Actinostromatida Occ                    0.23         0.315     0.33         0.141    21\n",
            "Stromatoporida Occ                      0.19         0.422     0.31         0.169    21\n",
            "Stromatoporellida Occ                   0.11         0.622     0.39        0.0841    21\n",
            "Syringostromatida Occ                   0.39        0.0825     0.45*       0.0417    21\n",
            "Amphiporida Occ                         0.28         0.213     0.41        0.0666    21\n",
            "Total Diversity                         0.12         0.601     0.14         0.538    21\n",
            "Derived Diversity                       0.26         0.258     0.26         0.254    21\n",
            "Basal Diversity                        -0.26         0.256    -0.21         0.355    21\n",
            "Labechiida Div                         -0.48*       0.0295    -0.13         0.577    21\n",
            "Clathrodictyida Div                    -0.25         0.269    -0.15         0.521    21\n",
            "Actinostromatida Div                   -0.00         0.997    -0.11         0.623    21\n",
            "Stromatoporida Div                      0.20         0.383     0.32         0.152    21\n",
            "Stromatoporellida Div                   0.10         0.656     0.32         0.158    21\n",
            "Syringostromatida Div                   0.30         0.181     0.27         0.237    21\n",
            "Amphiporida Div                         0.22         0.341     0.28         0.224    21\n",
            "Rugose Diversity                        0.00         0.998     0.07         0.766    21\n",
            "Tabulate Diversity                      0.06         0.806     0.10         0.672    21\n",
            "Rugose Occurrence                       0.14         0.544     0.29         0.202    21\n",
            "Tabulate Occurrence                     0.04         0.854     0.10         0.675    21\n",
            "Total Area                              0.24         0.305     0.31         0.174    21\n",
            "Carb Area                               0.28          0.22     0.24         0.287    21\n",
            "Carb %                                 -0.23         0.322    -0.26         0.254    21\n",
            "SST                                     0.06         0.793     0.06         0.794    21\n",
            "Sea Level                              -0.04         0.871    -0.01         0.975    21\n",
            "Atm O2                                  0.11          0.65     0.11         0.642    21\n",
            "Atm CO2                                -0.05         0.836    -0.06         0.801    21\n",
            "Dissolved O2                            0.05         0.832     0.07          0.75    21\n",
            "δ¹³C                                   -0.15         0.527    -0.11         0.625    21\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "TARGET: THICKNESS (LOG)  |  rows used (after no-strom filter): 24\n",
            "------------------------------------------------------------------------------------------\n",
            "Variable                                 rho     p(rho)        r       p(r)     n\n",
            "Derived Proportion                      0.88***   1.67e-08     0.93***   7.06e-11    24\n",
            "Basal Proportion                       -0.88***   1.67e-08    -0.93***   7.06e-11    24\n",
            "Labechiida Prop                        -0.83***    5.6e-07    -0.83***   6.98e-07    24\n",
            "Clathrodictyida Prop                    0.14           0.5    -0.07          0.75    24\n",
            "Actinostromatida Prop                   0.71***   0.000105     0.67***   0.000373    24\n",
            "Stromatoporida Prop                     0.64***   0.000777     0.58**     0.00307    24\n",
            "Stromatoporellida Prop                  0.82***   9.88e-07     0.82***   8.75e-07    24\n",
            "Syringostromatida Prop                  0.77***   1.16e-05     0.60**     0.00177    24\n",
            "Amphiporida Prop                        0.89***   9.28e-09     0.79***   4.02e-06    24\n",
            "Total Occurrence                        0.41*       0.0461     0.42*       0.0401    24\n",
            "Derived Occurrence                      0.80***   2.42e-06     0.57**     0.00348    24\n",
            "Basal Occurrence                       -0.03         0.883    -0.27         0.202    24\n",
            "Labechiida Occ                         -0.46*       0.0234    -0.39        0.0584    24\n",
            "Clathrodictyida Occ                     0.30         0.158    -0.06         0.787    24\n",
            "Actinostromatida Occ                    0.70***    0.00014     0.59**     0.00228    24\n",
            "Stromatoporida Occ                      0.72***   6.86e-05     0.52**     0.00848    24\n",
            "Stromatoporellida Occ                   0.85***   1.89e-07     0.50*       0.0131    24\n",
            "Syringostromatida Occ                   0.75***   2.22e-05     0.58**     0.00278    24\n",
            "Amphiporida Occ                         0.83***      5e-07     0.56**     0.00446    24\n",
            "Total Diversity                         0.60**     0.00192     0.65***   0.000603    24\n",
            "Derived Diversity                       0.78***   8.22e-06     0.79***   3.54e-06    24\n",
            "Basal Diversity                        -0.05         0.831    -0.10         0.657    24\n",
            "Labechiida Div                         -0.55**     0.00538    -0.51*       0.0107    24\n",
            "Clathrodictyida Div                     0.45*       0.0256     0.52**     0.00899    24\n",
            "Actinostromatida Div                    0.40        0.0556     0.21         0.315    24\n",
            "Stromatoporida Div                      0.77***   1.05e-05     0.71***   9.74e-05    24\n",
            "Stromatoporellida Div                   0.84***   2.21e-07     0.82***   7.65e-07    24\n",
            "Syringostromatida Div                   0.76***    1.9e-05     0.78***   8.48e-06    24\n",
            "Amphiporida Div                         0.72***   7.87e-05     0.73***   5.62e-05    24\n",
            "Rugose Diversity                        0.47*       0.0205     0.56**     0.00417    24\n",
            "Tabulate Diversity                      0.29         0.165     0.41*       0.0495    24\n",
            "Rugose Occurrence                       0.37        0.0775     0.30         0.151    24\n",
            "Tabulate Occurrence                     0.23         0.281     0.16         0.451    24\n",
            "Total Area                             -0.02         0.942    -0.11          0.61    24\n",
            "Carb Area                              -0.30         0.152    -0.29         0.162    24\n",
            "Carb %                                 -0.59**     0.00242    -0.54**     0.00643    24\n",
            "SST                                    -0.43*       0.0374    -0.46*       0.0236    24\n",
            "Sea Level                              -0.61**     0.00167    -0.61**      0.0017    24\n",
            "Atm O2                                  0.19         0.376     0.22         0.292    24\n",
            "Atm CO2                                -0.44*       0.0321    -0.49*        0.014    24\n",
            "Dissolved O2                            0.31         0.134     0.32         0.132    24\n",
            "δ¹³C                                    0.27         0.209     0.26         0.213    24\n",
            "\n",
            "------------------------------------------------------------------------------------------\n",
            "TARGET: WIDTH (LOG)  |  rows used (after no-strom filter): 24\n",
            "------------------------------------------------------------------------------------------\n",
            "Variable                                 rho     p(rho)        r       p(r)     n\n",
            "Derived Proportion                      0.57**     0.00369     0.59**     0.00225    24\n",
            "Basal Proportion                       -0.57**     0.00369    -0.59**     0.00225    24\n",
            "Labechiida Prop                        -0.45*       0.0286    -0.29         0.171    24\n",
            "Clathrodictyida Prop                   -0.37         0.079    -0.49*       0.0155    24\n",
            "Actinostromatida Prop                   0.28         0.183     0.27         0.204    24\n",
            "Stromatoporida Prop                     0.21         0.334     0.12         0.588    24\n",
            "Stromatoporellida Prop                  0.39        0.0579     0.45*       0.0286    24\n",
            "Syringostromatida Prop                  0.68***   0.000264     0.71***   0.000112    24\n",
            "Amphiporida Prop                        0.59**     0.00226     0.57**      0.0034    24\n",
            "Total Occurrence                        0.28         0.178     0.40        0.0521    24\n",
            "Derived Occurrence                      0.53**       0.008     0.54**     0.00607    24\n",
            "Basal Occurrence                       -0.13         0.548    -0.25          0.23    24\n",
            "Labechiida Occ                         -0.37         0.075    -0.10         0.627    24\n",
            "Clathrodictyida Occ                     0.02         0.909    -0.27         0.201    24\n",
            "Actinostromatida Occ                    0.43*       0.0371     0.44*       0.0318    24\n",
            "Stromatoporida Occ                      0.41*        0.049     0.41*       0.0491    24\n",
            "Stromatoporellida Occ                   0.53**     0.00826     0.49*       0.0149    24\n",
            "Syringostromatida Occ                   0.65***   0.000604     0.68***   0.000264    24\n",
            "Amphiporida Occ                         0.59**     0.00256     0.54**     0.00664    24\n",
            "Total Diversity                         0.44*         0.03     0.43*       0.0382    24\n",
            "Derived Diversity                       0.49*       0.0149     0.54**     0.00702    24\n",
            "Basal Diversity                        -0.03         0.887    -0.09         0.675    24\n",
            "Labechiida Div                         -0.44*        0.031    -0.26         0.225    24\n",
            "Clathrodictyida Div                     0.14         0.522     0.20         0.357    24\n",
            "Actinostromatida Div                    0.07         0.736    -0.10         0.629    24\n",
            "Stromatoporida Div                      0.50*       0.0128     0.52**     0.00988    24\n",
            "Stromatoporellida Div                   0.55**     0.00519     0.67***   0.000377    24\n",
            "Syringostromatida Div                   0.63***   0.000961     0.66***   0.000491    24\n",
            "Amphiporida Div                         0.39        0.0564     0.40         0.052    24\n",
            "Rugose Diversity                        0.15         0.474     0.26         0.225    24\n",
            "Tabulate Diversity                      0.12         0.585     0.23         0.278    24\n",
            "Rugose Occurrence                       0.23         0.279     0.29         0.162    24\n",
            "Tabulate Occurrence                     0.04         0.843     0.08         0.721    24\n",
            "Total Area                              0.12         0.563     0.11         0.619    24\n",
            "Carb Area                              -0.06         0.766    -0.04         0.848    24\n",
            "Carb %                                 -0.33         0.117    -0.23         0.275    24\n",
            "SST                                    -0.16         0.451    -0.12         0.563    24\n",
            "Sea Level                              -0.19         0.379    -0.11         0.594    24\n",
            "Atm O2                                  0.07          0.76     0.10         0.657    24\n",
            "Atm CO2                                -0.06         0.787    -0.09          0.68    24\n",
            "Dissolved O2                            0.09         0.659     0.10         0.658    24\n",
            "δ¹³C                                   -0.14         0.526    -0.14         0.502    24\n",
            "\n",
            "Saved: ./output/results_correlations_stage.csv\n",
            "Saved: ./output/results_correlations_5myr.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Scope          Target              Predictor                  Label  \\\n",
              "0   Stage  thickness_mean     derived_strom_prop     Derived Proportion   \n",
              "1   Stage  thickness_mean       basal_strom_prop       Basal Proportion   \n",
              "2   Stage  thickness_mean        Labechiida_prop        Labechiida Prop   \n",
              "3   Stage  thickness_mean   Clathrodictyida_prop   Clathrodictyida Prop   \n",
              "4   Stage  thickness_mean  Actinostromatida_prop  Actinostromatida Prop   \n",
              "..    ...             ...                    ...                    ...   \n",
              "79  Stage      width_mean              sea_level              Sea Level   \n",
              "80  Stage      width_mean         atmospheric_O2                 Atm O2   \n",
              "81  Stage      width_mean        atmospheric_CO2                Atm CO2   \n",
              "82  Stage      width_mean           dissolved_O2           Dissolved O2   \n",
              "83  Stage      width_mean                   d13C                   δ¹³C   \n",
              "\n",
              "          Group  Spearman_Rho    Spearman_P  Pearson_R     Pearson_P   N  \n",
              "0   Strom Props      0.872563  2.501763e-07   0.853369  8.771189e-07  21  \n",
              "1   Strom Props     -0.872563  2.501763e-07  -0.853369  8.771189e-07  21  \n",
              "2   Strom Props     -0.778648  3.214033e-05  -0.733873  1.527280e-04  21  \n",
              "3   Strom Props      0.047557  8.378025e-01  -0.151025  5.134546e-01  21  \n",
              "4   Strom Props      0.613505  3.098310e-03   0.595730  4.377395e-03  21  \n",
              "..          ...           ...           ...        ...           ...  ..  \n",
              "79      Proxies     -0.037675  8.712021e-01  -0.007430  9.745005e-01  21  \n",
              "80      Proxies      0.105229  6.498606e-01   0.107905  6.415237e-01  21  \n",
              "81      Proxies     -0.048068  8.360847e-01  -0.058657  8.006085e-01  21  \n",
              "82      Proxies      0.049367  8.317147e-01   0.074091  7.495915e-01  21  \n",
              "83      Proxies     -0.146151  5.272904e-01  -0.113166  6.252595e-01  21  \n",
              "\n",
              "[84 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8b06f8be-ec12-4fef-aa8f-e15f191283ae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Scope</th>\n",
              "      <th>Target</th>\n",
              "      <th>Predictor</th>\n",
              "      <th>Label</th>\n",
              "      <th>Group</th>\n",
              "      <th>Spearman_Rho</th>\n",
              "      <th>Spearman_P</th>\n",
              "      <th>Pearson_R</th>\n",
              "      <th>Pearson_P</th>\n",
              "      <th>N</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stage</td>\n",
              "      <td>thickness_mean</td>\n",
              "      <td>derived_strom_prop</td>\n",
              "      <td>Derived Proportion</td>\n",
              "      <td>Strom Props</td>\n",
              "      <td>0.872563</td>\n",
              "      <td>2.501763e-07</td>\n",
              "      <td>0.853369</td>\n",
              "      <td>8.771189e-07</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Stage</td>\n",
              "      <td>thickness_mean</td>\n",
              "      <td>basal_strom_prop</td>\n",
              "      <td>Basal Proportion</td>\n",
              "      <td>Strom Props</td>\n",
              "      <td>-0.872563</td>\n",
              "      <td>2.501763e-07</td>\n",
              "      <td>-0.853369</td>\n",
              "      <td>8.771189e-07</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Stage</td>\n",
              "      <td>thickness_mean</td>\n",
              "      <td>Labechiida_prop</td>\n",
              "      <td>Labechiida Prop</td>\n",
              "      <td>Strom Props</td>\n",
              "      <td>-0.778648</td>\n",
              "      <td>3.214033e-05</td>\n",
              "      <td>-0.733873</td>\n",
              "      <td>1.527280e-04</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stage</td>\n",
              "      <td>thickness_mean</td>\n",
              "      <td>Clathrodictyida_prop</td>\n",
              "      <td>Clathrodictyida Prop</td>\n",
              "      <td>Strom Props</td>\n",
              "      <td>0.047557</td>\n",
              "      <td>8.378025e-01</td>\n",
              "      <td>-0.151025</td>\n",
              "      <td>5.134546e-01</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Stage</td>\n",
              "      <td>thickness_mean</td>\n",
              "      <td>Actinostromatida_prop</td>\n",
              "      <td>Actinostromatida Prop</td>\n",
              "      <td>Strom Props</td>\n",
              "      <td>0.613505</td>\n",
              "      <td>3.098310e-03</td>\n",
              "      <td>0.595730</td>\n",
              "      <td>4.377395e-03</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>Stage</td>\n",
              "      <td>width_mean</td>\n",
              "      <td>sea_level</td>\n",
              "      <td>Sea Level</td>\n",
              "      <td>Proxies</td>\n",
              "      <td>-0.037675</td>\n",
              "      <td>8.712021e-01</td>\n",
              "      <td>-0.007430</td>\n",
              "      <td>9.745005e-01</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>Stage</td>\n",
              "      <td>width_mean</td>\n",
              "      <td>atmospheric_O2</td>\n",
              "      <td>Atm O2</td>\n",
              "      <td>Proxies</td>\n",
              "      <td>0.105229</td>\n",
              "      <td>6.498606e-01</td>\n",
              "      <td>0.107905</td>\n",
              "      <td>6.415237e-01</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>Stage</td>\n",
              "      <td>width_mean</td>\n",
              "      <td>atmospheric_CO2</td>\n",
              "      <td>Atm CO2</td>\n",
              "      <td>Proxies</td>\n",
              "      <td>-0.048068</td>\n",
              "      <td>8.360847e-01</td>\n",
              "      <td>-0.058657</td>\n",
              "      <td>8.006085e-01</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>Stage</td>\n",
              "      <td>width_mean</td>\n",
              "      <td>dissolved_O2</td>\n",
              "      <td>Dissolved O2</td>\n",
              "      <td>Proxies</td>\n",
              "      <td>0.049367</td>\n",
              "      <td>8.317147e-01</td>\n",
              "      <td>0.074091</td>\n",
              "      <td>7.495915e-01</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>Stage</td>\n",
              "      <td>width_mean</td>\n",
              "      <td>d13C</td>\n",
              "      <td>δ¹³C</td>\n",
              "      <td>Proxies</td>\n",
              "      <td>-0.146151</td>\n",
              "      <td>5.272904e-01</td>\n",
              "      <td>-0.113166</td>\n",
              "      <td>6.252595e-01</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>84 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b06f8be-ec12-4fef-aa8f-e15f191283ae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8b06f8be-ec12-4fef-aa8f-e15f191283ae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8b06f8be-ec12-4fef-aa8f-e15f191283ae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_2a402aac-fcee-40d7-a9de-9a18e36202e7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('stage_results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2a402aac-fcee-40d7-a9de-9a18e36202e7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('stage_results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "stage_results_df",
              "summary": "{\n  \"name\": \"stage_results_df\",\n  \"rows\": 84,\n  \"fields\": [\n    {\n      \"column\": \"Scope\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Stage\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"width_mean\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Predictor\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"Stromatoporida_genus\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"Stromatoporida Div\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Group\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Strom Props\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spearman_Rho\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.387550793370269,\n        \"min\": -0.8725625062887773,\n        \"max\": 0.8725625062887773,\n        \"num_unique_values\": 83,\n        \"samples\": [\n          0.3559597459579221\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spearman_P\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.30475477287239405,\n        \"min\": 2.5017628461079054e-07,\n        \"max\": 0.9977704105490508,\n        \"num_unique_values\": 81,\n        \"samples\": [\n          0.07547637795047234\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pearson_R\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.34857987882751423,\n        \"min\": -0.8533693231861407,\n        \"max\": 0.8533693231861407,\n        \"num_unique_values\": 84,\n        \"samples\": [\n          0.29044351462912754\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pearson_P\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2776638161541995,\n        \"min\": 8.77118909244623e-07,\n        \"max\": 0.9745004675975981,\n        \"num_unique_values\": 82,\n        \"samples\": [\n          0.10463303925576017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 21,\n        \"max\": 21,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          21\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Scope          Target              Predictor                  Label  \\\n",
              "0   5-Myr  thickness_mean     derived_strom_prop     Derived Proportion   \n",
              "1   5-Myr  thickness_mean       basal_strom_prop       Basal Proportion   \n",
              "2   5-Myr  thickness_mean        Labechiida_prop        Labechiida Prop   \n",
              "3   5-Myr  thickness_mean   Clathrodictyida_prop   Clathrodictyida Prop   \n",
              "4   5-Myr  thickness_mean  Actinostromatida_prop  Actinostromatida Prop   \n",
              "..    ...             ...                    ...                    ...   \n",
              "79  5-Myr      width_mean              sea_level              Sea Level   \n",
              "80  5-Myr      width_mean         atmospheric_O2                 Atm O2   \n",
              "81  5-Myr      width_mean        atmospheric_CO2                Atm CO2   \n",
              "82  5-Myr      width_mean           dissolved_O2           Dissolved O2   \n",
              "83  5-Myr      width_mean                   d13C                   δ¹³C   \n",
              "\n",
              "          Group  Spearman_Rho    Spearman_P  Pearson_R     Pearson_P   N  \n",
              "0   Strom Props      0.878380  1.673329e-08   0.927584  7.061008e-11  24  \n",
              "1   Strom Props     -0.878380  1.673329e-08  -0.927584  7.061008e-11  24  \n",
              "2   Strom Props     -0.828941  5.599693e-07  -0.825183  6.980943e-07  24  \n",
              "3   Strom Props      0.144726  4.998510e-01  -0.068724  7.496635e-01  24  \n",
              "4   Strom Props      0.708982  1.051787e-04   0.666808  3.730702e-04  24  \n",
              "..          ...           ...           ...        ...           ...  ..  \n",
              "79      Proxies     -0.188113  3.787188e-01  -0.114646  5.937347e-01  24  \n",
              "80      Proxies      0.065752  7.601729e-01   0.095466  6.572337e-01  24  \n",
              "81      Proxies     -0.058350  7.865258e-01  -0.088770  6.799854e-01  24  \n",
              "82      Proxies      0.094927  6.590544e-01   0.095234  6.580181e-01  24  \n",
              "83      Proxies     -0.136136  5.258928e-01  -0.143891  5.023529e-01  24  \n",
              "\n",
              "[84 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6eb2658d-43d0-4d3a-b217-c1b952e75768\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Scope</th>\n",
              "      <th>Target</th>\n",
              "      <th>Predictor</th>\n",
              "      <th>Label</th>\n",
              "      <th>Group</th>\n",
              "      <th>Spearman_Rho</th>\n",
              "      <th>Spearman_P</th>\n",
              "      <th>Pearson_R</th>\n",
              "      <th>Pearson_P</th>\n",
              "      <th>N</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5-Myr</td>\n",
              "      <td>thickness_mean</td>\n",
              "      <td>derived_strom_prop</td>\n",
              "      <td>Derived Proportion</td>\n",
              "      <td>Strom Props</td>\n",
              "      <td>0.878380</td>\n",
              "      <td>1.673329e-08</td>\n",
              "      <td>0.927584</td>\n",
              "      <td>7.061008e-11</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5-Myr</td>\n",
              "      <td>thickness_mean</td>\n",
              "      <td>basal_strom_prop</td>\n",
              "      <td>Basal Proportion</td>\n",
              "      <td>Strom Props</td>\n",
              "      <td>-0.878380</td>\n",
              "      <td>1.673329e-08</td>\n",
              "      <td>-0.927584</td>\n",
              "      <td>7.061008e-11</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5-Myr</td>\n",
              "      <td>thickness_mean</td>\n",
              "      <td>Labechiida_prop</td>\n",
              "      <td>Labechiida Prop</td>\n",
              "      <td>Strom Props</td>\n",
              "      <td>-0.828941</td>\n",
              "      <td>5.599693e-07</td>\n",
              "      <td>-0.825183</td>\n",
              "      <td>6.980943e-07</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5-Myr</td>\n",
              "      <td>thickness_mean</td>\n",
              "      <td>Clathrodictyida_prop</td>\n",
              "      <td>Clathrodictyida Prop</td>\n",
              "      <td>Strom Props</td>\n",
              "      <td>0.144726</td>\n",
              "      <td>4.998510e-01</td>\n",
              "      <td>-0.068724</td>\n",
              "      <td>7.496635e-01</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5-Myr</td>\n",
              "      <td>thickness_mean</td>\n",
              "      <td>Actinostromatida_prop</td>\n",
              "      <td>Actinostromatida Prop</td>\n",
              "      <td>Strom Props</td>\n",
              "      <td>0.708982</td>\n",
              "      <td>1.051787e-04</td>\n",
              "      <td>0.666808</td>\n",
              "      <td>3.730702e-04</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>5-Myr</td>\n",
              "      <td>width_mean</td>\n",
              "      <td>sea_level</td>\n",
              "      <td>Sea Level</td>\n",
              "      <td>Proxies</td>\n",
              "      <td>-0.188113</td>\n",
              "      <td>3.787188e-01</td>\n",
              "      <td>-0.114646</td>\n",
              "      <td>5.937347e-01</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>5-Myr</td>\n",
              "      <td>width_mean</td>\n",
              "      <td>atmospheric_O2</td>\n",
              "      <td>Atm O2</td>\n",
              "      <td>Proxies</td>\n",
              "      <td>0.065752</td>\n",
              "      <td>7.601729e-01</td>\n",
              "      <td>0.095466</td>\n",
              "      <td>6.572337e-01</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>5-Myr</td>\n",
              "      <td>width_mean</td>\n",
              "      <td>atmospheric_CO2</td>\n",
              "      <td>Atm CO2</td>\n",
              "      <td>Proxies</td>\n",
              "      <td>-0.058350</td>\n",
              "      <td>7.865258e-01</td>\n",
              "      <td>-0.088770</td>\n",
              "      <td>6.799854e-01</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>5-Myr</td>\n",
              "      <td>width_mean</td>\n",
              "      <td>dissolved_O2</td>\n",
              "      <td>Dissolved O2</td>\n",
              "      <td>Proxies</td>\n",
              "      <td>0.094927</td>\n",
              "      <td>6.590544e-01</td>\n",
              "      <td>0.095234</td>\n",
              "      <td>6.580181e-01</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>5-Myr</td>\n",
              "      <td>width_mean</td>\n",
              "      <td>d13C</td>\n",
              "      <td>δ¹³C</td>\n",
              "      <td>Proxies</td>\n",
              "      <td>-0.136136</td>\n",
              "      <td>5.258928e-01</td>\n",
              "      <td>-0.143891</td>\n",
              "      <td>5.023529e-01</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>84 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6eb2658d-43d0-4d3a-b217-c1b952e75768')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6eb2658d-43d0-4d3a-b217-c1b952e75768 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6eb2658d-43d0-4d3a-b217-c1b952e75768');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_33c8169a-6455-4c3d-a9b0-5f296104a2f5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('myr_results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_33c8169a-6455-4c3d-a9b0-5f296104a2f5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('myr_results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "myr_results_df",
              "summary": "{\n  \"name\": \"myr_results_df\",\n  \"rows\": 84,\n  \"fields\": [\n    {\n      \"column\": \"Scope\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"5-Myr\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"width_mean\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Predictor\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"Stromatoporida_genus\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"Stromatoporida Div\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Group\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Strom Props\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spearman_Rho\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4497670658555151,\n        \"min\": -0.8783797136997108,\n        \"max\": 0.8850632123965714,\n        \"num_unique_values\": 84,\n        \"samples\": [\n          0.2304008935408921\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spearman_P\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2907438143587732,\n        \"min\": 9.28059032977321e-09,\n        \"max\": 0.942106924134627,\n        \"num_unique_values\": 82,\n        \"samples\": [\n          0.0775451974874182\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pearson_R\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.43628335400292306,\n        \"min\": -0.9275841519122263,\n        \"max\": 0.9275841519122263,\n        \"num_unique_values\": 84,\n        \"samples\": [\n          0.2947869676132322\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pearson_P\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2582626699218748,\n        \"min\": 7.061007671886164e-11,\n        \"max\": 0.8482332683866401,\n        \"num_unique_values\": 82,\n        \"samples\": [\n          0.15084225081927555\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 24,\n        \"max\": 24,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 12: COMPREHENSIVE CORRELATION ANALYSIS (MULTI-METRIC & 5-MYR BIOLOGY)\n",
        "#   - NO T/W ratio computed or compared\n",
        "#   - Excludes rows with NO stromatoporoids (strom_total_occ<=0) from statistics\n",
        "#   - Includes δ13C (expects column name 'd13C')\n",
        "#   - Standardizes atmospheric O2/CO2 column names for correlation output:\n",
        "#       atmospheric_O2, atmospheric_CO2\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import re\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\"COMPREHENSIVE CORRELATION ANALYSIS\")\n",
        "print(\"Metrics: Thickness, Width\")\n",
        "print(\"Scopes:  Stage-Level AND 5-Myr Bins\")\n",
        "print(\"Notes:   Excluding strom_total_occ<=0 rows; NO T/W ratio; includes δ13C + atm O2/CO2 if present\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 0. SAFETY: define STROM_ORDERS if missing\n",
        "# -----------------------------------------------------------------------------\n",
        "if 'STROM_ORDERS' not in globals():\n",
        "    STROM_ORDERS = [\n",
        "        'Labechiida', 'Clathrodictyida', 'Actinostromatida',\n",
        "        'Stromatoporida', 'Stromatoporellida', 'Syringostromatida', 'Amphiporida'\n",
        "    ]\n",
        "    print(\"[WARN] STROM_ORDERS not found in globals(); using default list.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. PRE-PROCESSING: CALCULATE DERIVED VARIABLES (NO T/W)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"Pre-processing data...\")\n",
        "\n",
        "def calculate_derived_metrics(dataset, label):\n",
        "    if dataset is None or dataset.empty:\n",
        "        print(f\"  - [{label}] Empty dataset; skip derived metrics.\")\n",
        "        return dataset\n",
        "\n",
        "    dataset = dataset.copy()\n",
        "\n",
        "    # Remove T/W ratio if it exists from older runs (do NOT compute it)\n",
        "    if 'thickness_width_ratio' in dataset.columns:\n",
        "        dataset = dataset.drop(columns=['thickness_width_ratio'])\n",
        "\n",
        "    # Stromatoporoid Proportions & Groups\n",
        "    strom_cols = [c for c in dataset.columns if c.endswith('_occ') and c != 'strom_total_occ']\n",
        "    if strom_cols:\n",
        "        if 'strom_total_occ' not in dataset.columns:\n",
        "            dataset['strom_total_occ'] = dataset[strom_cols].sum(axis=1)\n",
        "\n",
        "        # Ensure numeric\n",
        "        dataset['strom_total_occ'] = pd.to_numeric(dataset['strom_total_occ'], errors='coerce')\n",
        "\n",
        "        for order in STROM_ORDERS:\n",
        "            col_occ = f'{order}_occ'\n",
        "            if col_occ in dataset.columns:\n",
        "                dataset[col_occ] = pd.to_numeric(dataset[col_occ], errors='coerce').fillna(0)\n",
        "                dataset[f'{order}_prop'] = np.where(\n",
        "                    dataset['strom_total_occ'] > 0,\n",
        "                    dataset[col_occ] / dataset['strom_total_occ'],\n",
        "                    np.nan\n",
        "                )\n",
        "\n",
        "        derived_orders = ['Actinostromatida', 'Stromatoporida', 'Stromatoporellida',\n",
        "                          'Syringostromatida', 'Amphiporida']\n",
        "        basal_orders   = ['Labechiida', 'Clathrodictyida']\n",
        "\n",
        "        # occurrences\n",
        "        dataset['derived_strom_occ'] = 0.0\n",
        "        dataset['basal_strom_occ']   = 0.0\n",
        "        for o in derived_orders:\n",
        "            c = f'{o}_occ'\n",
        "            if c in dataset.columns:\n",
        "                dataset['derived_strom_occ'] += pd.to_numeric(dataset[c], errors='coerce').fillna(0)\n",
        "        for o in basal_orders:\n",
        "            c = f'{o}_occ'\n",
        "            if c in dataset.columns:\n",
        "                dataset['basal_strom_occ'] += pd.to_numeric(dataset[c], errors='coerce').fillna(0)\n",
        "\n",
        "        # diversity (genus)\n",
        "        dataset['derived_strom_div'] = 0.0\n",
        "        dataset['basal_strom_div']   = 0.0\n",
        "        for o in derived_orders:\n",
        "            c = f'{o}_genus'\n",
        "            if c in dataset.columns:\n",
        "                dataset['derived_strom_div'] += pd.to_numeric(dataset[c], errors='coerce').fillna(0)\n",
        "        for o in basal_orders:\n",
        "            c = f'{o}_genus'\n",
        "            if c in dataset.columns:\n",
        "                dataset['basal_strom_div'] += pd.to_numeric(dataset[c], errors='coerce').fillna(0)\n",
        "\n",
        "        dataset['derived_strom_prop'] = np.where(\n",
        "            dataset['strom_total_occ'] > 0,\n",
        "            dataset['derived_strom_occ'] / dataset['strom_total_occ'],\n",
        "            np.nan\n",
        "        )\n",
        "        dataset['basal_strom_prop'] = np.where(\n",
        "            dataset['strom_total_occ'] > 0,\n",
        "            dataset['basal_strom_occ'] / dataset['strom_total_occ'],\n",
        "            np.nan\n",
        "        )\n",
        "\n",
        "        print(f\"  [OK] [{label}] Calculated Stromatoporoid proportions and groupings\")\n",
        "    else:\n",
        "        print(f\"  - [{label}] No Stromatoporoid occurrence data found.\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "df = calculate_derived_metrics(df, \"Stage\")\n",
        "df_5myr = calculate_derived_metrics(df_5myr, \"5-Myr\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1A. STANDARDIZE ATMOSPHERIC O2/CO2 COLUMN NAMES (Stage + 5-Myr)\n",
        "#   Outputs standardized columns:\n",
        "#     - atmospheric_O2\n",
        "#     - atmospheric_CO2\n",
        "# -----------------------------------------------------------------------------\n",
        "def standardize_atm_cols(dataset, label):\n",
        "    if dataset is None or dataset.empty:\n",
        "        return dataset\n",
        "    dataset = dataset.copy()\n",
        "\n",
        "    def _find_col(candidates, regex_pat=None):\n",
        "        for c in candidates:\n",
        "            if c in dataset.columns:\n",
        "                return c\n",
        "        if regex_pat is not None:\n",
        "            hits = [c for c in dataset.columns if re.search(regex_pat, c, flags=re.IGNORECASE)]\n",
        "            return hits[0] if hits else None\n",
        "        return None\n",
        "\n",
        "    # Atmospheric O2\n",
        "    if 'atmospheric_O2' not in dataset.columns:\n",
        "        o2_src = _find_col(\n",
        "            candidates=['atm_O2','atmospheric_o2','oxygen','pO2','PO2','O2_atm','oxygen_atm'],\n",
        "            regex_pat=r'(atm|atmos).*o2|po2'\n",
        "        )\n",
        "        if o2_src is not None:\n",
        "            dataset['atmospheric_O2'] = pd.to_numeric(dataset[o2_src], errors='coerce')\n",
        "            print(f\"  [OK] [{label}] atmospheric_O2 <- {o2_src}\")\n",
        "        else:\n",
        "            print(f\"  [WARN] [{label}] No atmospheric O2 column found (atmospheric_O2 will be missing).\")\n",
        "\n",
        "    # Atmospheric CO2\n",
        "    if 'atmospheric_CO2' not in dataset.columns:\n",
        "        co2_src = _find_col(\n",
        "            candidates=['atm_CO2','atmospheric_co2','co2','pCO2','PCO2','CO2_atm','co2_atm'],\n",
        "            regex_pat=r'(atm|atmos).*co2|pco2'\n",
        "        )\n",
        "        if co2_src is not None:\n",
        "            dataset['atmospheric_CO2'] = pd.to_numeric(dataset[co2_src], errors='coerce')\n",
        "            print(f\"  [OK] [{label}] atmospheric_CO2 <- {co2_src}\")\n",
        "        else:\n",
        "            print(f\"  [WARN] [{label}] No atmospheric CO2 column found (atmospheric_CO2 will be missing).\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "df = standardize_atm_cols(df, \"Stage\")\n",
        "df_5myr = standardize_atm_cols(df_5myr, \"5-Myr\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1B. FILTER: EXCLUDE ROWS WITH NO STROMATOPOROIDS FROM STATISTICS\n",
        "# -----------------------------------------------------------------------------\n",
        "def filter_has_strom(df_in, label):\n",
        "    if df_in is None or df_in.empty:\n",
        "        print(f\"  - [{label}] Empty dataset; nothing to filter.\")\n",
        "        return df_in\n",
        "    if 'strom_total_occ' not in df_in.columns:\n",
        "        print(f\"  [WARN] [{label}] 'strom_total_occ' not found; keeping all rows.\")\n",
        "        return df_in\n",
        "    df_out = df_in[df_in['strom_total_occ'].notna() & (pd.to_numeric(df_in['strom_total_occ'], errors='coerce') > 0)].copy()\n",
        "    removed = len(df_in) - len(df_out)\n",
        "    print(f\"  - [{label}] Excluding strom_total_occ<=0 rows: removed {removed}, kept {len(df_out)}.\")\n",
        "    return df_out\n",
        "\n",
        "df_corr = filter_has_strom(df, \"Stage\")\n",
        "df_5myr_corr = filter_has_strom(df_5myr, \"5-Myr\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. DEFINE VARIABLE GROUPS\n",
        "# -----------------------------------------------------------------------------\n",
        "reef_targets = [\n",
        "    ('thickness_mean', 'Thickness (Log)'),\n",
        "    ('width_mean', 'Width (Log)')\n",
        "]\n",
        "\n",
        "strom_prop_vars = [('derived_strom_prop', 'Derived Proportion'), ('basal_strom_prop', 'Basal Proportion')] + \\\n",
        "                  [(f'{order}_prop', f'{order} Prop') for order in STROM_ORDERS]\n",
        "\n",
        "strom_occ_vars = [('strom_total_occ', 'Total Occurrence'), ('derived_strom_occ', 'Derived Occurrence'), ('basal_strom_occ', 'Basal Occurrence')] + \\\n",
        "                 [(f'{order}_occ', f'{order} Occ') for order in STROM_ORDERS]\n",
        "\n",
        "strom_div_vars = [('strom_total_gen', 'Total Diversity'), ('derived_strom_div', 'Derived Diversity'), ('basal_strom_div', 'Basal Diversity')] + \\\n",
        "                 [(f'{order}_genus', f'{order} Div') for order in STROM_ORDERS]\n",
        "\n",
        "coral_div_vars = [('rugose_div', 'Rugose Diversity'), ('tabulate_div', 'Tabulate Diversity')]\n",
        "coral_occ_vars = [('rugose_occ', 'Rugose Occurrence'), ('tabulate_occ', 'Tabulate Occurrence')]\n",
        "\n",
        "env_vars_macrostrat = [\n",
        "    ('total_area_km2', 'Total Area'),\n",
        "    ('carbonate_area_km2', 'Carb Area'),\n",
        "    ('carbonate_percentage', 'Carb %')\n",
        "]\n",
        "\n",
        "# IMPORTANT: δ13C expected as 'd13C'\n",
        "env_vars_proxies = [\n",
        "    ('temperature', 'SST'),\n",
        "    ('sea_level', 'Sea Level'),\n",
        "    ('atmospheric_O2', 'Atm O2'),\n",
        "    ('atmospheric_CO2', 'Atm CO2'),\n",
        "    ('dissolved_O2', 'Dissolved O2'),\n",
        "    ('d13C', 'δ¹³C')\n",
        "]\n",
        "\n",
        "var_groups = [\n",
        "    ('Strom Props', strom_prop_vars),\n",
        "    ('Strom Occ', strom_occ_vars),\n",
        "    ('Strom Div', strom_div_vars),\n",
        "    ('Coral Div', coral_div_vars),\n",
        "    ('Coral Occ', coral_occ_vars),\n",
        "    ('Macrostrat', env_vars_macrostrat),\n",
        "    ('Proxies', env_vars_proxies),\n",
        "]\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. ANALYSIS FUNCTIONS\n",
        "# -----------------------------------------------------------------------------\n",
        "def get_significance_stars(p):\n",
        "    if p is None or np.isnan(p):\n",
        "        return \"\"\n",
        "    if p < 0.001:\n",
        "        return \"***\"\n",
        "    if p < 0.01:\n",
        "        return \"**\"\n",
        "    if p < 0.05:\n",
        "        return \"*\"\n",
        "    return \"\"\n",
        "\n",
        "def calc_stats(x, y):\n",
        "    mask = np.isfinite(x) & np.isfinite(y)\n",
        "    x = x[mask]\n",
        "    y = y[mask]\n",
        "    n = len(x)\n",
        "    if n < 5:\n",
        "        return dict(n=n, spearman_rho=np.nan, spearman_p=np.nan, pearson_r=np.nan, pearson_p=np.nan)\n",
        "    sr = stats.spearmanr(x, y)\n",
        "    pr = stats.pearsonr(x, y)\n",
        "    return dict(\n",
        "        n=n,\n",
        "        spearman_rho=float(sr.correlation),\n",
        "        spearman_p=float(sr.pvalue),\n",
        "        pearson_r=float(pr.statistic),\n",
        "        pearson_p=float(pr.pvalue)\n",
        "    )\n",
        "\n",
        "def run_correlation_suite(dataset, target_col, target_name):\n",
        "    if dataset is None or dataset.empty or target_col not in dataset.columns:\n",
        "        print(f\"[WARN] Missing target {target_col} or dataset empty; skipping {target_name}.\")\n",
        "        return []\n",
        "\n",
        "    print(\"\\n\" + \"-\"*90)\n",
        "    print(f\"TARGET: {target_name.upper()}  |  rows used (after no-strom filter): {len(dataset)}\")\n",
        "    print(\"-\"*90)\n",
        "    print(\"{:<35} {:>8} {:>10} {:>8} {:>10} {:>5}\".format(\"Variable\", \"rho\", \"p(rho)\", \"r\", \"p(r)\", \"n\"))\n",
        "\n",
        "    results = []\n",
        "    y = pd.to_numeric(dataset[target_col], errors='coerce').values.astype(float)\n",
        "\n",
        "    for group_name, group_vars in var_groups:\n",
        "        for var, label in group_vars:\n",
        "            if var not in dataset.columns:\n",
        "                continue\n",
        "\n",
        "            x = pd.to_numeric(dataset[var], errors='coerce').values.astype(float)\n",
        "            s = calc_stats(x, y)\n",
        "\n",
        "            # Only print/store if enough data\n",
        "            if s['n'] >= 5 and np.isfinite(s['spearman_rho']):\n",
        "                s_sig = get_significance_stars(s['spearman_p'])\n",
        "                p_sig = get_significance_stars(s['pearson_p'])\n",
        "\n",
        "                print(f\"{label:35s} {s['spearman_rho']:8.2f}{s_sig:3s} {s['spearman_p']:10.3g} \"\n",
        "                      f\"{s['pearson_r']:8.2f}{p_sig:3s} {s['pearson_p']:10.3g} {s['n']:5d}\")\n",
        "\n",
        "                results.append({\n",
        "                    'Scope': 'Stage' if dataset is df_corr else '5-Myr',\n",
        "                    'Target': target_col,\n",
        "                    'Predictor': var,\n",
        "                    'Label': label,\n",
        "                    'Group': group_name,\n",
        "                    'Spearman_Rho': s['spearman_rho'],\n",
        "                    'Spearman_P': s['spearman_p'],\n",
        "                    'Pearson_R': s['pearson_r'],\n",
        "                    'Pearson_P': s['pearson_p'],\n",
        "                    'N': int(s['n'])\n",
        "                })\n",
        "\n",
        "    return results\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. RUN: Stage + 5-Myr\n",
        "# -----------------------------------------------------------------------------\n",
        "all_results_stage = []\n",
        "for target_col, target_name in reef_targets:\n",
        "    all_results_stage.extend(run_correlation_suite(df_corr, target_col, target_name))\n",
        "\n",
        "all_results_5myr = []\n",
        "for target_col, target_name in reef_targets:\n",
        "    all_results_5myr.extend(run_correlation_suite(df_5myr_corr, target_col, target_name))\n",
        "\n",
        "stage_results_df = pd.DataFrame(all_results_stage)\n",
        "myr_results_df = pd.DataFrame(all_results_5myr)\n",
        "\n",
        "if 'OUTPUT_DIR' in globals():\n",
        "    if not stage_results_df.empty:\n",
        "        stage_results_df.to_csv(f\"{OUTPUT_DIR}/results_correlations_stage.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"\\nSaved: {OUTPUT_DIR}/results_correlations_stage.csv\")\n",
        "    else:\n",
        "        print(\"\\n[WARN] Stage correlations empty (nothing met n>=5 criteria).\")\n",
        "\n",
        "    if not myr_results_df.empty:\n",
        "        myr_results_df.to_csv(f\"{OUTPUT_DIR}/results_correlations_5myr.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"Saved: {OUTPUT_DIR}/results_correlations_5myr.csv\")\n",
        "    else:\n",
        "        print(\"[WARN] 5-Myr correlations empty (nothing met n>=5 criteria).\")\n",
        "\n",
        "# Show quick views\n",
        "display(stage_results_df if not stage_results_df.empty else pd.DataFrame())\n",
        "display(myr_results_df if not myr_results_df.empty else pd.DataFrame())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}