{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeong-HyunLee/stromatoporoid-reef/blob/main/stromatoporoid_reef_size_v16_final_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiczYjViaSKp",
        "outputId": "8ac602ca-55be-4ddc-b794-6c863c0fd5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STROMATOPOROID TURNOVER AND REEF MORPHOLOGY ANALYSIS\n",
            "WITH PEARSON AND SPEARMAN CORRELATIONS\n",
            "STAGE-LEVEL AND 5-MYR BIN ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Libraries loaded successfully!\n",
            "Output directory: ./output\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "#@title CELL 1: SETUP AND IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "# Install required packages (uncomment if needed)\n",
        "# !pip install openpyxl geopandas shapely requests\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib.lines import Line2D\n",
        "from scipy import stats\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import resample\n",
        "import statsmodels.api as sm\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up matplotlib for publication-quality vector figures\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.labelsize'] = 11\n",
        "plt.rcParams['axes.titlesize'] = 12\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['pdf.fonttype'] = 42  # TrueType fonts in PDF\n",
        "plt.rcParams['ps.fonttype'] = 42\n",
        "plt.rcParams['svg.fonttype'] = 'none'  # Text as text in SVG\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STROMATOPOROID TURNOVER AND REEF MORPHOLOGY ANALYSIS\")\n",
        "print(\"WITH PEARSON AND SPEARMAN CORRELATIONS\")\n",
        "print(\"STAGE-LEVEL AND 5-MYR BIN ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nLibraries loaded successfully!\")\n",
        "\n",
        "# Output directory\n",
        "import os\n",
        "OUTPUT_DIR = \"./output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vx2CXdOaivD",
        "outputId": "a40bbb60-c07d-41d4-a772-bbb7ffa29c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "GENERATING MACROSTRAT DATA\n",
            "======================================================================\n",
            "Generating Macrostrat data from API...\n",
            "  Fetching data for Ordovician...\n",
            "    Retrieved 2943 geological units\n",
            "  Fetching data for Silurian...\n",
            "    Retrieved 1715 geological units\n",
            "  Fetching data for Devonian...\n",
            "    Retrieved 2793 geological units\n",
            "  Combined dataset: 7451 geological units\n",
            "  ✓ Saved paleozoic_stage_data.csv\n",
            "  ✓ Saved paleozoic_5myr_data.csv\n",
            "✓ Macrostrat data ready\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "#@title CELL 2: GENERATE MACROSTRAT DATA (paleozoic_stage_data.csv, paleozoic_5myr_data.csv)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"GENERATING MACROSTRAT DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if files already exist\n",
        "macrostrat_stage_file = 'paleozoic_stage_data.csv'\n",
        "macrostrat_5myr_file = 'paleozoic_5myr_data.csv'\n",
        "\n",
        "if os.path.exists(macrostrat_stage_file) and os.path.exists(macrostrat_5myr_file):\n",
        "    print(f\"✓ {macrostrat_stage_file} already exists\")\n",
        "    print(f\"✓ {macrostrat_5myr_file} already exists\")\n",
        "    print(\"Skipping Macrostrat data generation...\")\n",
        "else:\n",
        "    print(\"Generating Macrostrat data from API...\")\n",
        "\n",
        "    import requests\n",
        "    try:\n",
        "        import geopandas as gpd\n",
        "    except ImportError:\n",
        "        print(\"Installing geopandas...\")\n",
        "        import subprocess\n",
        "        subprocess.run(['pip', 'install', 'geopandas', '-q'])\n",
        "        import geopandas as gpd\n",
        "\n",
        "    # Define Paleozoic Period Age Ranges\n",
        "    periods = {\n",
        "        \"Ordovician\": {\"start\": 485.4, \"end\": 443.8, \"color\": \"#00a9ce\"},\n",
        "        \"Silurian\": {\"start\": 443.8, \"end\": 419.2, \"color\": \"#b3e1af\"},\n",
        "        \"Devonian\": {\"start\": 419.2, \"end\": 358.9, \"color\": \"#cb8c37\"}\n",
        "    }\n",
        "\n",
        "    # Define stages\n",
        "    ordovician_stages = {\n",
        "        \"Tremadocian\": (478.6, 485.4), \"Floian\": (470.0, 478.6),\n",
        "        \"Dapingian\": (467.3, 470.0), \"Darriwilian\": (458.4, 467.3),\n",
        "        \"Sandbian\": (453.0, 458.4), \"Katian\": (445.2, 453.0),\n",
        "        \"Hirnantian\": (443.8, 445.2)\n",
        "    }\n",
        "    silurian_stages = {\n",
        "        \"Rhuddanian\": (440.8, 443.8), \"Aeronian\": (438.5, 440.8),\n",
        "        \"Telychian\": (433.4, 438.5), \"Sheinwoodian\": (430.5, 433.4),\n",
        "        \"Homerian\": (427.4, 430.5), \"Gorstian\": (425.6, 427.4),\n",
        "        \"Ludfordian\": (423.0, 425.6), \"Pridolian\": (419.2, 423.0)\n",
        "    }\n",
        "    devonian_stages = {\n",
        "        \"Lochkovian\": (410.8, 419.2), \"Pragian\": (407.6, 410.8),\n",
        "        \"Emsian\": (393.3, 407.6), \"Eifelian\": (387.7, 393.3),\n",
        "        \"Givetian\": (382.7, 387.7), \"Frasnian\": (372.2, 382.7),\n",
        "        \"Famennian\": (358.9, 372.2)\n",
        "    }\n",
        "\n",
        "    # Create stages dataframe\n",
        "    stages_data = []\n",
        "    for stage, (end_age, start_age) in ordovician_stages.items():\n",
        "        stages_data.append({\"stage\": stage, \"start_age\": start_age, \"end_age\": end_age,\n",
        "                           \"mid_age\": (start_age + end_age) / 2, \"period\": \"Ordovician\"})\n",
        "    for stage, (end_age, start_age) in silurian_stages.items():\n",
        "        stages_data.append({\"stage\": stage, \"start_age\": start_age, \"end_age\": end_age,\n",
        "                           \"mid_age\": (start_age + end_age) / 2, \"period\": \"Silurian\"})\n",
        "    for stage, (end_age, start_age) in devonian_stages.items():\n",
        "        stages_data.append({\"stage\": stage, \"start_age\": start_age, \"end_age\": end_age,\n",
        "                           \"mid_age\": (start_age + end_age) / 2, \"period\": \"Devonian\"})\n",
        "    stages_df = pd.DataFrame(stages_data)\n",
        "\n",
        "    # Retrieve Macrostrat Data\n",
        "    periods_to_fetch = [\"Ordovician\", \"Silurian\", \"Devonian\"]\n",
        "    all_units_list = []\n",
        "\n",
        "    for period in periods_to_fetch:\n",
        "        url = f\"https://macrostrat.org/api/units?interval_name={period}&format=geojson&response=long\"\n",
        "        print(f\"  Fetching data for {period}...\")\n",
        "        try:\n",
        "            response = requests.get(url, timeout=60)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                features = data.get(\"success\", {}).get(\"data\", [])\n",
        "                if features:\n",
        "                    period_units = gpd.GeoDataFrame.from_features(features)\n",
        "                    print(f\"    Retrieved {len(period_units)} geological units\")\n",
        "                    period_units['source_period'] = period\n",
        "                    all_units_list.append(period_units)\n",
        "        except Exception as e:\n",
        "            print(f\"    Error fetching {period}: {e}\")\n",
        "\n",
        "    if all_units_list:\n",
        "        units = pd.concat(all_units_list, ignore_index=True)\n",
        "        print(f\"  Combined dataset: {len(units)} geological units\")\n",
        "\n",
        "        # Process units\n",
        "        try:\n",
        "            if units.crs is None:\n",
        "                units.set_crs(epsg=4326, inplace=True)\n",
        "            units = units.to_crs(epsg=3857)\n",
        "            if 'col_area' in units.columns:\n",
        "                units['area_km2'] = pd.to_numeric(units['col_area'], errors='coerce')\n",
        "            else:\n",
        "                units['area_km2'] = units.geometry.area / 1e6\n",
        "        except:\n",
        "            units['area_km2'] = 100  # Default\n",
        "\n",
        "        units['t_age'] = pd.to_numeric(units['t_age'], errors='coerce')\n",
        "        units['b_age'] = pd.to_numeric(units['b_age'], errors='coerce')\n",
        "        units['mid_age'] = (units['t_age'] + units['b_age']) / 2.0\n",
        "        units.dropna(subset=['mid_age'], inplace=True)\n",
        "\n",
        "        # Identify carbonates\n",
        "        def check_if_carbonate(lithologies):\n",
        "            if isinstance(lithologies, list):\n",
        "                for lith in lithologies:\n",
        "                    if isinstance(lith, dict) and 'type' in lith and 'carbonate' in str(lith['type']).lower():\n",
        "                        return True\n",
        "            elif isinstance(lithologies, str):\n",
        "                return 'carbonate' in lithologies.lower()\n",
        "            return False\n",
        "\n",
        "        units['is_carbonate'] = units['lith'].apply(check_if_carbonate)\n",
        "        carbonate_units = units[units['is_carbonate']].copy()\n",
        "\n",
        "        # Assign stages\n",
        "        all_stages = {**ordovician_stages, **silurian_stages, **devonian_stages}\n",
        "        def assign_stage(age):\n",
        "            for stage, (end, start) in all_stages.items():\n",
        "                if start >= age >= end:\n",
        "                    return stage\n",
        "            return None\n",
        "\n",
        "        units['stage'] = units['mid_age'].apply(assign_stage)\n",
        "        carbonate_units['stage'] = carbonate_units['mid_age'].apply(assign_stage)\n",
        "\n",
        "        # Aggregate by stage\n",
        "        stage_totals = units.groupby('stage')['area_km2'].sum().reset_index()\n",
        "        stage_totals.rename(columns={'area_km2': 'total_area_km2'}, inplace=True)\n",
        "        stage_carbonates = carbonate_units.groupby('stage')['area_km2'].sum().reset_index()\n",
        "        stage_carbonates.rename(columns={'area_km2': 'carbonate_area_km2'}, inplace=True)\n",
        "\n",
        "        stage_summary = pd.merge(stage_totals, stage_carbonates, on='stage', how='left')\n",
        "        stage_summary['carbonate_area_km2'] = stage_summary['carbonate_area_km2'].fillna(0)\n",
        "        stage_summary['carbonate_percentage'] = (stage_summary['carbonate_area_km2'] / stage_summary['total_area_km2']) * 100\n",
        "\n",
        "        macrostrat_data = pd.merge(stages_df, stage_summary, on='stage', how='left')\n",
        "        macrostrat_data = macrostrat_data.sort_values('start_age', ascending=False).reset_index(drop=True)\n",
        "        macrostrat_data.to_csv(macrostrat_stage_file, index=False)\n",
        "        print(f\"  ✓ Saved {macrostrat_stage_file}\")\n",
        "\n",
        "        # 5 Myr bins\n",
        "        max_age = 490\n",
        "        min_age = 355\n",
        "        manual_bins = np.arange(min_age, max_age + 5, 5)\n",
        "\n",
        "        units['time_bin'] = pd.cut(units['mid_age'], bins=manual_bins, include_lowest=True, right=False)\n",
        "        carbonate_units['time_bin'] = pd.cut(carbonate_units['mid_age'], bins=manual_bins, include_lowest=True, right=False)\n",
        "\n",
        "        macro_all_5myr = units.groupby('time_bin')['area_km2'].sum().reset_index()\n",
        "        macro_all_5myr.rename(columns={'area_km2': 'total_area_km2'}, inplace=True)\n",
        "        macro_carb_5myr = carbonate_units.groupby('time_bin')['area_km2'].sum().reset_index()\n",
        "        macro_carb_5myr.rename(columns={'area_km2': 'carbonate_area_km2'}, inplace=True)\n",
        "\n",
        "        macrostrat_5myr = pd.merge(macro_all_5myr, macro_carb_5myr, on='time_bin', how='left')\n",
        "        macrostrat_5myr['carbonate_area_km2'] = macrostrat_5myr['carbonate_area_km2'].fillna(0)\n",
        "        macrostrat_5myr['carbonate_percentage'] = (macrostrat_5myr['carbonate_area_km2'] / macrostrat_5myr['total_area_km2']) * 100\n",
        "        macrostrat_5myr['bin_mid'] = macrostrat_5myr['time_bin'].apply(lambda x: (x.left + x.right) / 2 if pd.notna(x) else np.nan)\n",
        "        macrostrat_5myr.to_csv(macrostrat_5myr_file, index=False)\n",
        "        print(f\"  ✓ Saved {macrostrat_5myr_file}\")\n",
        "    else:\n",
        "        print(\"  WARNING: Could not fetch Macrostrat data. Creating placeholder files...\")\n",
        "        # Create placeholder files\n",
        "        pd.DataFrame(columns=['stage', 'total_area_km2', 'carbonate_area_km2', 'carbonate_percentage']).to_csv(macrostrat_stage_file, index=False)\n",
        "        pd.DataFrame(columns=['bin_mid', 'total_area_km2', 'carbonate_area_km2', 'carbonate_percentage']).to_csv(macrostrat_5myr_file, index=False)\n",
        "\n",
        "print(\"✓ Macrostrat data ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "Fk40AhaealAV",
        "outputId": "07e2ceae-36b0-403b-cc58-49d727cfb85f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "GENERATING PARED REEF DATA\n",
            "======================================================================\n",
            "Source file 'PARED_reef_All_numerical.csv' not found.\n",
            "Please upload it now:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dc15009b-16da-4d22-a8dd-15761e54a248\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dc15009b-16da-4d22-a8dd-15761e54a248\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# =============================================================================\n",
        "#@title CELL 3: GENERATE PARED REEF DATA (reef stage and 5myr files)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING PARED REEF DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "reef_stage_file = 'ordovician_devonian_reef_data_stage_for_analysis.csv'\n",
        "reef_5myr_file = 'ordovician_devonian_reef_data_5myr_for_analysis.csv'\n",
        "pared_source_file = 'PARED_reef_All_numerical.csv'\n",
        "\n",
        "# Force regeneration to include new variable\n",
        "if os.path.exists(reef_stage_file) and os.path.exists(reef_5myr_file) and False:\n",
        "    print(f\"✓ {reef_stage_file} already exists\")\n",
        "    print(f\"✓ {reef_5myr_file} already exists\")\n",
        "    print(\"Skipping PARED reef data generation...\")\n",
        "else:\n",
        "    # Check if source file exists\n",
        "    if not os.path.exists(pared_source_file):\n",
        "        print(f\"Source file '{pared_source_file}' not found.\")\n",
        "        print(\"Please upload it now:\")\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            uploaded_pared = files.upload()\n",
        "            uploaded_name = list(uploaded_pared.keys())[0]\n",
        "            if uploaded_name != pared_source_file:\n",
        "                os.rename(uploaded_name, pared_source_file)\n",
        "        except ImportError:\n",
        "            raise FileNotFoundError(f\"Please place '{pared_source_file}' in the current directory.\")\n",
        "\n",
        "    print(f\"Processing {pared_source_file}...\")\n",
        "\n",
        "    # Load PARED data\n",
        "    try:\n",
        "        pared_df = pd.read_csv(pared_source_file, encoding='utf-8')\n",
        "    except UnicodeDecodeError:\n",
        "        try:\n",
        "            pared_df = pd.read_csv(pared_source_file, encoding='latin-1')\n",
        "        except:\n",
        "            pared_df = pd.read_csv(pared_source_file, encoding='cp1252')\n",
        "\n",
        "    # --- NEW CODE: Calculate Paired Ratio (Log Difference) ---\n",
        "    # Ensure numeric\n",
        "    pared_df['thickness'] = pd.to_numeric(pared_df['thickness'], errors='coerce')\n",
        "    pared_df['width'] = pd.to_numeric(pared_df['width'], errors='coerce')\n",
        "\n",
        "    # Calculate difference (Log Thickness - Log Width)\n",
        "    # This automatically becomes NaN if either value is missing\n",
        "    pared_df['t_w_log_ratio'] = pared_df['thickness'] - pared_df['width']\n",
        "    # ---------------------------------------------------------\n",
        "\n",
        "    def analyze_pared_data(dataframe, bin_definitions, analysis_type_label):\n",
        "        \"\"\"Calculate statistics using OVERLAP method\"\"\"\n",
        "        results = []\n",
        "        # Added 't_w_log_ratio' to variables\n",
        "        variables = ['thickness', 'width', 'extension', 't_w_log_ratio']\n",
        "\n",
        "        for bin_def in bin_definitions:\n",
        "            s_start = bin_def['start_ma']\n",
        "            s_end = bin_def['end_ma']\n",
        "            name = bin_def['time_identifier']\n",
        "\n",
        "            # OVERLAP LOGIC\n",
        "            mask = (dataframe['min_ma'] < s_end) & (dataframe['max_ma'] > s_start)\n",
        "            subset = dataframe[mask]\n",
        "\n",
        "            row_data = {\n",
        "                'bin_center': (s_start + s_end) / 2.0,\n",
        "                'start_age': s_start,\n",
        "                'end_age': s_end,\n",
        "                'reef_count': len(subset),\n",
        "            }\n",
        "\n",
        "            for var in variables:\n",
        "                data = subset[var].dropna() if var in subset.columns else pd.Series()\n",
        "                if len(data) > 0:\n",
        "                    row_data[f'{var}_mean'] = data.mean()\n",
        "                    row_data[f'{var}_std'] = data.std()\n",
        "                    row_data[f'{var}_stderr'] = data.sem()\n",
        "                    row_data[f'{var}_median'] = data.median()\n",
        "                    row_data[f'{var}_count'] = len(data)\n",
        "                else:\n",
        "                    for suffix in ['mean', 'std', 'stderr', 'median']:\n",
        "                        row_data[f'{var}_{suffix}'] = np.nan\n",
        "                    row_data[f'{var}_count'] = 0\n",
        "\n",
        "            row_data['analysis_type'] = analysis_type_label\n",
        "            row_data['time_identifier'] = name\n",
        "            row_data['name'] = name\n",
        "            row_data['start_ma'] = s_start\n",
        "            row_data['end_ma'] = s_end\n",
        "            row_data['midpoint_ma'] = (s_start + s_end) / 2.0\n",
        "            row_data['duration_myr'] = round(s_end - s_start, 1)\n",
        "\n",
        "            results.append(row_data)\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    # Stage definitions (PRESERVED FROM ORIGINAL)\n",
        "    stages_data = [\n",
        "        {'time_identifier': 'Famennian', 'start_ma': 358.9, 'end_ma': 372.2},\n",
        "        {'time_identifier': 'Frasnian', 'start_ma': 372.2, 'end_ma': 382.7},\n",
        "        {'time_identifier': 'Givetian', 'start_ma': 382.7, 'end_ma': 387.7},\n",
        "        {'time_identifier': 'Eifelian', 'start_ma': 387.7, 'end_ma': 393.3},\n",
        "        {'time_identifier': 'Emsian', 'start_ma': 393.3, 'end_ma': 407.6},\n",
        "        {'time_identifier': 'Pragian', 'start_ma': 407.6, 'end_ma': 410.8},\n",
        "        {'time_identifier': 'Lochkovian', 'start_ma': 410.8, 'end_ma': 419.2},\n",
        "        {'time_identifier': 'Pridoli', 'start_ma': 419.2, 'end_ma': 423.0},\n",
        "        {'time_identifier': 'Ludfordian', 'start_ma': 423.0, 'end_ma': 425.6},\n",
        "        {'time_identifier': 'Gorstian', 'start_ma': 425.6, 'end_ma': 427.4},\n",
        "        {'time_identifier': 'Homerian', 'start_ma': 427.4, 'end_ma': 430.5},\n",
        "        {'time_identifier': 'Sheinwoodian', 'start_ma': 430.5, 'end_ma': 433.4},\n",
        "        {'time_identifier': 'Telychian', 'start_ma': 433.4, 'end_ma': 438.5},\n",
        "        {'time_identifier': 'Aeronian', 'start_ma': 438.5, 'end_ma': 440.8},\n",
        "        {'time_identifier': 'Rhuddanian', 'start_ma': 440.8, 'end_ma': 443.8},\n",
        "        {'time_identifier': 'Hirnantian', 'start_ma': 443.8, 'end_ma': 445.2},\n",
        "        {'time_identifier': 'Katian', 'start_ma': 445.2, 'end_ma': 453.0},\n",
        "        {'time_identifier': 'Sandbian', 'start_ma': 453.0, 'end_ma': 458.4},\n",
        "        {'time_identifier': 'Darriwilian', 'start_ma': 458.4, 'end_ma': 467.3},\n",
        "        {'time_identifier': 'Dapingian', 'start_ma': 467.3, 'end_ma': 470.0},\n",
        "        {'time_identifier': 'Floian', 'start_ma': 470.0, 'end_ma': 477.7},\n",
        "        {'time_identifier': 'Tremadocian', 'start_ma': 477.7, 'end_ma': 485.4},\n",
        "    ]\n",
        "\n",
        "    # 5-Myr bins\n",
        "    bins_5myr = []\n",
        "    for age in range(355, 490, 5):\n",
        "        bins_5myr.append({\n",
        "            'time_identifier': f\"{age}-{age+5} Ma\",\n",
        "            'start_ma': float(age),\n",
        "            'end_ma': float(age + 5)\n",
        "        })\n",
        "\n",
        "    # Generate stage data\n",
        "    df_stages = analyze_pared_data(pared_df, stages_data, 'Geological_Stages')\n",
        "    df_stages.to_csv(reef_stage_file, index=False)\n",
        "    print(f\"  ✓ Generated {reef_stage_file}\")\n",
        "\n",
        "    # Generate 5myr data\n",
        "    df_5myr = analyze_pared_data(pared_df, bins_5myr, '5_myr_bins')\n",
        "    df_5myr.to_csv(reef_5myr_file, index=False)\n",
        "    print(f\"  ✓ Generated {reef_5myr_file}\")\n",
        "\n",
        "print(\"✓ PARED reef data ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-CR5FkScbhfW"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 4: GENERATE PBDB DIVERSITY AND OCCURRENCE DATA (GENERIC 485.0 BINS)\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP: Create Time References\n",
        "# ==========================================\n",
        "ics_data = \"\"\"stage,series,period,start_ma,end_ma\n",
        "Tremadocian,Lower Ordovician,Ordovician,485.4,477.7\n",
        "Floian,Lower Ordovician,Ordovician,477.7,470\n",
        "Dapingian,Middle Ordovician,Ordovician,470,467.3\n",
        "Darriwilian,Middle Ordovician,Ordovician,467.3,458.4\n",
        "Sandbian,Upper Ordovician,Ordovician,458.4,453\n",
        "Katian,Upper Ordovician,Ordovician,453,445.2\n",
        "Hirnantian,Upper Ordovician,Ordovician,445.2,443.8\n",
        "Rhuddanian,Llandovery,Silurian,443.8,440.8\n",
        "Aeronian,Llandovery,Silurian,440.8,438.5\n",
        "Telychian,Llandovery,Silurian,438.5,433.4\n",
        "Sheinwoodian,Wenlock,Silurian,433.4,430.5\n",
        "Homerian,Wenlock,Silurian,430.5,427.4\n",
        "Gorstian,Ludlow,Silurian,427.4,425.6\n",
        "Ludfordian,Ludlow,Silurian,425.6,423\n",
        "Pridoli,Pridoli,Silurian,423,419.2\n",
        "Lochkovian,Lower Devonian,Devonian,419.2,410.8\n",
        "Pragian,Lower Devonian,Devonian,410.8,407.6\n",
        "Emsian,Lower Devonian,Devonian,407.6,393.3\n",
        "Eifelian,Middle Devonian,Devonian,393.3,387.7\n",
        "Givetian,Middle Devonian,Devonian,387.7,382.7\n",
        "Frasnian,Upper Devonian,Devonian,382.7,372.2\n",
        "Famennian,Upper Devonian,Devonian,372.2,358.9\"\"\"\n",
        "\n",
        "with open(\"ICS_stage_boundaries.csv\", \"w\") as f:\n",
        "    f.write(ics_data)\n",
        "\n",
        "# MODIFIED: Start at 485.0 to align with generic grid\n",
        "def create_5myr_bins(start_ma=485.0, end_ma=358.9, step=5.0):\n",
        "    bins = []\n",
        "    current = start_ma\n",
        "    # Ensure we cover down to the end_ma\n",
        "    while current > end_ma - step:\n",
        "        # Logic: Stop if the bottom of the bin is way below end_ma?\n",
        "        # Typically we want the bin containing end_ma (358.9).\n",
        "        # Bin 360-355 covers 358.9. 355 is < 358.9? No.\n",
        "        # Let's keep standard logic:\n",
        "        if current < 360 and current < end_ma: break # Safety break\n",
        "\n",
        "        top = current\n",
        "        bottom = current - step\n",
        "\n",
        "        # Check if we are going too far (e.g. into Carboniferous)\n",
        "        # We want to stop after covering Famennian (ends 358.9).\n",
        "        # Bin 365-360: Covers 360+.\n",
        "        # Bin 360-355: Covers 358.9.\n",
        "        # Bin 355-350: Unnecessary.\n",
        "\n",
        "        label = f\"{top:.1f}-{bottom:.1f}\"\n",
        "        bins.append({'bin_label': label, 'bin_top': top, 'bin_bottom': bottom})\n",
        "\n",
        "        # Break if this bin covered the end of the period\n",
        "        if bottom < end_ma:\n",
        "             current = bottom\n",
        "             break\n",
        "\n",
        "        current = bottom\n",
        "    return pd.DataFrame(bins)\n",
        "\n",
        "bins_5myr_df = create_5myr_bins()\n",
        "stages_df = pd.read_csv(\"ICS_stage_boundaries.csv\")\n",
        "\n",
        "print(\"Created Generic 5-Myr bins (Aligned to 485.0):\")\n",
        "print(bins_5myr_df.head())\n",
        "print(bins_5myr_df.tail())\n",
        "\n",
        "# ==========================================\n",
        "# 2. FILE CHECK\n",
        "# ==========================================\n",
        "print(\"\\n--- CHECKING FILE SYSTEM ---\")\n",
        "found_files = []\n",
        "for f in os.listdir('.'):\n",
        "    if f.lower().startswith(\"pbdb_data_\") and f.lower().endswith(\".csv\"):\n",
        "        found_files.append(f)\n",
        "\n",
        "if not found_files:\n",
        "    print(\"No 'pbdb_data_*.csv' files found. Please upload your RAW PBDB files now.\")\n",
        "    files.upload()\n",
        "    found_files = [f for f in os.listdir('.') if f.lower().startswith(\"pbdb_data_\") and f.lower().endswith(\".csv\")]\n",
        "\n",
        "print(f\"Found {len(found_files)} files: {found_files}\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def smart_read_pbdb(file_path):\n",
        "    header_row = None\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "            lines = [f.readline() for _ in range(50)]\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"occurrence_no\" in line:\n",
        "                header_row = i\n",
        "                break\n",
        "        if header_row is None:\n",
        "            print(f\"    CRITICAL: Could not find 'occurrence_no' header in {file_path}\")\n",
        "            return None\n",
        "        return pd.read_csv(file_path, header=header_row)\n",
        "    except Exception as e:\n",
        "        print(f\"    Error reading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_genus(accepted_name):\n",
        "    if pd.isna(accepted_name): return None\n",
        "    return str(accepted_name).split(' ')[0]\n",
        "\n",
        "def get_stage_from_age(age, stages_df):\n",
        "    match = stages_df[(stages_df['start_ma'] >= age) & (stages_df['end_ma'] < age)]\n",
        "    if match.empty:\n",
        "        if abs(age - stages_df['end_ma'].min()) < 0.001: return stages_df.iloc[-1]['stage']\n",
        "    if not match.empty: return match.iloc[0]['stage']\n",
        "    return None\n",
        "\n",
        "def get_bin_from_age(age, bins_df):\n",
        "    # Strict containment\n",
        "    match = bins_df[(bins_df['bin_top'] >= age) & (bins_df['bin_bottom'] < age)]\n",
        "\n",
        "    # MODIFIED: Tolerance/Snap for oldest points\n",
        "    # If age is slightly older than the top bin (e.g. 485.4 vs 485.0), snap it to the first bin\n",
        "    if match.empty:\n",
        "        max_top = bins_df['bin_top'].max()\n",
        "        if age > max_top and (age - max_top) < 1.5: # 1.5 Ma tolerance for Tremadocian start\n",
        "             return bins_df.iloc[0]['bin_label']\n",
        "\n",
        "    if not match.empty:\n",
        "        return match.iloc[0]['bin_label']\n",
        "    return None\n",
        "\n",
        "def process_midpoint(df, group_name, reference_df, ref_type=\"stage\"):\n",
        "    if 'accepted_name' not in df.columns:\n",
        "        print(f\"    Error: 'accepted_name' column missing.\")\n",
        "        return None\n",
        "\n",
        "    df['genus_name'] = df['accepted_name'].apply(extract_genus)\n",
        "    df = df.dropna(subset=['genus_name'])\n",
        "\n",
        "    # Midpoint Logic\n",
        "    df['midpoint'] = (df['max_ma'] + df['min_ma']) / 2\n",
        "\n",
        "    if ref_type == \"stage\":\n",
        "        df['assigned_interval'] = df['midpoint'].apply(lambda x: get_stage_from_age(x, reference_df))\n",
        "        merge_col = 'stage'\n",
        "    else:\n",
        "        df['assigned_interval'] = df['midpoint'].apply(lambda x: get_bin_from_age(x, reference_df))\n",
        "        merge_col = 'bin_label'\n",
        "\n",
        "    df = df.dropna(subset=['assigned_interval'])\n",
        "\n",
        "    # Aggregation\n",
        "    genus_counts = df.groupby('assigned_interval')['genus_name'].nunique()\n",
        "    genus_counts.name = f'{group_name}_genus'\n",
        "    occ_counts = df.groupby('assigned_interval')['occurrence_no'].nunique()\n",
        "    occ_counts.name = f'{group_name}_occ'\n",
        "\n",
        "    # Merging\n",
        "    final_df = reference_df.copy()\n",
        "    final_df = final_df.merge(genus_counts, left_on=merge_col, right_index=True, how='left')\n",
        "    final_df = final_df.merge(occ_counts, left_on=merge_col, right_index=True, how='left')\n",
        "\n",
        "    # Cleanup\n",
        "    cols_to_fix = [f'{group_name}_genus', f'{group_name}_occ']\n",
        "    final_df[cols_to_fix] = final_df[cols_to_fix].fillna(0).astype(int)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# ==========================================\n",
        "# 4. MAIN EXECUTION LOOP\n",
        "# ==========================================\n",
        "print(\"\\n--- STARTING ANALYSIS (Generic 485.0 Bins) ---\")\n",
        "\n",
        "for file_path in found_files:\n",
        "    filename = os.path.basename(file_path)\n",
        "    group_name = filename.replace(\"pbdb_data_\", \"\").replace(\".csv\", \"\")\n",
        "    print(f\"\\nAnalyzing {group_name}...\")\n",
        "\n",
        "    df = smart_read_pbdb(file_path)\n",
        "    if df is not None:\n",
        "        try:\n",
        "            # A. Stages\n",
        "            stage_df = process_midpoint(df.copy(), group_name, stages_df, \"stage\")\n",
        "            if stage_df is not None:\n",
        "                out_stage = f\"pbdb_{group_name}_midpoint_stages.csv\"\n",
        "                stage_df.to_csv(out_stage, index=False)\n",
        "                print(f\"  -> Created: {out_stage}\")\n",
        "\n",
        "            # B. 5-Myr Bins\n",
        "            bin_df = process_midpoint(df.copy(), group_name, bins_5myr_df, \"bin\")\n",
        "            if bin_df is not None:\n",
        "                out_bin = f\"pbdb_{group_name}_midpoint_5myr_bins.csv\"\n",
        "                bin_df.to_csv(out_bin, index=False)\n",
        "                print(f\"  -> Created: {out_bin}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"DONE! New 485.0-aligned bin files created.\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OHVXM-Zdbn9n"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "#@title CELL 5: UPLOAD ENVIRONMENT DATA FILES (Google Colab) - CONDITIONAL\n",
        "# =============================================================================\n",
        "\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Define required files\n",
        "required_files = [\n",
        "    'temperature.csv',\n",
        "    'DO.csv',\n",
        "    'oxygen.csv',\n",
        "    'sealevel.csv',\n",
        "    'd13C_5Myr_Cam-Dev.csv',\n",
        "    'd13C_stage_binned_Cam-Dev.csv'\n",
        "]\n",
        "# Check which files are missing\n",
        "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "\n",
        "if missing_files:\n",
        "    print(\"The following files are missing and need to be uploaded:\")\n",
        "    for i, f in enumerate(missing_files, 1):\n",
        "        print(f\"  {i}. {f}\")\n",
        "    print(\"\\nClick 'Choose Files' and select the missing files...\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    print(f\"\\n✓ Uploaded {len(uploaded)} files:\")\n",
        "    for fn in uploaded.keys():\n",
        "        print(f\"  - {fn}\")\n",
        "else:\n",
        "    print(\"✓ All required files already exist in the folder\")\n",
        "    uploaded = {}\n",
        "    # Load existing files into uploaded dict for compatibility\n",
        "    for f in required_files:\n",
        "        with open(f, 'rb') as file:\n",
        "            uploaded[f] = file.read()\n",
        "\n",
        "print(f\"\\n✓ {len(required_files)} files ready for analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_J9_ZRvYbrYT"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 6: LOAD AND PROCESS DATA (FROM CONTENT FOLDER)\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Helper: Find file path by keywords in the current directory\n",
        "def get_file_path_robust(keywords, search_dir='.'):\n",
        "    \"\"\"\n",
        "    Finds a filename in the search_dir that contains ALL keywords (case-insensitive).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        files = os.listdir(search_dir)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  ! Error: Directory '{search_dir}' not found.\")\n",
        "        return None\n",
        "\n",
        "    for filename in files:\n",
        "        if not filename.endswith(('.csv', '.xlsx', '.xls')):\n",
        "            continue\n",
        "        # Check if ALL keywords are present in this filename\n",
        "        if all(str(k).lower() in filename.lower() for k in keywords):\n",
        "            return os.path.join(search_dir, filename)\n",
        "    return None\n",
        "\n",
        "def load_and_merge_from_disk(target_list, merge_cols, dataset_name):\n",
        "    \"\"\"\n",
        "    Iterates through a list of target filenames, finds them on disk, and merges them.\n",
        "    \"\"\"\n",
        "    merged_df = None\n",
        "    print(f\"\\nProcessing {dataset_name}...\")\n",
        "\n",
        "    for target in target_list:\n",
        "        # Extract keywords from the target filename\n",
        "        clean_name = target.replace('.csv', '').replace('.xlsx', '')\n",
        "        keywords = [k for k in clean_name.split('_') if k and k.lower() != 'pbdb']\n",
        "\n",
        "        # Search for the file\n",
        "        filepath = get_file_path_robust(keywords)\n",
        "\n",
        "        if filepath:\n",
        "            print(f\"  ✓ Found: {filepath}\")\n",
        "            try:\n",
        "                df = pd.read_csv(filepath)\n",
        "                df.columns = df.columns.str.strip()\n",
        "\n",
        "                if merged_df is None:\n",
        "                    merged_df = df\n",
        "                else:\n",
        "                    merged_df = pd.merge(merged_df, df, on=merge_cols, how='outer')\n",
        "            except Exception as e:\n",
        "                print(f\"  ! Error reading {filepath}: {e}\")\n",
        "        else:\n",
        "            # Retry with minimal keywords (Taxon + Resolution)\n",
        "            # This handles cases where the user filename might differ slightly from the instruction\n",
        "            taxon = next((k for k in keywords if k.lower() not in ['midpoint', 'stages', '5myr', 'bins']), None)\n",
        "            resolution = '5myr' if '5myr' in target.lower() else 'stages'\n",
        "\n",
        "            if taxon:\n",
        "                filepath_retry = get_file_path_robust([taxon, resolution])\n",
        "                if filepath_retry:\n",
        "                    print(f\"  ✓ Found (fallback): {filepath_retry}\")\n",
        "                    try:\n",
        "                        df = pd.read_csv(filepath_retry)\n",
        "                        df.columns = df.columns.str.strip()\n",
        "                        if merged_df is None: merged_df = df\n",
        "                        else: merged_df = pd.merge(merged_df, df, on=merge_cols, how='outer')\n",
        "                    except Exception as e:\n",
        "                        print(f\"  ! Error reading {filepath_retry}: {e}\")\n",
        "                else:\n",
        "                     print(f\"  x Could not find file for: {taxon} ({resolution})\")\n",
        "            else:\n",
        "                 print(f\"  x Could not find file matching: {keywords}\")\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "# Common merge columns\n",
        "stage_merge_cols = ['stage', 'series', 'period', 'start_ma', 'end_ma']\n",
        "bin_merge_cols = ['bin_label', 'bin_top', 'bin_bottom']\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Substitute Stromatoporoid Data (Stage)\n",
        "# =============================================================================\n",
        "strom_stage_targets = [\n",
        "    \"pbdb_Stromatoporida_midpoint_stages.csv\",\n",
        "    \"pbdb_Labechiida_midpoint_stages.csv\",\n",
        "    \"pbdb_Actinostromatida_midpoint_stages.csv\",\n",
        "    \"pbdb_clathrodictyida_midpoint_stages.csv\",\n",
        "    \"pbdb_Syringostromatida_midpoint_stages.csv\",\n",
        "    \"pbdb_Stromatoporellida_midpoint_stages.csv\",\n",
        "    \"pbdb_Amphiporida_midpoint_stages.csv\"\n",
        "]\n",
        "\n",
        "strom_df = load_and_merge_from_disk(strom_stage_targets, stage_merge_cols, \"Stromatoporoid (Stage)\")\n",
        "\n",
        "if strom_df is not None:\n",
        "    strom_df = strom_df.fillna(0)\n",
        "    # Calculate Totals\n",
        "    genus_cols = [c for c in strom_df.columns if c.endswith('_genus')]\n",
        "    occ_cols = [c for c in strom_df.columns if c.endswith('_occ')]\n",
        "    strom_df['Total_genus'] = strom_df[genus_cols].sum(axis=1)\n",
        "    strom_df['Total_occ'] = strom_df[occ_cols].sum(axis=1)\n",
        "\n",
        "    # === FILTER: Remove empty stages ===\n",
        "    dropped_s = len(strom_df[strom_df['Total_occ'] == 0])\n",
        "    strom_df = strom_df[strom_df['Total_occ'] > 0].copy()\n",
        "    if dropped_s > 0:\n",
        "        print(f\"  -> Dropped {dropped_s} Stromatoporoid stage(s) with zero occurrences.\")\n",
        "    # ===================================\n",
        "\n",
        "    print(f\"  -> Merged Stromatoporoid Stages. Rows: {len(strom_df)}\")\n",
        "else:\n",
        "    print(\"  ! Error: Stromatoporoid dataframe is empty.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 2. Substitute Coral Data (Stage)\n",
        "# =============================================================================\n",
        "coral_stage_targets = [\n",
        "    \"pbdb_tabulata_midpoint_stages.csv\",\n",
        "    \"pbdb_Rugosa_midpoint_stages.csv\"\n",
        "]\n",
        "\n",
        "coral_df = load_and_merge_from_disk(coral_stage_targets, stage_merge_cols, \"Coral (Stage)\")\n",
        "\n",
        "if coral_df is not None:\n",
        "    coral_df = coral_df.fillna(0)\n",
        "\n",
        "    # Calculate Totals (Added for filtering)\n",
        "    c_genus_cols = [c for c in coral_df.columns if c.endswith('_genus')]\n",
        "    c_occ_cols = [c for c in coral_df.columns if c.endswith('_occ')]\n",
        "    coral_df['Total_genus'] = coral_df[c_genus_cols].sum(axis=1)\n",
        "    coral_df['Total_occ'] = coral_df[c_occ_cols].sum(axis=1)\n",
        "\n",
        "    # === FILTER: Remove empty stages ===\n",
        "    dropped_c = len(coral_df[coral_df['Total_occ'] == 0])\n",
        "    coral_df = coral_df[coral_df['Total_occ'] > 0].copy()\n",
        "    if dropped_c > 0:\n",
        "        print(f\"  -> Dropped {dropped_c} Coral stage(s) with zero occurrences.\")\n",
        "    # ===================================\n",
        "\n",
        "    print(f\"  -> Merged Coral Stages. Rows: {len(coral_df)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 3. Create New Dataset for 5-Myr Bins\n",
        "# =============================================================================\n",
        "# Stromatoporoids\n",
        "strom_bin_targets = [\n",
        "    \"pbdb_Stromatoporida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Labechiida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Actinostromatida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_clathrodictyida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Syringostromatida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Stromatoporellida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Amphiporida_midpoint_5myr_bins.csv\"\n",
        "]\n",
        "\n",
        "strom_5myr_df = load_and_merge_from_disk(strom_bin_targets, bin_merge_cols, \"Stromatoporoid (5-Myr)\")\n",
        "\n",
        "if strom_5myr_df is not None:\n",
        "    strom_5myr_df = strom_5myr_df.fillna(0)\n",
        "    genus_cols = [c for c in strom_5myr_df.columns if c.endswith('_genus')]\n",
        "    occ_cols = [c for c in strom_5myr_df.columns if c.endswith('_occ')]\n",
        "    strom_5myr_df['Total_genus'] = strom_5myr_df[genus_cols].sum(axis=1)\n",
        "    strom_5myr_df['Total_occ'] = strom_5myr_df[occ_cols].sum(axis=1)\n",
        "\n",
        "    # === FILTER: Remove empty bins ===\n",
        "    dropped_s5 = len(strom_5myr_df[strom_5myr_df['Total_occ'] == 0])\n",
        "    strom_5myr_df = strom_5myr_df[strom_5myr_df['Total_occ'] > 0].copy()\n",
        "    if dropped_s5 > 0:\n",
        "        print(f\"  -> Dropped {dropped_s5} Stromatoporoid 5-Myr bin(s) with zero occurrences.\")\n",
        "    # =================================\n",
        "\n",
        "    print(f\"  -> Merged Stromatoporoid 5-Myr Bins. Rows: {len(strom_5myr_df)}\")\n",
        "\n",
        "# Corals\n",
        "coral_bin_targets = [\n",
        "    \"pbdb_tabulata_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Rugosa_midpoint_5myr_bins.csv\"\n",
        "]\n",
        "coral_5myr_df = load_and_merge_from_disk(coral_bin_targets, bin_merge_cols, \"Coral (5-Myr)\")\n",
        "\n",
        "if coral_5myr_df is not None:\n",
        "    coral_5myr_df = coral_5myr_df.fillna(0)\n",
        "\n",
        "    # Calculate Totals (Added for filtering)\n",
        "    c5_genus_cols = [c for c in coral_5myr_df.columns if c.endswith('_genus')]\n",
        "    c5_occ_cols = [c for c in coral_5myr_df.columns if c.endswith('_occ')]\n",
        "    coral_5myr_df['Total_genus'] = coral_5myr_df[c5_genus_cols].sum(axis=1)\n",
        "    coral_5myr_df['Total_occ'] = coral_5myr_df[c5_occ_cols].sum(axis=1)\n",
        "\n",
        "    # === FILTER: Remove empty bins ===\n",
        "    dropped_c5 = len(coral_5myr_df[coral_5myr_df['Total_occ'] == 0])\n",
        "    coral_5myr_df = coral_5myr_df[coral_5myr_df['Total_occ'] > 0].copy()\n",
        "    if dropped_c5 > 0:\n",
        "        print(f\"  -> Dropped {dropped_c5} Coral 5-Myr bin(s) with zero occurrences.\")\n",
        "    # =================================\n",
        "\n",
        "    print(f\"  -> Merged Coral 5-Myr Bins. Rows: {len(coral_5myr_df)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 4. Load Remaining Contextual Data\n",
        "# =============================================================================\n",
        "print(\"\\nLoading Contextual Data...\")\n",
        "\n",
        "def quick_load(keywords):\n",
        "    path = get_file_path_robust(keywords)\n",
        "    return pd.read_csv(path) if path else pd.DataFrame()\n",
        "\n",
        "reef_df = quick_load([\"reef_data\", \"stage\"])\n",
        "reef_5myr_df = quick_load([\"reef_data\", \"5myr\"])\n",
        "\n",
        "macro_stage = quick_load([\"paleozoic_stage_data\"])\n",
        "if 'stage' in macro_stage.columns: macro_stage['stage'] = macro_stage['stage'].replace('Pridolian', 'Pridoli')\n",
        "\n",
        "macro_5myr = quick_load([\"paleozoic_5myr_data\"])\n",
        "\n",
        "# Environmental\n",
        "env_files = {\n",
        "    'temperature': 'temperature',\n",
        "    'do': 'DO',\n",
        "    'oxygen': 'oxygen',\n",
        "    'sealevel': 'sealevel',\n",
        "    # δ13C files (already binned; DO NOT re-bin/interpolate these)\n",
        "    'd13c_5myr': 'd13C_5Myr_Cam-Dev',\n",
        "    'd13c_stage': 'd13C_stage_binned_Cam-Dev'\n",
        "}\n",
        "env_dfs = {}\n",
        "for var, key in env_files.items():\n",
        "    df = quick_load([key])\n",
        "    if not df.empty: df.columns = df.columns.str.strip().str.replace('\\ufeff', '')\n",
        "    env_dfs[var] = df\n",
        "\n",
        "temp_df = env_dfs.get('temperature', pd.DataFrame())\n",
        "do_df = env_dfs.get('do', pd.DataFrame())\n",
        "oxygen_df = env_dfs.get('oxygen', pd.DataFrame())\n",
        "sealevel_df = env_dfs.get('sealevel', pd.DataFrame())\n",
        "d13c_5myr_df = env_dfs.get('d13c_5myr', pd.DataFrame())\n",
        "d13c_stage_df = env_dfs.get('d13c_stage', pd.DataFrame())\n",
        "print(\"\\n✓ Data loading complete.\")\n",
        "\n",
        "print(\"\\nSaving intermediate merged datasets...\")\n",
        "\n",
        "if 'strom_df' in locals() and strom_df is not None:\n",
        "    strom_df.to_csv(f\"{OUTPUT_DIR}/intermediate_strom_stage.csv\", index=False)\n",
        "if 'coral_df' in locals() and coral_df is not None:\n",
        "    coral_df.to_csv(f\"{OUTPUT_DIR}/intermediate_coral_stage.csv\", index=False)\n",
        "\n",
        "if 'strom_5myr_df' in locals() and strom_5myr_df is not None:\n",
        "    strom_5myr_df.to_csv(f\"{OUTPUT_DIR}/intermediate_strom_5myr.csv\", index=False)\n",
        "if 'coral_5myr_df' in locals() and coral_5myr_df is not None:\n",
        "    coral_5myr_df.to_csv(f\"{OUTPUT_DIR}/intermediate_coral_5myr.csv\", index=False)\n",
        "\n",
        "print(\"✓ Intermediate files saved to ./output\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZqhKwRrKbtoq"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 7: DEFINE CONSTANTS AND STAGE INFORMATION\n",
        "# =============================================================================\n",
        "\n",
        "# 1. Stage definitions (ICS 2023)\n",
        "STAGES = {\n",
        "    'Tremadocian': {'start': 485.4, 'end': 477.7, 'mid': 481.55, 'period': 'Ordovician'},\n",
        "    'Floian': {'start': 477.7, 'end': 470.0, 'mid': 473.85, 'period': 'Ordovician'},\n",
        "    'Dapingian': {'start': 470.0, 'end': 467.3, 'mid': 468.65, 'period': 'Ordovician'},\n",
        "    'Darriwilian': {'start': 467.3, 'end': 458.4, 'mid': 462.85, 'period': 'Ordovician'},\n",
        "    'Sandbian': {'start': 458.4, 'end': 453.0, 'mid': 455.7, 'period': 'Ordovician'},\n",
        "    'Katian': {'start': 453.0, 'end': 445.2, 'mid': 449.1, 'period': 'Ordovician'},\n",
        "    'Hirnantian': {'start': 445.2, 'end': 443.8, 'mid': 444.5, 'period': 'Ordovician'},\n",
        "    'Rhuddanian': {'start': 443.8, 'end': 440.8, 'mid': 442.3, 'period': 'Silurian'},\n",
        "    'Aeronian': {'start': 440.8, 'end': 438.5, 'mid': 439.65, 'period': 'Silurian'},\n",
        "    'Telychian': {'start': 438.5, 'end': 433.4, 'mid': 435.95, 'period': 'Silurian'},\n",
        "    'Sheinwoodian': {'start': 433.4, 'end': 430.5, 'mid': 431.95, 'period': 'Silurian'},\n",
        "    'Homerian': {'start': 430.5, 'end': 427.4, 'mid': 428.95, 'period': 'Silurian'},\n",
        "    'Gorstian': {'start': 427.4, 'end': 425.6, 'mid': 426.5, 'period': 'Silurian'},\n",
        "    'Ludfordian': {'start': 425.6, 'end': 423.0, 'mid': 424.3, 'period': 'Silurian'},\n",
        "    'Pridoli': {'start': 423.0, 'end': 419.2, 'mid': 421.1, 'period': 'Silurian'},\n",
        "    'Lochkovian': {'start': 419.2, 'end': 410.8, 'mid': 415.0, 'period': 'Devonian'},\n",
        "    'Pragian': {'start': 410.8, 'end': 407.6, 'mid': 409.2, 'period': 'Devonian'},\n",
        "    'Emsian': {'start': 407.6, 'end': 393.3, 'mid': 400.45, 'period': 'Devonian'},\n",
        "    'Eifelian': {'start': 393.3, 'end': 387.7, 'mid': 390.5, 'period': 'Devonian'},\n",
        "    'Givetian': {'start': 387.7, 'end': 382.7, 'mid': 385.2, 'period': 'Devonian'},\n",
        "    'Frasnian': {'start': 382.7, 'end': 372.2, 'mid': 377.45, 'period': 'Devonian'},\n",
        "    'Famennian': {'start': 372.2, 'end': 358.9, 'mid': 365.55, 'period': 'Devonian'}\n",
        "}\n",
        "STAGE_ORDER = list(STAGES.keys())\n",
        "\n",
        "# 2. Period colors (ICS standard)\n",
        "PERIOD_COLORS = {\n",
        "    'Ordovician': '#009270',\n",
        "    'Silurian': '#B3E1B6',\n",
        "    'Devonian': '#CB8C37'\n",
        "}\n",
        "\n",
        "# 3. Stromatoporoid order colors (phylogenetically informed)\n",
        "STROM_COLORS = {\n",
        "    'Labechiida': '#8B0000',       # Dark red - basal\n",
        "    'Clathrodictyida': '#CD5C5C',  # Indian red - early-diverging\n",
        "    'Actinostromatida': '#FF8C00', # Dark orange - derived\n",
        "    'Stromatoporida': '#FFD700',   # Gold - derived\n",
        "    'Stromatoporellida': '#32CD32',# Lime green - derived\n",
        "    'Syringostromatida': '#4169E1',# Royal blue - derived reef builders\n",
        "    'Amphiporida': '#9370DB'       # Medium purple - derived\n",
        "}\n",
        "STROM_ORDERS = ['Labechiida', 'Clathrodictyida', 'Actinostromatida',\n",
        "                'Stromatoporida', 'Stromatoporellida', 'Syringostromatida', 'Amphiporida']\n",
        "\n",
        "# 4. Coral colors (New definitions for Rugosa/Tabulata)\n",
        "CORAL_COLORS = {\n",
        "    'Rugosa': '#800080',    # Purple\n",
        "    'Tabulata': '#D2691E'   # Chocolate/Orange-Brown\n",
        "}\n",
        "CORAL_GROUPS = ['Rugosa', 'Tabulata']\n",
        "\n",
        "# =============================================================================\n",
        "# DATA NORMALIZATION: Ensure DataFrame columns match capitalized constants\n",
        "# =============================================================================\n",
        "# Some files were lowercase (e.g., 'clathrodictyida'), but constants are TitleCase.\n",
        "# We fix this here to prevent KeyErrors in future plotting cells.\n",
        "\n",
        "def normalize_columns(df, target_orders):\n",
        "    if df is None: return df\n",
        "\n",
        "    # Get current columns\n",
        "    cols = df.columns.tolist()\n",
        "    rename_map = {}\n",
        "\n",
        "    for order in target_orders:\n",
        "        # Check if TitleCase version exists (e.g., 'Clathrodictyida_genus')\n",
        "        title_genus = f\"{order}_genus\"\n",
        "        title_occ = f\"{order}_occ\"\n",
        "\n",
        "        # Check if LowerCase version exists (e.g., 'clathrodictyida_genus')\n",
        "        lower_genus = f\"{order.lower()}_genus\"\n",
        "        lower_occ = f\"{order.lower()}_occ\"\n",
        "\n",
        "        # If TitleCase missing but LowerCase present, map Lower -> Title\n",
        "        if title_genus not in cols and lower_genus in cols:\n",
        "            rename_map[lower_genus] = title_genus\n",
        "        if title_occ not in cols and lower_occ in cols:\n",
        "            rename_map[lower_occ] = title_occ\n",
        "\n",
        "        # Also handle \"tabulata\" (lowercase t)\n",
        "        if order == 'Tabulata' and 'tabulata_genus' in cols:\n",
        "             rename_map['tabulata_genus'] = 'Tabulata_genus'\n",
        "             rename_map['tabulata_occ'] = 'Tabulata_occ'\n",
        "\n",
        "    if rename_map:\n",
        "        print(f\"  Note: Renaming columns to match standard capitalization: {list(rename_map.keys())}\")\n",
        "        df = df.rename(columns=rename_map)\n",
        "    return df\n",
        "\n",
        "# Apply normalization to the datasets loaded in Cell 5\n",
        "if 'strom_df' in locals(): strom_df = normalize_columns(strom_df, STROM_ORDERS)\n",
        "if 'strom_5myr_df' in locals(): strom_5myr_df = normalize_columns(strom_5myr_df, STROM_ORDERS)\n",
        "if 'coral_df' in locals(): coral_df = normalize_columns(coral_df, CORAL_GROUPS)\n",
        "if 'coral_5myr_df' in locals(): coral_5myr_df = normalize_columns(coral_5myr_df, CORAL_GROUPS)\n",
        "\n",
        "print(\"✓ Constants defined and DataFrames normalized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2mRadY5BbvJm"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 8: RELOAD ENV DATA & INTERPOLATE (ROBUST FIX)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.interpolate import interp1d\n",
        "import os\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. RELOAD ENVIRONMENTAL DATA (To ensure it is not empty)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"Reloading environmental datasets...\")\n",
        "\n",
        "def load_env_file(filenames, target_cols):\n",
        "    \"\"\"Try to load a file from a list of possible names\"\"\"\n",
        "    for fname in filenames:\n",
        "        if os.path.exists(fname):\n",
        "            try:\n",
        "                df = pd.read_csv(fname)\n",
        "                df.columns = df.columns.str.strip() # clean whitespace\n",
        "                print(f\"  ✓ Loaded {fname} ({len(df)} rows)\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                print(f\"  ! Error loading {fname}: {e}\")\n",
        "    print(f\"  ! WARNING: Could not find any of {filenames}\")\n",
        "    return pd.DataFrame() # Return empty if not found\n",
        "\n",
        "# Load with specific fallbacks\n",
        "temp_df     = load_env_file(['temperature.csv', 'Temperature.csv'], ['Age', 'SST'])\n",
        "do_df       = load_env_file(['DO.csv', 'do.csv'], ['Age', 'DO'])\n",
        "oxygen_df   = load_env_file(['oxygen.csv', 'Oxygen.csv'], ['Age', 'O2'])\n",
        "sealevel_df = load_env_file(['sealevel.csv', 'Sealevel.csv'], ['Age', 'Eustatic'])\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. STANDARDIZE COLUMNS\n",
        "# -----------------------------------------------------------------------------\n",
        "def standardize_env_columns(df, name, target_age='Age', target_val=None):\n",
        "    if df.empty: return df\n",
        "\n",
        "    # Fix Age column\n",
        "    if target_age not in df.columns:\n",
        "        for candidate in ['age', 'AGE', 'Time', 'Ma', 'time']:\n",
        "            if candidate in df.columns:\n",
        "                df = df.rename(columns={candidate: target_age})\n",
        "                break\n",
        "\n",
        "    # Fix Value column\n",
        "    if target_val and target_val not in df.columns:\n",
        "        # Check case-insensitive match\n",
        "        for col in df.columns:\n",
        "            if col.lower() == target_val.lower():\n",
        "                df = df.rename(columns={col: target_val})\n",
        "                break\n",
        "\n",
        "    return df\n",
        "\n",
        "do_df = standardize_env_columns(do_df, 'Dissolved Oxygen', target_val='DO')\n",
        "temp_df = standardize_env_columns(temp_df, 'Temperature', target_val='SST')\n",
        "oxygen_df = standardize_env_columns(oxygen_df, 'Atmosphere', target_age='Age')\n",
        "sealevel_df = standardize_env_columns(sealevel_df, 'Sea Level', target_val='Eustatic Sea Level')\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. INTERPOLATION\n",
        "# -----------------------------------------------------------------------------\n",
        "def interpolate_to_ages(source_df, age_col, value_col, target_ages):\n",
        "    \"\"\"Interpolate values to target ages, skipping if columns missing.\"\"\"\n",
        "    # Check if empty or missing columns\n",
        "    if source_df.empty or age_col not in source_df.columns or value_col not in source_df.columns:\n",
        "        return np.full_like(target_ages, np.nan)\n",
        "\n",
        "    # Drop NaNs in source\n",
        "    source_df = source_df.dropna(subset=[age_col, value_col])\n",
        "    # Sort by Age (Crucial for interp1d)\n",
        "    source_df = source_df.sort_values(age_col)\n",
        "\n",
        "    if len(source_df) < 2:\n",
        "        return np.full_like(target_ages, np.nan)\n",
        "\n",
        "    # Interpolate\n",
        "    f = interp1d(source_df[age_col], source_df[value_col],\n",
        "                 kind='linear', fill_value='extrapolate', bounds_error=False)\n",
        "    return f(target_ages)\n",
        "\n",
        "# Get Stage Midpoints from Cell 7 constants\n",
        "stage_midpoints = np.array([STAGES[s]['mid'] for s in STAGE_ORDER])\n",
        "\n",
        "print(\"\\nInterpolating environmental proxies to stage midpoints...\")\n",
        "env_data = pd.DataFrame({'stage': STAGE_ORDER, 'midpoint_ma': stage_midpoints})\n",
        "\n",
        "env_data['temperature']     = interpolate_to_ages(temp_df, 'Age', 'SST', stage_midpoints)\n",
        "env_data['dissolved_O2']    = interpolate_to_ages(do_df, 'Age', 'DO', stage_midpoints)\n",
        "env_data['atm_O2']          = interpolate_to_ages(oxygen_df, 'Age', 'O2', stage_midpoints)\n",
        "env_data['atm_CO2']         = interpolate_to_ages(oxygen_df, 'Age', 'CO2', stage_midpoints)\n",
        "env_data['sea_level']       = interpolate_to_ages(sealevel_df, 'Age', 'Eustatic Sea Level', stage_midpoints)\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# MERGE δ13C (STAGE-BINNED) — prefer Stage-name join; fallback to age-nearest\n",
        "# Uses attached d13C_stage_binned_Cam-Dev.csv WITHOUT re-binning/conversion.\n",
        "# -----------------------------------------------------------------------------\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "def _norm_stage(s):\n",
        "    if pd.isna(s):\n",
        "        return np.nan\n",
        "    s = str(s).strip().lower()\n",
        "    # remove spaces/punct to survive minor naming differences\n",
        "    return re.sub(r'[^a-z0-9]+', '', s)\n",
        "\n",
        "try:\n",
        "    # Load attached stage-binned δ13C table\n",
        "    cand = [\n",
        "        Path(\"d13C_stage_binned_Cam-Dev.csv\"),\n",
        "        Path(\"/mnt/data/d13C_stage_binned_Cam-Dev.csv\")\n",
        "    ]\n",
        "    f = next((p for p in cand if p.exists()), None)\n",
        "    if f is None:\n",
        "        raise FileNotFoundError(\"d13C_stage_binned_Cam-Dev.csv not found in working dir or /mnt/data\")\n",
        "\n",
        "    d13_stage = pd.read_csv(f)\n",
        "    d13_stage.columns = d13_stage.columns.str.strip().str.replace('\\ufeff', '')\n",
        "\n",
        "    # Required columns in attached file\n",
        "    if \"Stage\" not in d13_stage.columns or \"Mid_Ma\" not in d13_stage.columns or \"d13C_mean\" not in d13_stage.columns:\n",
        "        raise ValueError(\"δ13C stage file must have columns: Stage, Mid_Ma, d13C_mean\")\n",
        "\n",
        "    # Clean numeric + stage keys\n",
        "    d13_stage[\"Mid_Ma\"] = pd.to_numeric(d13_stage[\"Mid_Ma\"], errors=\"coerce\")\n",
        "    d13_stage[\"d13C_mean\"] = pd.to_numeric(d13_stage[\"d13C_mean\"], errors=\"coerce\")\n",
        "    d13_stage[\"stage_key\"] = d13_stage[\"Stage\"].apply(_norm_stage)\n",
        "\n",
        "    env_data[\"midpoint_ma\"] = pd.to_numeric(env_data[\"midpoint_ma\"], errors=\"coerce\")\n",
        "\n",
        "    # --- 1) Stage-name merge (best) ---\n",
        "    if \"stage\" in env_data.columns:\n",
        "        env_data[\"stage_key\"] = env_data[\"stage\"].apply(_norm_stage)\n",
        "\n",
        "        _m1 = env_data.merge(\n",
        "            d13_stage[[\"stage_key\", \"d13C_mean\", \"Mid_Ma\"]],\n",
        "            on=\"stage_key\",\n",
        "            how=\"left\",\n",
        "            suffixes=(\"\", \"_d13\")\n",
        "        )\n",
        "\n",
        "        # Keep δ13C as a single downstream name\n",
        "        _m1[\"d13C\"] = _m1[\"d13C_mean\"]\n",
        "\n",
        "        # --- 2) Fallback: for unmatched stages, fill by age-nearest ---\n",
        "        missing = _m1[\"d13C\"].isna() & _m1[\"midpoint_ma\"].notna()\n",
        "        if missing.any():\n",
        "            _left = _m1.loc[missing, [\"midpoint_ma\"]].copy().sort_values(\"midpoint_ma\")\n",
        "            _right = d13_stage[[\"Mid_Ma\", \"d13C_mean\"]].dropna().sort_values(\"Mid_Ma\")\n",
        "\n",
        "            _fill = pd.merge_asof(\n",
        "                _left,\n",
        "                _right,\n",
        "                left_on=\"midpoint_ma\",\n",
        "                right_on=\"Mid_Ma\",\n",
        "                direction=\"nearest\",\n",
        "                tolerance=2.0  # stage midpoints can differ by >0.25; allow reasonable slack\n",
        "            )\n",
        "            _m1.loc[missing, \"d13C\"] = _fill[\"d13C_mean\"].values\n",
        "\n",
        "        # Cleanup\n",
        "        env_data = _m1.drop(columns=[c for c in [\"d13C_mean\", \"Mid_Ma\", \"stage_key\"] if c in _m1.columns])\n",
        "        env_data = env_data.sort_values(\"midpoint_ma\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    else:\n",
        "        # If env_data has no stage column, do age-nearest only (more tolerant)\n",
        "        _left = env_data.dropna(subset=[\"midpoint_ma\"]).sort_values(\"midpoint_ma\")\n",
        "        _right = d13_stage[[\"Mid_Ma\", \"d13C_mean\"]].dropna().sort_values(\"Mid_Ma\")\n",
        "        _m = pd.merge_asof(_left, _right, left_on=\"midpoint_ma\", right_on=\"Mid_Ma\", direction=\"nearest\", tolerance=2.0)\n",
        "        env_data = _m.drop(columns=[\"Mid_Ma\"]).rename(columns={\"d13C_mean\": \"d13C\"})\n",
        "        env_data = env_data.sort_values(\"midpoint_ma\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    n_valid = int(env_data[\"d13C\"].notna().sum()) if \"d13C\" in env_data.columns else 0\n",
        "    print(f\"  -> δ13C (stage-binned) merged: {n_valid}/{len(env_data)} values\")\n",
        "\n",
        "except Exception as e:\n",
        "    env_data[\"d13C\"] = np.nan\n",
        "    print(\"  ! Warning: δ13C stage merge failed:\", e)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. INTERPOLATE TO 5-MYR BINS\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nInterpolating environmental proxies to 5-Myr bin midpoints...\")\n",
        "\n",
        "# Define bins (Logic from previous step)\n",
        "if 'reef_5myr_df' in locals() and not reef_5myr_df.empty:\n",
        "    bin_midpoints_5myr = reef_5myr_df['midpoint_ma'].values\n",
        "    bin_ids = reef_5myr_df['time_identifier']\n",
        "elif 'strom_5myr_df' in locals() and not strom_5myr_df.empty:\n",
        "    # Recalculate if column missing\n",
        "    if 'midpoint_ma' not in strom_5myr_df.columns:\n",
        "        strom_5myr_df['midpoint_ma'] = (strom_5myr_df['bin_top'] + strom_5myr_df['bin_bottom']) / 2\n",
        "    bin_midpoints_5myr = strom_5myr_df['midpoint_ma'].values\n",
        "    bin_ids = strom_5myr_df['bin_label']\n",
        "else:\n",
        "    # Fallback\n",
        "    bin_midpoints_5myr = np.arange(482.5, 359, -5)\n",
        "    bin_ids = [f\"{x}\" for x in bin_midpoints_5myr]\n",
        "\n",
        "env_data_5myr = pd.DataFrame({'bin_id': bin_ids, 'midpoint_ma': bin_midpoints_5myr})\n",
        "\n",
        "env_data_5myr['temperature']     = interpolate_to_ages(temp_df, 'Age', 'SST', bin_midpoints_5myr)\n",
        "env_data_5myr['dissolved_O2']    = interpolate_to_ages(do_df, 'Age', 'DO', bin_midpoints_5myr)\n",
        "env_data_5myr['atm_O2']          = interpolate_to_ages(oxygen_df, 'Age', 'O2', bin_midpoints_5myr)\n",
        "env_data_5myr['atm_CO2']         = interpolate_to_ages(oxygen_df, 'Age', 'CO2', bin_midpoints_5myr)\n",
        "env_data_5myr['sea_level']       = interpolate_to_ages(sealevel_df, 'Age', 'Eustatic Sea Level', bin_midpoints_5myr)\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4b. MERGE δ13C (5-MYR BINNED) WITHOUT INTERPOLATION / RE-BINNING\n",
        "#   Float bin midpoints often differ by tiny rounding; if exact merge yields\n",
        "#   few/zero matches, fall back to merge_asof (requires ascending sort).\n",
        "# -----------------------------------------------------------------------------\n",
        "try:\n",
        "    if 'd13c_5myr_df' in globals() and isinstance(d13c_5myr_df, pd.DataFrame) and (not d13c_5myr_df.empty):\n",
        "        _d13_5 = d13c_5myr_df.copy()\n",
        "    else:\n",
        "        cand = [\n",
        "            'd13C_5Myr_Cam-Dev.csv',\n",
        "            './output/d13C_5Myr_Cam-Dev.csv',\n",
        "            'd13C_5Myr.csv',\n",
        "            './output/d13C_5Myr.csv'\n",
        "        ]\n",
        "        _d13_5 = None\n",
        "        for p in cand:\n",
        "            if os.path.exists(p):\n",
        "                _d13_5 = pd.read_csv(p)\n",
        "                break\n",
        "        if _d13_5 is None:\n",
        "            raise FileNotFoundError(\"No 5-Myr δ13C file found (tried: \" + \", \".join(cand) + \").\")\n",
        "\n",
        "    _d13_5.columns = _d13_5.columns.str.strip().str.replace('\\ufeff', '')\n",
        "\n",
        "    # Standardize columns\n",
        "    if 'age_Ma' in _d13_5.columns and 'midpoint_ma' not in _d13_5.columns:\n",
        "        _d13_5 = _d13_5.rename(columns={'age_Ma': 'midpoint_ma'})\n",
        "    if 'Mid_Ma' in _d13_5.columns and 'midpoint_ma' not in _d13_5.columns:\n",
        "        _d13_5 = _d13_5.rename(columns={'Mid_Ma': 'midpoint_ma'})\n",
        "    if 'd13Ccarb_permille' in _d13_5.columns and 'd13C' not in _d13_5.columns:\n",
        "        _d13_5 = _d13_5.rename(columns={'d13Ccarb_permille': 'd13C'})\n",
        "    if 'd13C_mean' in _d13_5.columns and 'd13C' not in _d13_5.columns:\n",
        "        _d13_5 = _d13_5.rename(columns={'d13C_mean': 'd13C'})\n",
        "\n",
        "    _d13_5['midpoint_ma'] = pd.to_numeric(_d13_5['midpoint_ma'], errors='coerce')\n",
        "    _d13_5['d13C'] = pd.to_numeric(_d13_5['d13C'], errors='coerce')\n",
        "    _d13_5 = _d13_5.dropna(subset=['midpoint_ma', 'd13C']).copy()\n",
        "\n",
        "    # --- Attempt exact merge after rounding to reduce floating mismatch\n",
        "    env_data_5myr['midpoint_ma'] = pd.to_numeric(env_data_5myr['midpoint_ma'], errors='coerce')\n",
        "    _left = env_data_5myr.copy()\n",
        "    _left['midpoint_ma_round'] = _left['midpoint_ma'].round(3)\n",
        "    _right = _d13_5[['midpoint_ma', 'd13C']].copy()\n",
        "    _right['midpoint_ma_round'] = _right['midpoint_ma'].round(3)\n",
        "\n",
        "    env_data_5myr = _left.merge(_right[['midpoint_ma_round', 'd13C']], on='midpoint_ma_round', how='left')\n",
        "    env_data_5myr = env_data_5myr.drop(columns=['midpoint_ma_round'])\n",
        "\n",
        "    n_valid = env_data_5myr['d13C'].notna().sum()\n",
        "\n",
        "    # --- Fallback: nearest-age join if exact merge fails\n",
        "    if n_valid == 0 and len(_d13_5) > 0:\n",
        "        _left_sorted = env_data_5myr.drop(columns=['d13C'], errors='ignore').copy()\n",
        "        _left_sorted = _left_sorted.dropna(subset=['midpoint_ma']).sort_values('midpoint_ma', ascending=True).reset_index(drop=True)\n",
        "\n",
        "        _right_sorted = _d13_5[['midpoint_ma', 'd13C']].sort_values('midpoint_ma', ascending=True).reset_index(drop=True)\n",
        "\n",
        "        _m = pd.merge_asof(\n",
        "            _left_sorted,\n",
        "            _right_sorted,\n",
        "            on='midpoint_ma',\n",
        "            direction='nearest',\n",
        "            tolerance=2.6  # ~half of 5-Myr bin width\n",
        "        )\n",
        "        env_data_5myr = _m.sort_values('midpoint_ma', ascending=False).reset_index(drop=True)\n",
        "        n_valid = env_data_5myr['d13C'].notna().sum()\n",
        "\n",
        "    print(f\"  -> δ13C (5-Myr binned) merged: {n_valid}/{len(env_data_5myr)} values\")\n",
        "\n",
        "except Exception as e:\n",
        "    env_data_5myr['d13C'] = np.nan\n",
        "    print(\"  ! Warning: δ13C 5-Myr merge failed:\", e)\n",
        "\n",
        "print(\"✓ Environmental proxies interpolated (Stage & 5-Myr).\")\n",
        "\n",
        "# =============================================================================\n",
        "# [ADDED] SAVE INTERPOLATED ENV DATA\n",
        "# =============================================================================\n",
        "if 'env_data' in locals() and not env_data.empty:\n",
        "    env_data.to_csv(f\"{OUTPUT_DIR}/intermediate_env_data_stage.csv\", index=False)\n",
        "\n",
        "if 'env_data_5myr' in locals() and not env_data_5myr.empty:\n",
        "    env_data_5myr.to_csv(f\"{OUTPUT_DIR}/intermediate_env_data_5myr.csv\", index=False)\n",
        "\n",
        "print(f\"✓ Interpolated environmental data saved to {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "t2BtkuBab2Kp"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 9: CREATE MASTER DATASET (STAGES AND 5-MYR BINS)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. BUILD MASTER STAGE DATASET\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"Building Master Stage Dataset...\")\n",
        "data = []\n",
        "\n",
        "for stage, info in STAGES.items():\n",
        "    row = {\n",
        "        'stage': stage,\n",
        "        'midpoint_ma': info['mid'],\n",
        "        'start_ma': info['start'],\n",
        "        'end_ma': info['end'],\n",
        "        'period': info['period']\n",
        "    }\n",
        "\n",
        "# A. Reef data (if available)\n",
        "    if 'reef_df' in locals() and not reef_df.empty:\n",
        "        reef_row = reef_df[reef_df['name'] == stage]\n",
        "        if len(reef_row) > 0:\n",
        "            for col in [\n",
        "                'thickness_mean', 'thickness_std', 'thickness_stderr', 'thickness_median',\n",
        "                'thickness_min', 'thickness_max', 'thickness_q25', 'thickness_q75', 'thickness_count',\n",
        "                'width_mean', 'width_std', 'width_median', 'width_min', 'width_max',\n",
        "                'reef_count'\n",
        "            ]:\n",
        "                if col in reef_row.columns:\n",
        "                    row[col] = reef_row[col].values[0]\n",
        "\n",
        "    # B. Stromatoporoid data (from strom_df)\n",
        "    if 'strom_df' in locals() and not strom_df.empty:\n",
        "        # Check column name (case insensitive)\n",
        "        strom_row = strom_df[strom_df['stage'].str.lower() == stage.lower()]\n",
        "        if len(strom_row) > 0:\n",
        "            for order in STROM_ORDERS:\n",
        "                col_occ = f'{order}_occ'\n",
        "                col_gen = f'{order}_genus'\n",
        "                # Check actual columns (constants are TitleCase, data is normalized in Cell 7)\n",
        "                if col_occ in strom_row.columns:\n",
        "                    row[col_occ] = strom_row[col_occ].values[0]\n",
        "                if col_gen in strom_row.columns:\n",
        "                    row[col_gen] = strom_row[col_gen].values[0]\n",
        "\n",
        "            if 'Total_occ' in strom_row.columns:\n",
        "                row['strom_total_occ'] = strom_row['Total_occ'].values[0]\n",
        "            if 'Total_genus' in strom_row.columns:\n",
        "                row['strom_total_gen'] = strom_row['Total_genus'].values[0]\n",
        "\n",
        "    # C. Coral data (from coral_df)\n",
        "    if 'coral_df' in locals() and not coral_df.empty:\n",
        "        coral_row = coral_df[coral_df['stage'].str.lower() == stage.lower()]\n",
        "        if len(coral_row) > 0:\n",
        "            # Rugosa\n",
        "            if 'Rugosa_genus' in coral_row.columns: row['rugose_div'] = coral_row['Rugosa_genus'].values[0]\n",
        "            if 'Rugosa_occ' in coral_row.columns:   row['rugose_occ'] = coral_row['Rugosa_occ'].values[0]\n",
        "\n",
        "            # Tabulata\n",
        "            if 'Tabulata_genus' in coral_row.columns: row['tabulate_div'] = coral_row['Tabulata_genus'].values[0]\n",
        "            if 'Tabulata_occ' in coral_row.columns:   row['tabulate_occ'] = coral_row['Tabulata_occ'].values[0]\n",
        "\n",
        "    # D. Macrostrat data\n",
        "    if 'macro_stage' in locals() and not macro_stage.empty:\n",
        "        macro_row = macro_stage[macro_stage['stage'] == stage]\n",
        "        if len(macro_row) > 0:\n",
        "            row['total_area_km2'] = macro_row['total_area_km2'].values[0]\n",
        "            row['carbonate_area_km2'] = macro_row['carbonate_area_km2'].values[0]\n",
        "            row['carbonate_percentage'] = macro_row['carbonate_percentage'].values[0]\n",
        "\n",
        "    # E. Environmental proxies\n",
        "    if 'env_data' in locals() and not env_data.empty:\n",
        "        env_row = env_data[env_data['stage'] == stage]\n",
        "        if len(env_row) > 0:\n",
        "            row['temperature'] = env_row['temperature'].values[0]\n",
        "            row['dissolved_O2'] = env_row['dissolved_O2'].values[0]\n",
        "            row['atm_O2'] = env_row['atm_O2'].values[0]\n",
        "            row['atm_CO2'] = env_row['atm_CO2'].values[0]\n",
        "            row['sea_level'] = env_row['sea_level'].values[0]\n",
        "            if 'd13C' in env_row.columns: row['d13C'] = env_row['d13C'].values[0]\n",
        "\n",
        "    data.append(row)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate Stromatoporoid Proportions\n",
        "if 'strom_total_occ' in df.columns:\n",
        "    for order in STROM_ORDERS:\n",
        "        col_occ = f'{order}_occ'\n",
        "        if col_occ in df.columns:\n",
        "            df[f'{order}_prop'] = np.where(\n",
        "                df['strom_total_occ'] > 0,\n",
        "                df[col_occ].fillna(0) / df['strom_total_occ'],\n",
        "                0.0\n",
        "            )\n",
        "\n",
        "    # Derived vs Basal\n",
        "    derived_orders = ['Actinostromatida', 'Stromatoporida', 'Stromatoporellida',\n",
        "                      'Syringostromatida', 'Amphiporida']\n",
        "    basal_orders = ['Labechiida', 'Clathrodictyida']\n",
        "\n",
        "    # Occurrences\n",
        "    df['derived_strom_occ'] = sum(df[f'{o}_occ'].fillna(0) for o in derived_orders if f'{o}_occ' in df.columns)\n",
        "    df['basal_strom_occ'] = sum(df[f'{o}_occ'].fillna(0) for o in basal_orders if f'{o}_occ' in df.columns)\n",
        "\n",
        "    # Diversity\n",
        "    df['derived_strom_div'] = sum(df[f'{o}_genus'].fillna(0) for o in derived_orders if f'{o}_genus' in df.columns)\n",
        "    df['basal_strom_div'] = sum(df[f'{o}_genus'].fillna(0) for o in basal_orders if f'{o}_genus' in df.columns)\n",
        "\n",
        "    # Proportions\n",
        "    df['derived_strom_prop'] = np.where(df['strom_total_occ'] > 0, df['derived_strom_occ'] / df['strom_total_occ'], 0.0)\n",
        "    df['basal_strom_prop'] = np.where(df['strom_total_occ'] > 0, df['basal_strom_occ'] / df['strom_total_occ'], 0.0)\n",
        "\n",
        "# Sort\n",
        "df = df.sort_values('midpoint_ma', ascending=False).reset_index(drop=True)\n",
        "print(f\"✓ Master dataset (STAGES) created: {len(df)} stages, {len(df.columns)} variables\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. BUILD MASTER 5-MYR DATASET\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING 5-MYR BIN MASTER DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Determine the primary source for 5-Myr bins\n",
        "# We prefer reef data if available, otherwise we use the biological data\n",
        "if 'reef_5myr_df' in locals() and not reef_5myr_df.empty:\n",
        "    primary_bins = reef_5myr_df\n",
        "    print(\"Using Reef Data as primary 5-Myr bin source.\")\n",
        "elif 'strom_5myr_df' in locals() and not strom_5myr_df.empty:\n",
        "    primary_bins = strom_5myr_df\n",
        "    # Add midpoint if missing\n",
        "    if 'midpoint_ma' not in primary_bins.columns:\n",
        "        primary_bins['midpoint_ma'] = (primary_bins['bin_top'] + primary_bins['bin_bottom']) / 2\n",
        "    print(\"Using Stromatoporoid Data as primary 5-Myr bin source.\")\n",
        "else:\n",
        "    # Fallback to env_data_5myr if bio data missing\n",
        "    primary_bins = env_data_5myr\n",
        "    print(\"Using Environmental Data as primary 5-Myr bin source.\")\n",
        "\n",
        "data_5myr = []\n",
        "\n",
        "# Iterate through the chosen primary bins\n",
        "for idx, row_ref in primary_bins.iterrows():\n",
        "    # Identify bin\n",
        "    if 'time_identifier' in row_ref: bin_id = row_ref['time_identifier']\n",
        "    elif 'bin_label' in row_ref: bin_id = row_ref['bin_label']\n",
        "    elif 'bin_id' in row_ref: bin_id = row_ref['bin_id']\n",
        "    else: bin_id = idx # fallback\n",
        "\n",
        "    midpoint = row_ref['midpoint_ma']\n",
        "\n",
        "    row = {\n",
        "        'bin_id': bin_id,\n",
        "        'midpoint_ma': midpoint\n",
        "    }\n",
        "\n",
        "    # A. Reef Data (if available)\n",
        "    if 'reef_5myr_df' in locals() and not reef_5myr_df.empty:\n",
        "        # Find matching reef row (if not already iterating it)\n",
        "        if primary_bins is not reef_5myr_df:\n",
        "            # Match by midpoint proximity (float comparison)\n",
        "            reef_match = reef_5myr_df[abs(reef_5myr_df['midpoint_ma'] - midpoint) < 0.1]\n",
        "            if len(reef_match) > 0:\n",
        "                row_ref_for_reef = reef_match.iloc[0]\n",
        "            else:\n",
        "                row_ref_for_reef = pd.Series()\n",
        "        else:\n",
        "            row_ref_for_reef = row_ref\n",
        "\n",
        "        for col in ['thickness_mean', 'thickness_std', 'thickness_stderr', 'thickness_median',\n",
        "                    'thickness_count', 'width_mean', 'width_std', 'reef_count']:\n",
        "            if col in row_ref_for_reef.index:\n",
        "                row[col] = row_ref_for_reef[col]\n",
        "\n",
        "    # B. Stromatoporoid Data (5-Myr) - NOW INCLUDED\n",
        "    if 'strom_5myr_df' in locals() and not strom_5myr_df.empty:\n",
        "        # Match by midpoint\n",
        "        strom_match = strom_5myr_df[abs(((strom_5myr_df['bin_top'] + strom_5myr_df['bin_bottom'])/2) - midpoint) < 0.1]\n",
        "        if len(strom_match) > 0:\n",
        "            s_row = strom_match.iloc[0]\n",
        "            for order in STROM_ORDERS:\n",
        "                if f'{order}_occ' in s_row: row[f'{order}_occ'] = s_row[f'{order}_occ']\n",
        "                if f'{order}_genus' in s_row: row[f'{order}_genus'] = s_row[f'{order}_genus']\n",
        "            if 'Total_occ' in s_row: row['strom_total_occ'] = s_row['Total_occ']\n",
        "            if 'Total_genus' in s_row: row['strom_total_gen'] = s_row['Total_genus']\n",
        "\n",
        "    # C. Coral Data (5-Myr) - NOW INCLUDED\n",
        "    if 'coral_5myr_df' in locals() and not coral_5myr_df.empty:\n",
        "        # Match by midpoint\n",
        "        coral_match = coral_5myr_df[abs(((coral_5myr_df['bin_top'] + coral_5myr_df['bin_bottom'])/2) - midpoint) < 0.1]\n",
        "        if len(coral_match) > 0:\n",
        "            c_row = coral_match.iloc[0]\n",
        "            if 'Rugosa_occ' in c_row: row['rugose_occ'] = c_row['Rugosa_occ']\n",
        "            if 'Rugosa_genus' in c_row: row['rugose_div'] = c_row['Rugosa_genus']\n",
        "            if 'Tabulata_occ' in c_row: row['tabulate_occ'] = c_row['Tabulata_occ']\n",
        "            if 'Tabulata_genus' in c_row: row['tabulate_div'] = c_row['Tabulata_genus']\n",
        "\n",
        "    # D. Macrostrat Data\n",
        "    if 'macro_5myr' in locals() and not macro_5myr.empty and 'bin_mid' in macro_5myr.columns:\n",
        "        macro_match = macro_5myr[abs(macro_5myr['bin_mid'] - midpoint) < 2.5]\n",
        "        if len(macro_match) > 0:\n",
        "            row['total_area_km2'] = macro_match['total_area_km2'].values[0]\n",
        "            row['carbonate_area_km2'] = macro_match['carbonate_area_km2'].values[0]\n",
        "            row['carbonate_percentage'] = macro_match['carbonate_percentage'].values[0]\n",
        "\n",
        "    # E. Environmental Proxies\n",
        "    if 'env_data_5myr' in locals() and not env_data_5myr.empty:\n",
        "        # Match by midpoint\n",
        "        env_match = env_data_5myr[abs(env_data_5myr['midpoint_ma'] - midpoint) < 0.1]\n",
        "        if len(env_match) > 0:\n",
        "            env_val = env_match.iloc[0]\n",
        "            row['temperature'] = env_val['temperature']\n",
        "            row['dissolved_O2'] = env_val['dissolved_O2']\n",
        "            row['atm_O2'] = env_val['atm_O2']\n",
        "            row['atm_CO2'] = env_val['atm_CO2']\n",
        "            row['sea_level'] = env_val['sea_level']\n",
        "            if 'd13C' in env_val.index: row['d13C'] = env_val['d13C']\n",
        "\n",
        "    data_5myr.append(row)\n",
        "\n",
        "df_5myr = pd.DataFrame(data_5myr)\n",
        "df_5myr = df_5myr.sort_values('midpoint_ma', ascending=False).reset_index(drop=True)\n",
        "print(f\"✓ Master dataset (5-MYR BINS) created: {len(df_5myr)} bins, {len(df_5myr.columns)} variables\")\n",
        "\n",
        "# =============================================================================\n",
        "# [ADDED] SAVE MASTER DATASETS\n",
        "# =============================================================================\n",
        "# Save the master datasets immediately after creation\n",
        "if 'df' in locals() and not df.empty:\n",
        "    df.to_csv(f\"{OUTPUT_DIR}/MASTER_dataset_stage.csv\", index=False)\n",
        "    print(f\"✓ Saved: {OUTPUT_DIR}/MASTER_dataset_stage.csv\")\n",
        "\n",
        "if 'df_5myr' in locals() and not df_5myr.empty:\n",
        "    df_5myr.to_csv(f\"{OUTPUT_DIR}/MASTER_dataset_5myr.csv\", index=False)\n",
        "    print(f\"✓ Saved: {OUTPUT_DIR}/MASTER_dataset_5myr.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jpj1BjT5d8eO"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 10: CLR COMPOSITIONAL TRANSFORMATION (WITH BASAL/DERIVED GROUPS)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import gmean\n",
        "import scipy.stats as stats\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"PRE-PROCESSING: CLR TRANSFORMATION\")\n",
        "print(\"Transforming closed compositional data (proportions) to open log-ratios\")\n",
        "print(\"=\"*90)\n",
        "# Global list for CLR results\n",
        "global_clr_results = []\n",
        "def calculate_missing_props(input_df):\n",
        "    \"\"\"\n",
        "    Helper: If _prop columns are missing, calculate them from _occ columns.\n",
        "    \"\"\"\n",
        "    df_copy = input_df.copy()\n",
        "    orders = ['Labechiida', 'Clathrodictyida', 'Actinostromatida',\n",
        "              'Stromatoporida', 'Stromatoporellida',\n",
        "              'Syringostromatida', 'Amphiporida']\n",
        "    # Check if we have occurrence columns\n",
        "    occ_cols = [f\"{o}_occ\" for o in orders if f\"{o}_occ\" in df_copy.columns]\n",
        "    if not occ_cols:\n",
        "        return df_copy\n",
        "    # Recalculate Total\n",
        "    if 'strom_total_occ' not in df_copy.columns:\n",
        "        df_copy['strom_total_occ'] = df_copy[occ_cols].sum(axis=1)\n",
        "    # Calculate Proportions\n",
        "    for o in orders:\n",
        "        occ_col = f\"{o}_occ\"\n",
        "        prop_col = f\"{o}_prop\"\n",
        "        if occ_col in df_copy.columns:\n",
        "            total = df_copy['strom_total_occ'].replace(0, np.nan)\n",
        "            df_copy[prop_col] = df_copy[occ_col] / total\n",
        "            df_copy[prop_col] = df_copy[prop_col].fillna(0)\n",
        "    return df_copy\n",
        "def apply_clr(input_df, label):\n",
        "    \"\"\"Apply CLR transformation and calculate group-level metrics.\"\"\"\n",
        "\n",
        "    dataset = input_df.copy()\n",
        "\n",
        "    # 1. AUTO-REPAIR: Ensure Proportion Columns Exist\n",
        "    dataset = calculate_missing_props(dataset)\n",
        "    # 2. Identify Proportion Columns\n",
        "    prop_cols = ['Labechiida_prop', 'Clathrodictyida_prop', 'Actinostromatida_prop',\n",
        "                 'Stromatoporida_prop', 'Stromatoporellida_prop',\n",
        "                 'Syringostromatida_prop', 'Amphiporida_prop']\n",
        "    available_cols = [c for c in prop_cols if c in dataset.columns]\n",
        "    # Define group membership\n",
        "    basal_orders = ['Labechiida_prop', 'Clathrodictyida_prop']\n",
        "    derived_orders = ['Actinostromatida_prop', 'Stromatoporida_prop',\n",
        "                      'Stromatoporellida_prop', 'Syringostromatida_prop',\n",
        "                      'Amphiporida_prop']\n",
        "    if len(available_cols) < 2:\n",
        "        print(f\"  {label}: ! Skipped. Found only {len(available_cols)} proportion columns.\")\n",
        "        return dataset\n",
        "    # 3. Extract and Handle Zeros\n",
        "    comp_data = dataset[available_cols].replace(0, 1e-5)\n",
        "    # 4. Geometric Mean per Row\n",
        "    gmeans = gmean(comp_data, axis=1)\n",
        "    # 5. Transform: ln(x / gmean)\n",
        "    clr_data = np.log(comp_data.div(gmeans, axis=0))\n",
        "\n",
        "    # Use CLR_ prefix (avoiding duplicates)\n",
        "    new_col_names = []\n",
        "    for c in available_cols:\n",
        "        new_name = f\"CLR_{c}\"\n",
        "        # Drop existing column if present to avoid duplicates\n",
        "        if new_name in dataset.columns:\n",
        "            dataset = dataset.drop(columns=[new_name])\n",
        "        new_col_names.append(new_name)\n",
        "    clr_data.columns = new_col_names\n",
        "    # 6. Create 'Derived vs Basal' Log-Ratio\n",
        "    basal_in = [c for c in basal_orders if c in dataset.columns]\n",
        "    derived_in = [c for c in derived_orders if c in dataset.columns]\n",
        "    if basal_in and derived_in:\n",
        "        b_sum = dataset[basal_in].sum(axis=1).replace(0, 1e-5)\n",
        "        d_sum = dataset[derived_in].sum(axis=1).replace(0, 1e-5)\n",
        "        dataset['log_derived_basal_ratio'] = np.log(d_sum / b_sum)\n",
        "        print(f\"  {label}: Created 'log_derived_basal_ratio'\")\n",
        "    # 7. Merge CLR columns back to dataset\n",
        "    dataset = pd.concat([dataset.reset_index(drop=True), clr_data.reset_index(drop=True)], axis=1)\n",
        "    print(f\"  {label}: Generated {len(clr_data.columns)} CLR variables.\")\n",
        "\n",
        "    # 8. Calculate GROUP-LEVEL CLR means\n",
        "    clr_basal_cols = [f\"CLR_{c}\" for c in basal_in]\n",
        "    clr_derived_cols = [f\"CLR_{c}\" for c in derived_in]\n",
        "\n",
        "    if clr_basal_cols:\n",
        "        dataset['CLR_basal_mean'] = dataset[clr_basal_cols].mean(axis=1)\n",
        "    if clr_derived_cols:\n",
        "        dataset['CLR_derived_mean'] = dataset[clr_derived_cols].mean(axis=1)\n",
        "\n",
        "    return dataset\n",
        "# Apply to both master datasets\n",
        "df = apply_clr(df, \"Stage-Level\")\n",
        "df_5myr = apply_clr(df_5myr, \"5-Myr Bins\")\n",
        "# =============================================================================\n",
        "# CLR CORRELATION ANALYSIS (Individual Taxa + Basal/Derived Groups)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"-\"*90)\n",
        "print(\"CLR CORRELATION ANALYSIS\")\n",
        "print(\"-\"*90)\n",
        "prop_cols = ['Labechiida_prop', 'Clathrodictyida_prop', 'Actinostromatida_prop',\n",
        "             'Stromatoporida_prop', 'Stromatoporellida_prop',\n",
        "             'Syringostromatida_prop', 'Amphiporida_prop']\n",
        "targets = ['thickness_mean', 'width_mean']\n",
        "def safe_spearman(x, y):\n",
        "    \"\"\"Calculate Spearman correlation safely, returning scalars.\"\"\"\n",
        "    try:\n",
        "        # Ensure 1D numpy arrays\n",
        "        x_arr = np.array(x).flatten()\n",
        "        y_arr = np.array(y).flatten()\n",
        "\n",
        "        # Remove NaN pairs\n",
        "        mask = ~(np.isnan(x_arr) | np.isnan(y_arr))\n",
        "        x_clean = x_arr[mask]\n",
        "        y_clean = y_arr[mask]\n",
        "\n",
        "        if len(x_clean) < 3:\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        result = stats.spearmanr(x_clean, y_clean)\n",
        "        # Handle both old and new scipy return types\n",
        "        if hasattr(result, 'correlation'):\n",
        "            return float(result.correlation), float(result.pvalue)\n",
        "        else:\n",
        "            return float(result[0]), float(result[1])\n",
        "    except:\n",
        "        return np.nan, np.nan\n",
        "for label, data in [('Stage-Level', df), ('5-Myr Bins', df_5myr)]:\n",
        "    print(f\"\\n--- {label} ---\")\n",
        "    print(f\"  {'Taxon/Group':<28s} | {'Orig ρ':>8s} | {'CLR ρ':>8s} | {'Diff':>7s} | {'CLR p':>10s}\")\n",
        "    print(\"  \" + \"-\"*75)\n",
        "\n",
        "    # A. INDIVIDUAL TAXA\n",
        "    for prop in prop_cols:\n",
        "        clr_col = f\"CLR_{prop}\"\n",
        "        if prop not in data.columns or clr_col not in data.columns:\n",
        "            continue\n",
        "\n",
        "        for target in targets:\n",
        "            if target not in data.columns:\n",
        "                continue\n",
        "\n",
        "            # Get data as 1D arrays\n",
        "            x_orig = data[prop].values\n",
        "            x_clr = data[clr_col].values\n",
        "            y = data[target].values\n",
        "\n",
        "            # Calculate correlations\n",
        "            r_orig, p_orig = safe_spearman(x_orig, y)\n",
        "            r_clr, p_clr = safe_spearman(x_clr, y)\n",
        "\n",
        "            if np.isnan(r_orig) or np.isnan(r_clr):\n",
        "                continue\n",
        "\n",
        "            diff = r_clr - r_orig\n",
        "\n",
        "            # Interpretation\n",
        "            if abs(diff) < 0.1:\n",
        "                interp = \"Stable\"\n",
        "            elif diff < -0.2:\n",
        "                interp = \"SUPPRESSED\"\n",
        "            elif diff > 0.2:\n",
        "                interp = \"INFLATED\"\n",
        "            else:\n",
        "                interp = \"Moderate\"\n",
        "\n",
        "            global_clr_results.append({\n",
        "                'Dataset': label, 'Level': 'Individual',\n",
        "                'Predictor': prop.replace('_prop', ''),\n",
        "                'Target': target, 'Original_Rho': r_orig, 'CLR_Rho': r_clr,\n",
        "                'Difference': diff, 'CLR_P': p_clr, 'Interpretation': interp\n",
        "            })\n",
        "\n",
        "            if target == 'thickness_mean':\n",
        "                sig = \"*\" if p_clr < 0.05 else \"\"\n",
        "                print(f\"  {prop.replace('_prop',''):<28s} | {r_orig:>+8.3f} | {r_clr:>+8.3f} | {diff:>+7.3f} | {p_clr:>10.4f} {sig}\")\n",
        "\n",
        "    # B. BASAL vs DERIVED GROUPS\n",
        "    print(\"  \" + \"-\"*75)\n",
        "    print(\"  GROUP COMPARISONS:\")\n",
        "\n",
        "    group_vars = [\n",
        "        ('CLR_basal_mean', 'Basal (Labech+Clathr) CLR'),\n",
        "        ('CLR_derived_mean', 'Derived (5 taxa) CLR'),\n",
        "        ('log_derived_basal_ratio', 'Log(Derived/Basal) Ratio')\n",
        "    ]\n",
        "\n",
        "    for col, name in group_vars:\n",
        "        if col not in data.columns:\n",
        "            continue\n",
        "\n",
        "        for target in targets:\n",
        "            if target not in data.columns:\n",
        "                continue\n",
        "\n",
        "            x = data[col].values\n",
        "            y = data[target].values\n",
        "\n",
        "            r, p = safe_spearman(x, y)\n",
        "\n",
        "            if np.isnan(r):\n",
        "                continue\n",
        "\n",
        "            global_clr_results.append({\n",
        "                'Dataset': label, 'Level': 'Group',\n",
        "                'Predictor': name, 'Target': target,\n",
        "                'Original_Rho': np.nan, 'CLR_Rho': r,\n",
        "                'Difference': np.nan, 'CLR_P': p, 'Interpretation': 'Group-level'\n",
        "            })\n",
        "\n",
        "            if target == 'thickness_mean':\n",
        "                sig = \"*\" if p < 0.05 else \"\"\n",
        "                print(f\"  {name:<28s} |      N/A | {r:>+8.3f} |     N/A | {p:>10.4f} {sig}\")\n",
        "# =============================================================================\n",
        "# SAVE CLR RESULTS\n",
        "# =============================================================================\n",
        "pd.DataFrame(global_clr_results).to_csv(f\"{OUTPUT_DIR}/results_clr.csv\", index=False)\n",
        "print(f\"\\nSaved: {OUTPUT_DIR}/results_clr.csv ({len(global_clr_results)} rows)\")\n",
        "# Update predictor lists\n",
        "if 'all_test_vars' in locals():\n",
        "    new_vars = [\n",
        "        ('log_derived_basal_ratio', 'Log(Derived/Basal)'),\n",
        "        ('CLR_basal_mean', 'CLR Basal Mean'),\n",
        "        ('CLR_derived_mean', 'CLR Derived Mean')\n",
        "    ]\n",
        "    for var in new_vars:\n",
        "        if not any(v[0] == var[0] for v in all_test_vars):\n",
        "            all_test_vars.append(var)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OhFo2l0de0YP"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 11: CORRELATION FUNCTIONS - SPEARMAN AND PEARSON\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "\"\"\"\n",
        "CORRELATION ANALYSIS METHODS\n",
        "============================\n",
        "This cell defines functions for both Spearman and Pearson correlations.\n",
        "\n",
        "SPEARMAN'S RHO (ρ):\n",
        "- Non-parametric rank correlation\n",
        "- Measures monotonic relationships (not just linear)\n",
        "- Robust to outliers and non-normal distributions\n",
        "- Appropriate for ordinal data or data with outliers\n",
        "- Reference: Spearman, C. (1904). American Journal of Psychology, 15(1), 72-101.\n",
        "\n",
        "PEARSON'S r:\n",
        "- Parametric correlation coefficient\n",
        "- Measures linear relationships specifically\n",
        "- Assumes normally distributed variables\n",
        "- More powerful when assumptions are met\n",
        "- Reference: Pearson, K. (1895). Philosophical Transactions of the Royal Society A, 186, 343-414.\n",
        "\n",
        "For geological time series:\n",
        "- Spearman is often preferred due to non-normal distributions\n",
        "- Pearson provides information on linear relationships\n",
        "- Presenting both allows comparison and assessment of relationship type\n",
        "\"\"\"\n",
        "\n",
        "def calc_spearman(data, v1, v2):\n",
        "    \"\"\"\n",
        "    Calculate Spearman rank correlation with significance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : DataFrame\n",
        "        Data containing the variables\n",
        "    v1, v2 : str\n",
        "        Column names to correlate\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    rho : float\n",
        "        Spearman correlation coefficient\n",
        "    p : float\n",
        "        Two-tailed p-value\n",
        "    n : int\n",
        "        Number of valid pairs\n",
        "    \"\"\"\n",
        "    # Check if columns exist\n",
        "    if v1 not in data.columns or v2 not in data.columns:\n",
        "        return np.nan, np.nan, 0\n",
        "\n",
        "    valid = data[[v1, v2]].dropna()\n",
        "\n",
        "    if len(valid) >= 5:\n",
        "        # Spearmanr returns a Result object or tuple depending on version, generic unpacking is safer\n",
        "        result = stats.spearmanr(valid[v1], valid[v2])\n",
        "        # Handle cases where result might be a struct or tuple\n",
        "        try:\n",
        "            r, p = result.correlation, result.pvalue\n",
        "        except AttributeError:\n",
        "            r, p = result[0], result[1]\n",
        "\n",
        "        return r, p, len(valid)\n",
        "\n",
        "    return np.nan, np.nan, 0\n",
        "\n",
        "def calc_pearson(data, v1, v2):\n",
        "    \"\"\"\n",
        "    Calculate Pearson correlation with significance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : DataFrame\n",
        "        Data containing the variables\n",
        "    v1, v2 : str\n",
        "        Column names to correlate\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    r : float\n",
        "        Pearson correlation coefficient\n",
        "    p : float\n",
        "        Two-tailed p-value\n",
        "    n : int\n",
        "        Number of valid pairs\n",
        "    \"\"\"\n",
        "    if v1 not in data.columns or v2 not in data.columns:\n",
        "        return np.nan, np.nan, 0\n",
        "\n",
        "    valid = data[[v1, v2]].dropna()\n",
        "\n",
        "    if len(valid) >= 5:\n",
        "        r, p = stats.pearsonr(valid[v1], valid[v2])\n",
        "        return r, p, len(valid)\n",
        "\n",
        "    return np.nan, np.nan, 0\n",
        "\n",
        "def calc_both_correlations(data, v1, v2):\n",
        "    \"\"\"\n",
        "    Calculate both Spearman and Pearson correlations\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict with both correlation results\n",
        "    \"\"\"\n",
        "    spearman_r, spearman_p, n = calc_spearman(data, v1, v2)\n",
        "    pearson_r, pearson_p, _ = calc_pearson(data, v1, v2)\n",
        "\n",
        "    return {\n",
        "        'spearman_rho': spearman_r,\n",
        "        'spearman_p': spearman_p,\n",
        "        'pearson_r': pearson_r,\n",
        "        'pearson_p': pearson_p,\n",
        "        'n': n\n",
        "    }\n",
        "\n",
        "def get_significance_stars(p):\n",
        "    \"\"\"Convert p-value to significance stars\"\"\"\n",
        "    if p is None or np.isnan(p):\n",
        "        return ''\n",
        "    if p < 0.001:\n",
        "        return '***'\n",
        "    elif p < 0.01:\n",
        "        return '**'\n",
        "    elif p < 0.05:\n",
        "        return '*'\n",
        "    else:\n",
        "        return 'ns'\n",
        "\n",
        "print(\"✓ Correlation functions defined (Spearman and Pearson)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0WIsgnZGe2v-"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 12: COMPREHENSIVE CORRELATION ANALYSIS (MULTI-METRIC & 5-MYR BIOLOGY)\n",
        "#   - NO T/W ratio computed or compared\n",
        "#   - Excludes rows with NO stromatoporoids (strom_total_occ<=0) from statistics\n",
        "#   - Includes δ13C (expects column name 'd13C')\n",
        "#   - Standardizes atmospheric O2/CO2 column names for correlation output:\n",
        "#       atmospheric_O2, atmospheric_CO2\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import re\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\"COMPREHENSIVE CORRELATION ANALYSIS\")\n",
        "print(\"Metrics: Thickness, Width\")\n",
        "print(\"Scopes:  Stage-Level AND 5-Myr Bins\")\n",
        "print(\"Notes:   Excluding strom_total_occ<=0 rows; NO T/W ratio; includes δ13C + atm O2/CO2 if present\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 0. SAFETY: define STROM_ORDERS if missing\n",
        "# -----------------------------------------------------------------------------\n",
        "if 'STROM_ORDERS' not in globals():\n",
        "    STROM_ORDERS = [\n",
        "        'Labechiida', 'Clathrodictyida', 'Actinostromatida',\n",
        "        'Stromatoporida', 'Stromatoporellida', 'Syringostromatida', 'Amphiporida'\n",
        "    ]\n",
        "    print(\"[WARN] STROM_ORDERS not found in globals(); using default list.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. PRE-PROCESSING: CALCULATE DERIVED VARIABLES (NO T/W)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"Pre-processing data...\")\n",
        "\n",
        "def calculate_derived_metrics(dataset, label):\n",
        "    if dataset is None or dataset.empty:\n",
        "        print(f\"  - [{label}] Empty dataset; skip derived metrics.\")\n",
        "        return dataset\n",
        "\n",
        "    dataset = dataset.copy()\n",
        "\n",
        "    # Remove T/W ratio if it exists from older runs (do NOT compute it)\n",
        "    if 'thickness_width_ratio' in dataset.columns:\n",
        "        dataset = dataset.drop(columns=['thickness_width_ratio'])\n",
        "\n",
        "    # Stromatoporoid Proportions & Groups\n",
        "    strom_cols = [c for c in dataset.columns if c.endswith('_occ') and c != 'strom_total_occ']\n",
        "    if strom_cols:\n",
        "        if 'strom_total_occ' not in dataset.columns:\n",
        "            dataset['strom_total_occ'] = dataset[strom_cols].sum(axis=1)\n",
        "\n",
        "        # Ensure numeric\n",
        "        dataset['strom_total_occ'] = pd.to_numeric(dataset['strom_total_occ'], errors='coerce')\n",
        "\n",
        "        for order in STROM_ORDERS:\n",
        "            col_occ = f'{order}_occ'\n",
        "            if col_occ in dataset.columns:\n",
        "                dataset[col_occ] = pd.to_numeric(dataset[col_occ], errors='coerce').fillna(0)\n",
        "                dataset[f'{order}_prop'] = np.where(\n",
        "                    dataset['strom_total_occ'] > 0,\n",
        "                    dataset[col_occ] / dataset['strom_total_occ'],\n",
        "                    np.nan\n",
        "                )\n",
        "\n",
        "        derived_orders = ['Actinostromatida', 'Stromatoporida', 'Stromatoporellida',\n",
        "                          'Syringostromatida', 'Amphiporida']\n",
        "        basal_orders   = ['Labechiida', 'Clathrodictyida']\n",
        "\n",
        "        # occurrences\n",
        "        dataset['derived_strom_occ'] = 0.0\n",
        "        dataset['basal_strom_occ']   = 0.0\n",
        "        for o in derived_orders:\n",
        "            c = f'{o}_occ'\n",
        "            if c in dataset.columns:\n",
        "                dataset['derived_strom_occ'] += pd.to_numeric(dataset[c], errors='coerce').fillna(0)\n",
        "        for o in basal_orders:\n",
        "            c = f'{o}_occ'\n",
        "            if c in dataset.columns:\n",
        "                dataset['basal_strom_occ'] += pd.to_numeric(dataset[c], errors='coerce').fillna(0)\n",
        "\n",
        "        # diversity (genus)\n",
        "        dataset['derived_strom_div'] = 0.0\n",
        "        dataset['basal_strom_div']   = 0.0\n",
        "        for o in derived_orders:\n",
        "            c = f'{o}_genus'\n",
        "            if c in dataset.columns:\n",
        "                dataset['derived_strom_div'] += pd.to_numeric(dataset[c], errors='coerce').fillna(0)\n",
        "        for o in basal_orders:\n",
        "            c = f'{o}_genus'\n",
        "            if c in dataset.columns:\n",
        "                dataset['basal_strom_div'] += pd.to_numeric(dataset[c], errors='coerce').fillna(0)\n",
        "\n",
        "        dataset['derived_strom_prop'] = np.where(\n",
        "            dataset['strom_total_occ'] > 0,\n",
        "            dataset['derived_strom_occ'] / dataset['strom_total_occ'],\n",
        "            np.nan\n",
        "        )\n",
        "        dataset['basal_strom_prop'] = np.where(\n",
        "            dataset['strom_total_occ'] > 0,\n",
        "            dataset['basal_strom_occ'] / dataset['strom_total_occ'],\n",
        "            np.nan\n",
        "        )\n",
        "\n",
        "        print(f\"  [OK] [{label}] Calculated Stromatoporoid proportions and groupings\")\n",
        "    else:\n",
        "        print(f\"  - [{label}] No Stromatoporoid occurrence data found.\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "df = calculate_derived_metrics(df, \"Stage\")\n",
        "df_5myr = calculate_derived_metrics(df_5myr, \"5-Myr\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1A. STANDARDIZE ATMOSPHERIC O2/CO2 COLUMN NAMES (Stage + 5-Myr)\n",
        "#   Outputs standardized columns:\n",
        "#     - atmospheric_O2\n",
        "#     - atmospheric_CO2\n",
        "# -----------------------------------------------------------------------------\n",
        "def standardize_atm_cols(dataset, label):\n",
        "    if dataset is None or dataset.empty:\n",
        "        return dataset\n",
        "    dataset = dataset.copy()\n",
        "\n",
        "    def _find_col(candidates, regex_pat=None):\n",
        "        for c in candidates:\n",
        "            if c in dataset.columns:\n",
        "                return c\n",
        "        if regex_pat is not None:\n",
        "            hits = [c for c in dataset.columns if re.search(regex_pat, c, flags=re.IGNORECASE)]\n",
        "            return hits[0] if hits else None\n",
        "        return None\n",
        "\n",
        "    # Atmospheric O2\n",
        "    if 'atmospheric_O2' not in dataset.columns:\n",
        "        o2_src = _find_col(\n",
        "            candidates=['atm_O2','atmospheric_o2','oxygen','pO2','PO2','O2_atm','oxygen_atm'],\n",
        "            regex_pat=r'(atm|atmos).*o2|po2'\n",
        "        )\n",
        "        if o2_src is not None:\n",
        "            dataset['atmospheric_O2'] = pd.to_numeric(dataset[o2_src], errors='coerce')\n",
        "            print(f\"  [OK] [{label}] atmospheric_O2 <- {o2_src}\")\n",
        "        else:\n",
        "            print(f\"  [WARN] [{label}] No atmospheric O2 column found (atmospheric_O2 will be missing).\")\n",
        "\n",
        "    # Atmospheric CO2\n",
        "    if 'atmospheric_CO2' not in dataset.columns:\n",
        "        co2_src = _find_col(\n",
        "            candidates=['atm_CO2','atmospheric_co2','co2','pCO2','PCO2','CO2_atm','co2_atm'],\n",
        "            regex_pat=r'(atm|atmos).*co2|pco2'\n",
        "        )\n",
        "        if co2_src is not None:\n",
        "            dataset['atmospheric_CO2'] = pd.to_numeric(dataset[co2_src], errors='coerce')\n",
        "            print(f\"  [OK] [{label}] atmospheric_CO2 <- {co2_src}\")\n",
        "        else:\n",
        "            print(f\"  [WARN] [{label}] No atmospheric CO2 column found (atmospheric_CO2 will be missing).\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "df = standardize_atm_cols(df, \"Stage\")\n",
        "df_5myr = standardize_atm_cols(df_5myr, \"5-Myr\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1B. FILTER: EXCLUDE ROWS WITH NO STROMATOPOROIDS FROM STATISTICS\n",
        "# -----------------------------------------------------------------------------\n",
        "def filter_has_strom(df_in, label):\n",
        "    if df_in is None or df_in.empty:\n",
        "        print(f\"  - [{label}] Empty dataset; nothing to filter.\")\n",
        "        return df_in\n",
        "    if 'strom_total_occ' not in df_in.columns:\n",
        "        print(f\"  [WARN] [{label}] 'strom_total_occ' not found; keeping all rows.\")\n",
        "        return df_in\n",
        "    df_out = df_in[df_in['strom_total_occ'].notna() & (pd.to_numeric(df_in['strom_total_occ'], errors='coerce') > 0)].copy()\n",
        "    removed = len(df_in) - len(df_out)\n",
        "    print(f\"  - [{label}] Excluding strom_total_occ<=0 rows: removed {removed}, kept {len(df_out)}.\")\n",
        "    return df_out\n",
        "\n",
        "df_corr = filter_has_strom(df, \"Stage\")\n",
        "df_5myr_corr = filter_has_strom(df_5myr, \"5-Myr\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. DEFINE VARIABLE GROUPS\n",
        "# -----------------------------------------------------------------------------\n",
        "reef_targets = [\n",
        "    ('thickness_mean', 'Thickness (Log)'),\n",
        "    ('width_mean', 'Width (Log)')\n",
        "]\n",
        "\n",
        "strom_prop_vars = [('derived_strom_prop', 'Derived Proportion'), ('basal_strom_prop', 'Basal Proportion')] + \\\n",
        "                  [(f'{order}_prop', f'{order} Prop') for order in STROM_ORDERS]\n",
        "\n",
        "strom_occ_vars = [('strom_total_occ', 'Total Occurrence'), ('derived_strom_occ', 'Derived Occurrence'), ('basal_strom_occ', 'Basal Occurrence')] + \\\n",
        "                 [(f'{order}_occ', f'{order} Occ') for order in STROM_ORDERS]\n",
        "\n",
        "strom_div_vars = [('strom_total_gen', 'Total Diversity'), ('derived_strom_div', 'Derived Diversity'), ('basal_strom_div', 'Basal Diversity')] + \\\n",
        "                 [(f'{order}_genus', f'{order} Div') for order in STROM_ORDERS]\n",
        "\n",
        "coral_div_vars = [('rugose_div', 'Rugose Diversity'), ('tabulate_div', 'Tabulate Diversity')]\n",
        "coral_occ_vars = [('rugose_occ', 'Rugose Occurrence'), ('tabulate_occ', 'Tabulate Occurrence')]\n",
        "\n",
        "env_vars_macrostrat = [\n",
        "    ('total_area_km2', 'Total Area'),\n",
        "    ('carbonate_area_km2', 'Carb Area'),\n",
        "    ('carbonate_percentage', 'Carb %')\n",
        "]\n",
        "\n",
        "# IMPORTANT: δ13C expected as 'd13C'\n",
        "env_vars_proxies = [\n",
        "    ('temperature', 'SST'),\n",
        "    ('sea_level', 'Sea Level'),\n",
        "    ('atmospheric_O2', 'Atm O2'),\n",
        "    ('atmospheric_CO2', 'Atm CO2'),\n",
        "    ('dissolved_O2', 'Dissolved O2'),\n",
        "    ('d13C', 'δ¹³C')\n",
        "]\n",
        "\n",
        "var_groups = [\n",
        "    ('Strom Props', strom_prop_vars),\n",
        "    ('Strom Occ', strom_occ_vars),\n",
        "    ('Strom Div', strom_div_vars),\n",
        "    ('Coral Div', coral_div_vars),\n",
        "    ('Coral Occ', coral_occ_vars),\n",
        "    ('Macrostrat', env_vars_macrostrat),\n",
        "    ('Proxies', env_vars_proxies),\n",
        "]\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. ANALYSIS FUNCTIONS\n",
        "# -----------------------------------------------------------------------------\n",
        "def get_significance_stars(p):\n",
        "    if p is None or np.isnan(p):\n",
        "        return \"\"\n",
        "    if p < 0.001:\n",
        "        return \"***\"\n",
        "    if p < 0.01:\n",
        "        return \"**\"\n",
        "    if p < 0.05:\n",
        "        return \"*\"\n",
        "    return \"\"\n",
        "\n",
        "def calc_stats(x, y):\n",
        "    mask = np.isfinite(x) & np.isfinite(y)\n",
        "    x = x[mask]\n",
        "    y = y[mask]\n",
        "    n = len(x)\n",
        "    if n < 5:\n",
        "        return dict(n=n, spearman_rho=np.nan, spearman_p=np.nan, pearson_r=np.nan, pearson_p=np.nan)\n",
        "    sr = stats.spearmanr(x, y)\n",
        "    pr = stats.pearsonr(x, y)\n",
        "    return dict(\n",
        "        n=n,\n",
        "        spearman_rho=float(sr.correlation),\n",
        "        spearman_p=float(sr.pvalue),\n",
        "        pearson_r=float(pr.statistic),\n",
        "        pearson_p=float(pr.pvalue)\n",
        "    )\n",
        "\n",
        "def run_correlation_suite(dataset, target_col, target_name):\n",
        "    if dataset is None or dataset.empty or target_col not in dataset.columns:\n",
        "        print(f\"[WARN] Missing target {target_col} or dataset empty; skipping {target_name}.\")\n",
        "        return []\n",
        "\n",
        "    print(\"\\n\" + \"-\"*90)\n",
        "    print(f\"TARGET: {target_name.upper()}  |  rows used (after no-strom filter): {len(dataset)}\")\n",
        "    print(\"-\"*90)\n",
        "    print(\"{:<35} {:>8} {:>10} {:>8} {:>10} {:>5}\".format(\"Variable\", \"rho\", \"p(rho)\", \"r\", \"p(r)\", \"n\"))\n",
        "\n",
        "    results = []\n",
        "    y = pd.to_numeric(dataset[target_col], errors='coerce').values.astype(float)\n",
        "\n",
        "    for group_name, group_vars in var_groups:\n",
        "        for var, label in group_vars:\n",
        "            if var not in dataset.columns:\n",
        "                continue\n",
        "\n",
        "            x = pd.to_numeric(dataset[var], errors='coerce').values.astype(float)\n",
        "            s = calc_stats(x, y)\n",
        "\n",
        "            # Only print/store if enough data\n",
        "            if s['n'] >= 5 and np.isfinite(s['spearman_rho']):\n",
        "                s_sig = get_significance_stars(s['spearman_p'])\n",
        "                p_sig = get_significance_stars(s['pearson_p'])\n",
        "\n",
        "                print(f\"{label:35s} {s['spearman_rho']:8.2f}{s_sig:3s} {s['spearman_p']:10.3g} \"\n",
        "                      f\"{s['pearson_r']:8.2f}{p_sig:3s} {s['pearson_p']:10.3g} {s['n']:5d}\")\n",
        "\n",
        "                results.append({\n",
        "                    'Scope': 'Stage' if dataset is df_corr else '5-Myr',\n",
        "                    'Target': target_col,\n",
        "                    'Predictor': var,\n",
        "                    'Label': label,\n",
        "                    'Group': group_name,\n",
        "                    'Spearman_Rho': s['spearman_rho'],\n",
        "                    'Spearman_P': s['spearman_p'],\n",
        "                    'Pearson_R': s['pearson_r'],\n",
        "                    'Pearson_P': s['pearson_p'],\n",
        "                    'N': int(s['n'])\n",
        "                })\n",
        "\n",
        "    return results\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. RUN: Stage + 5-Myr\n",
        "# -----------------------------------------------------------------------------\n",
        "all_results_stage = []\n",
        "for target_col, target_name in reef_targets:\n",
        "    all_results_stage.extend(run_correlation_suite(df_corr, target_col, target_name))\n",
        "\n",
        "all_results_5myr = []\n",
        "for target_col, target_name in reef_targets:\n",
        "    all_results_5myr.extend(run_correlation_suite(df_5myr_corr, target_col, target_name))\n",
        "\n",
        "stage_results_df = pd.DataFrame(all_results_stage)\n",
        "myr_results_df = pd.DataFrame(all_results_5myr)\n",
        "\n",
        "if 'OUTPUT_DIR' in globals():\n",
        "    if not stage_results_df.empty:\n",
        "        stage_results_df.to_csv(f\"{OUTPUT_DIR}/results_correlations_stage.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"\\nSaved: {OUTPUT_DIR}/results_correlations_stage.csv\")\n",
        "    else:\n",
        "        print(\"\\n[WARN] Stage correlations empty (nothing met n>=5 criteria).\")\n",
        "\n",
        "    if not myr_results_df.empty:\n",
        "        myr_results_df.to_csv(f\"{OUTPUT_DIR}/results_correlations_5myr.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"Saved: {OUTPUT_DIR}/results_correlations_5myr.csv\")\n",
        "    else:\n",
        "        print(\"[WARN] 5-Myr correlations empty (nothing met n>=5 criteria).\")\n",
        "\n",
        "# Show quick views\n",
        "display(stage_results_df if not stage_results_df.empty else pd.DataFrame())\n",
        "display(myr_results_df if not myr_results_df.empty else pd.DataFrame())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4oRd7FSq_Rab"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 13: PERFORM ADVANCED STATISTICAL ANALYSES (Centralized) — PART A (FIXED)\n",
        "#   Runs: (0) Spearman originals, (1) Bootstrap, (2) Permutation, (3) LOO,\n",
        "#         (4) Detrending, (5) Partial correlations, (6) Variance partitioning,\n",
        "#         (+) Univariate adjR2\n",
        "#\n",
        "# Key fixes incorporated:\n",
        "# - EXCLUDE stages with no stromatoporoids (strom_total_occ <= 0) from ALL analyses\n",
        "# - \"Absence\" columns are zero-filled (0 instead of empty) so they are not treated as NaN\n",
        "# - REMOVE basal_strom_prop everywhere (derived + basal sums to 1; avoids singularity)\n",
        "# - δ13C / atm_O2 / atm_CO2 included when present\n",
        "# - Detrending runs for ALL predictors (no silent skipping; status logged)\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "DATA_DIR = Path(\"./output\")\n",
        "OUTPUT_DIR = DATA_DIR\n",
        "N_BOOT = 10000\n",
        "N_PERM = 10000\n",
        "RNG_SEED = 0\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PERFORMING ADVANCED STATISTICS (CENTRALIZED) — PART A (FIXED)\")\n",
        "print(f\"Iterations: Bootstrap={N_BOOT}, Permutation={N_PERM}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# -------------------------\n",
        "# Load master dataset\n",
        "# -------------------------\n",
        "try:\n",
        "    df = pd.read_csv(DATA_DIR / \"MASTER_dataset_stage.csv\", encoding=\"utf-8-sig\")\n",
        "    print(f\"Loaded Master Dataset: {len(df)} rows\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: MASTER_dataset_stage.csv not found.\")\n",
        "    raise\n",
        "\n",
        "if df.empty:\n",
        "    raise SystemExit(\"ERROR: Dataset is empty.\")\n",
        "\n",
        "# -------------------------\n",
        "# Core columns\n",
        "# -------------------------\n",
        "target = \"thickness_mean\"\n",
        "if target not in df.columns:\n",
        "    raise SystemExit(f\"ERROR: target column '{target}' not found in MASTER dataset.\")\n",
        "\n",
        "# -------------------------\n",
        "# NEW: zero-fill \"absence\" columns so they aren't treated as NaN\n",
        "#  - DO NOT zero-fill environmental proxies or time/target\n",
        "# -------------------------\n",
        "DO_NOT_ZERO = set([c for c in [\n",
        "    \"midpoint_ma\", \"stage\", target,\n",
        "    \"temperature\", \"dissolved_O2\", \"carbonate_area_km2\", \"d13C\",\n",
        "    \"atm_O2\", \"atm_CO2\", \"pCO2\", \"pO2\"  # include common alternates\n",
        "] if c in df.columns])\n",
        "\n",
        "ZERO_FILL_COLS = []\n",
        "\n",
        "# typical biotic absence-coded columns\n",
        "for c in df.columns:\n",
        "    if c.endswith(\"_prop\") or c.endswith(\"_occ\") or c.endswith(\"_div\"):\n",
        "        ZERO_FILL_COLS.append(c)\n",
        "\n",
        "# typical counts\n",
        "for c in [\"reef_count\", \"strom_total_occ\"]:\n",
        "    if c in df.columns:\n",
        "        ZERO_FILL_COLS.append(c)\n",
        "\n",
        "# remove protected columns\n",
        "ZERO_FILL_COLS = sorted(set([c for c in ZERO_FILL_COLS if c not in DO_NOT_ZERO]))\n",
        "\n",
        "for c in ZERO_FILL_COLS:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "print(f\"[INFO] Zero-filled {len(ZERO_FILL_COLS)} absence-coded columns to 0 (not NaN).\")\n",
        "\n",
        "# -------------------------\n",
        "# Enforce numeric types for core fields (no fill)\n",
        "# -------------------------\n",
        "if \"midpoint_ma\" in df.columns:\n",
        "    df[\"midpoint_ma\"] = pd.to_numeric(df[\"midpoint_ma\"], errors=\"coerce\")\n",
        "df[target] = pd.to_numeric(df[target], errors=\"coerce\")\n",
        "\n",
        "# -------------------------\n",
        "# Helpers\n",
        "# -------------------------\n",
        "def _filter_no_strom(sub: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Exclude stages with no stromatoporoid occurrences.\"\"\"\n",
        "    if \"strom_total_occ\" in sub.columns:\n",
        "        sub = sub[pd.to_numeric(sub[\"strom_total_occ\"], errors=\"coerce\").fillna(0) > 0].copy()\n",
        "    return sub\n",
        "\n",
        "def get_analysis_data(d: pd.DataFrame, pred: str, targ: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Single source of truth filtering used by ALL stats:\n",
        "    - require pred & target numeric and non-NaN\n",
        "    - exclude no-strom rows (strom_total_occ<=0) if column exists\n",
        "    \"\"\"\n",
        "    cols = [pred, targ]\n",
        "    if \"strom_total_occ\" in d.columns:\n",
        "        cols.append(\"strom_total_occ\")\n",
        "    if \"midpoint_ma\" in d.columns:\n",
        "        cols.append(\"midpoint_ma\")\n",
        "    if \"stage\" in d.columns:\n",
        "        cols.append(\"stage\")\n",
        "\n",
        "    sub = d[cols].copy()\n",
        "    sub[pred] = pd.to_numeric(sub[pred], errors=\"coerce\")\n",
        "    sub[targ] = pd.to_numeric(sub[targ], errors=\"coerce\")\n",
        "    sub = sub.dropna(subset=[pred, targ])\n",
        "    sub = _filter_no_strom(sub)\n",
        "    return sub\n",
        "\n",
        "def _min_n_for_partial(k_controls: int) -> int:\n",
        "    # needs > controls + intercept + a little slack; avoid empty partial outputs\n",
        "    return max(8, k_controls + 3)\n",
        "\n",
        "def _safe_spearman(x, y):\n",
        "    x = pd.to_numeric(pd.Series(x), errors=\"coerce\")\n",
        "    y = pd.to_numeric(pd.Series(y), errors=\"coerce\")\n",
        "    m = np.isfinite(x.values) & np.isfinite(y.values)\n",
        "    if m.sum() < 5:\n",
        "        return np.nan, np.nan\n",
        "    return stats.spearmanr(x.values[m], y.values[m])\n",
        "\n",
        "# RNG\n",
        "rng = np.random.default_rng(RNG_SEED)\n",
        "\n",
        "# -------------------------\n",
        "# Predictor sets (basal INCLUDED for correlation-family analyses)\n",
        "# -------------------------\n",
        "strom_vars = [v for v in [\"derived_strom_prop\", \"basal_strom_prop\"] if v in df.columns]\n",
        "coral_vars = [v for v in [\"rugose_div\", \"tabulate_div\"] if v in df.columns]\n",
        "\n",
        "# Core env controls (use these for partial correlations to avoid NaN wipeouts)\n",
        "env_vars_core = [v for v in [\"carbonate_area_km2\", \"temperature\", \"dissolved_O2\"] if v in df.columns]\n",
        "\n",
        "# Additional predictors (treated as predictors, NOT forced into partial-control set)\n",
        "optional_env_predictors = []\n",
        "for v in [\"d13C\", \"atm_O2\", \"atm_CO2\", \"pO2\", \"pCO2\"]:\n",
        "    if v in df.columns and v not in env_vars_core:\n",
        "        optional_env_predictors.append(v)\n",
        "\n",
        "# Individual taxa\n",
        "individual_predictors = [\n",
        "    \"Labechiida_prop\", \"Clathrodictyida_prop\", \"Actinostromatida_prop\",\n",
        "    \"Stromatoporida_prop\", \"Stromatoporellida_prop\", \"Syringostromatida_prop\",\n",
        "    \"Amphiporida_prop\"\n",
        "]\n",
        "individual_predictors = [v for v in individual_predictors if v in df.columns]\n",
        "\n",
        "# Final predictor list (basal now included everywhere)\n",
        "all_predictors = list(dict.fromkeys(strom_vars + coral_vars + env_vars_core + optional_env_predictors + individual_predictors))\n",
        "\n",
        "print(\"[INFO] Predictors included:\", all_predictors)\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. ORIGINAL SPEARMAN CORRELATIONS (ALL predictors)\n",
        "# ==============================================================================\n",
        "print(\"\\n0. Computing original Spearman correlations (ALL predictors)...\")\n",
        "corr_rows = []\n",
        "for pred in all_predictors:\n",
        "    sub = get_analysis_data(df, pred, target)\n",
        "    r, p = _safe_spearman(sub[pred], sub[target]) if len(sub) else (np.nan, np.nan)\n",
        "    corr_rows.append({\n",
        "        \"Dataset\": \"Stage-Level Data\",\n",
        "        \"Target\": target,\n",
        "        \"Predictor\": pred,\n",
        "        \"N\": int(len(sub)),\n",
        "        \"spearman_rho\": r,\n",
        "        \"spearman_p\": p\n",
        "    })\n",
        "corr_df = pd.DataFrame(corr_rows)\n",
        "corr_df.to_csv(OUTPUT_DIR / \"results_spearman_original_all_predictors.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_spearman_original_all_predictors.csv\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. BOOTSTRAP (ALL predictors)\n",
        "# ==============================================================================\n",
        "print(f\"\\n1. Running Bootstrap Analysis ({N_BOOT} iter; ALL predictors)...\")\n",
        "boot_summary = []\n",
        "boot_dist_long = []  # long: Predictor, iter, rho\n",
        "\n",
        "for pred in all_predictors:\n",
        "    sub = get_analysis_data(df, pred, target)\n",
        "    n = len(sub)\n",
        "\n",
        "    if n < 5:\n",
        "        boot_summary.append({\n",
        "            \"Dataset\": \"Stage-Level Data\",\n",
        "            \"Target\": target,\n",
        "            \"Predictor\": pred,\n",
        "            \"N\": int(n),\n",
        "            \"spearman_observed\": np.nan,\n",
        "            \"spearman_ci_low\": np.nan,\n",
        "            \"spearman_ci_high\": np.nan,\n",
        "            \"Status\": \"SKIP_too_few_rows\"\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    rho_obs, _ = stats.spearmanr(sub[pred], sub[target])\n",
        "\n",
        "    idx = np.arange(n)\n",
        "    rhos = np.empty(N_BOOT, dtype=float)\n",
        "    for b in range(N_BOOT):\n",
        "        s = rng.choice(idx, size=n, replace=True)\n",
        "        r, _ = stats.spearmanr(sub[pred].iloc[s], sub[target].iloc[s])\n",
        "        rhos[b] = r\n",
        "\n",
        "    ci_low = np.nanpercentile(rhos, 2.5)\n",
        "    ci_high = np.nanpercentile(rhos, 97.5)\n",
        "\n",
        "    boot_summary.append({\n",
        "        \"Dataset\": \"Stage-Level Data\",\n",
        "        \"Target\": target,\n",
        "        \"Predictor\": pred,\n",
        "        \"N\": int(n),\n",
        "        \"spearman_observed\": float(rho_obs),\n",
        "        \"spearman_ci_low\": float(ci_low),\n",
        "        \"spearman_ci_high\": float(ci_high),\n",
        "        \"Status\": \"OK\"\n",
        "    })\n",
        "    boot_dist_long.extend([{\"Predictor\": pred, \"iter\": i+1, \"rho\": float(rhos[i])} for i in range(N_BOOT)])\n",
        "\n",
        "pd.DataFrame(boot_summary).to_csv(OUTPUT_DIR / \"results_bootstrap.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "pd.DataFrame(boot_dist_long).to_csv(OUTPUT_DIR / \"results_bootstrap_dist_all_predictors_long.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_bootstrap.csv\")\n",
        "print(\"  -> Saved results_bootstrap_dist_all_predictors_long.csv\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. PERMUTATION (ALL predictors)\n",
        "# ==============================================================================\n",
        "print(f\"2. Running Permutation Testing ({N_PERM} iter; ALL predictors)...\")\n",
        "perm_summary = []\n",
        "perm_dist_long = []\n",
        "\n",
        "for pred in all_predictors:\n",
        "    sub = get_analysis_data(df, pred, target)\n",
        "    n = len(sub)\n",
        "\n",
        "    if n < 5:\n",
        "        perm_summary.append({\n",
        "            \"Dataset\": \"Stage-Level Data\",\n",
        "            \"Target\": target,\n",
        "            \"Predictor\": pred,\n",
        "            \"N\": int(n),\n",
        "            \"spearman_observed\": np.nan,\n",
        "            \"spearman_p_permutation\": np.nan,\n",
        "            \"Status\": \"SKIP_too_few_rows\"\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    rho_obs, _ = stats.spearmanr(sub[pred], sub[target])\n",
        "    y = sub[target].values\n",
        "\n",
        "    perm_rhos = np.empty(N_PERM, dtype=float)\n",
        "    extreme = 0\n",
        "\n",
        "    for i in range(N_PERM):\n",
        "        y_perm = rng.permutation(y)\n",
        "        r_perm, _ = stats.spearmanr(sub[pred].values, y_perm)\n",
        "        perm_rhos[i] = r_perm\n",
        "        if np.isfinite(r_perm) and np.isfinite(rho_obs) and (abs(r_perm) >= abs(rho_obs)):\n",
        "            extreme += 1\n",
        "\n",
        "    p_val = extreme / N_PERM\n",
        "\n",
        "    perm_summary.append({\n",
        "        \"Dataset\": \"Stage-Level Data\",\n",
        "        \"Target\": target,\n",
        "        \"Predictor\": pred,\n",
        "        \"N\": int(n),\n",
        "        \"spearman_observed\": float(rho_obs),\n",
        "        \"spearman_p_permutation\": float(p_val),\n",
        "        \"Status\": \"OK\"\n",
        "    })\n",
        "    perm_dist_long.extend([{\"Predictor\": pred, \"iter\": i+1, \"rho\": float(perm_rhos[i])} for i in range(N_PERM)])\n",
        "\n",
        "pd.DataFrame(perm_summary).to_csv(OUTPUT_DIR / \"results_permutation.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "pd.DataFrame(perm_dist_long).to_csv(OUTPUT_DIR / \"results_permutation_dist_all_predictors_long.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_permutation.csv\")\n",
        "print(\"  -> Saved results_permutation_dist_all_predictors_long.csv\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. LEAVE-ONE-OUT (ALL predictors) — detailed + summary\n",
        "# ==============================================================================\n",
        "print(\"3. Running Leave-One-Out Analysis (ALL predictors)...\")\n",
        "loo_detailed_long = []\n",
        "loo_summary = []\n",
        "\n",
        "for pred in all_predictors:\n",
        "    cols = [\"stage\", pred, target] + ([\"strom_total_occ\"] if \"strom_total_occ\" in df.columns else [])\n",
        "    v = df[cols].copy()\n",
        "    v[pred] = pd.to_numeric(v[pred], errors=\"coerce\")\n",
        "    v[target] = pd.to_numeric(v[target], errors=\"coerce\")\n",
        "    v = v.dropna(subset=[\"stage\", pred, target])\n",
        "    v = _filter_no_strom(v).reset_index(drop=True)\n",
        "\n",
        "    n = len(v)\n",
        "    if n < 5:\n",
        "        loo_summary.append({\n",
        "            \"Dataset\": \"Stage-Level Data\",\n",
        "            \"Target\": target,\n",
        "            \"Predictor\": pred,\n",
        "            \"N\": int(n),\n",
        "            \"LOO_Mean_Rho\": np.nan,\n",
        "            \"LOO_Max_P\": np.nan,\n",
        "            \"LOO_Min_Rho\": np.nan,\n",
        "            \"LOO_Max_Rho\": np.nan,\n",
        "            \"Status\": \"SKIP_too_few_rows\"\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    rho_full, _ = stats.spearmanr(v[pred], v[target])\n",
        "\n",
        "    rhos = []\n",
        "    ps = []\n",
        "    for i in range(n):\n",
        "        vv = v.drop(index=i)\n",
        "        r, p = _safe_spearman(vv[pred], vv[target])\n",
        "        rhos.append(r)\n",
        "        ps.append(p)\n",
        "        loo_detailed_long.append({\n",
        "            \"Predictor\": pred,\n",
        "            \"Stage_Dropped\": v.loc[i, \"stage\"],\n",
        "            \"LOO_Rho\": r,\n",
        "            \"Diff_from_Full\": (r - rho_full) if (np.isfinite(r) and np.isfinite(rho_full)) else np.nan\n",
        "        })\n",
        "\n",
        "    loo_summary.append({\n",
        "        \"Dataset\": \"Stage-Level Data\",\n",
        "        \"Target\": target,\n",
        "        \"Predictor\": pred,\n",
        "        \"N\": int(n),\n",
        "        \"LOO_Mean_Rho\": float(np.nanmean(rhos)),\n",
        "        \"LOO_Max_P\": float(np.nanmax(ps)),\n",
        "        \"LOO_Min_Rho\": float(np.nanmin(rhos)),\n",
        "        \"LOO_Max_Rho\": float(np.nanmax(rhos)),\n",
        "        \"Status\": \"OK\"\n",
        "    })\n",
        "\n",
        "pd.DataFrame(loo_detailed_long).to_csv(OUTPUT_DIR / \"results_loo_detailed_all_predictors_long.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "pd.DataFrame(loo_summary).to_csv(OUTPUT_DIR / \"results_loo_summary.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_loo_detailed_all_predictors_long.csv\")\n",
        "print(\"  -> Saved results_loo_summary.csv\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. DETRENDING (ALL predictors) — status logged; no silent skipping\n",
        "# ==============================================================================\n",
        "print(\"4. Running Detrending Analysis (ALL predictors; status logged)...\")\n",
        "det_results = []\n",
        "\n",
        "if \"midpoint_ma\" not in df.columns:\n",
        "    print(\"  [ERROR] midpoint_ma missing -> detrending cannot run.\")\n",
        "else:\n",
        "    for pred in all_predictors:\n",
        "        row = {\"Dataset\": \"Stage-Level Data\", \"Target\": target, \"Predictor\": pred, \"N\": 0,\n",
        "               \"Detrend_Rho\": np.nan, \"Detrend_P\": np.nan, \"Status\": \"INIT\"}\n",
        "\n",
        "        cols = [\"midpoint_ma\", pred, target] + ([\"strom_total_occ\"] if \"strom_total_occ\" in df.columns else [])\n",
        "        if pred not in df.columns:\n",
        "            row[\"Status\"] = \"SKIP_missing_predictor_col\"\n",
        "            det_results.append(row)\n",
        "            continue\n",
        "\n",
        "        v = df[cols].copy()\n",
        "        v[\"midpoint_ma\"] = pd.to_numeric(v[\"midpoint_ma\"], errors=\"coerce\")\n",
        "        v[pred] = pd.to_numeric(v[pred], errors=\"coerce\")\n",
        "        v[target] = pd.to_numeric(v[target], errors=\"coerce\")\n",
        "        v = v.dropna(subset=[\"midpoint_ma\", pred, target])\n",
        "        v = _filter_no_strom(v)\n",
        "        n = len(v)\n",
        "        row[\"N\"] = int(n)\n",
        "\n",
        "        if n < 5:\n",
        "            row[\"Status\"] = \"SKIP_too_few_rows\"\n",
        "            det_results.append(row)\n",
        "            continue\n",
        "\n",
        "        if v[pred].nunique(dropna=True) < 2 or float(np.nanstd(v[pred].values)) == 0.0:\n",
        "            row[\"Status\"] = \"SKIP_constant_predictor\"\n",
        "            det_results.append(row)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            sx, ix, *_ = stats.linregress(v[\"midpoint_ma\"], v[pred])\n",
        "            sy, iy, *_ = stats.linregress(v[\"midpoint_ma\"], v[target])\n",
        "\n",
        "            resid_x = v[pred] - (sx * v[\"midpoint_ma\"] + ix)\n",
        "            resid_y = v[target] - (sy * v[\"midpoint_ma\"] + iy)\n",
        "\n",
        "            r, p = stats.spearmanr(resid_x, resid_y, nan_policy=\"omit\")\n",
        "            row[\"Detrend_Rho\"] = r\n",
        "            row[\"Detrend_P\"] = p\n",
        "            row[\"Status\"] = \"OK\"\n",
        "        except Exception as e:\n",
        "            row[\"Status\"] = f\"FAIL_{type(e).__name__}\"\n",
        "\n",
        "        det_results.append(row)\n",
        "\n",
        "pd.DataFrame(det_results).to_csv(OUTPUT_DIR / \"results_detrending_improved.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_detrending_improved.csv\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. PARTIAL CORRELATIONS (ALL predictors) — FIXED to actually compute\n",
        "# ==============================================================================\n",
        "print(\"\\n5. Running Partial Correlations (ALL predictors; fixed controls to avoid NaN wipeouts)...\")\n",
        "\n",
        "part_rows = []\n",
        "\n",
        "# controls that are usually complete\n",
        "env_controls = env_vars_core[:]  # carbonate_area_km2, temperature, dissolved_O2\n",
        "\n",
        "# biotic controls for environmental predictors (avoid derived+basal together)\n",
        "biotic_controls_for_env = []\n",
        "for v in [\"derived_strom_prop\", \"rugose_div\", \"tabulate_div\"]:\n",
        "    if v in df.columns:\n",
        "        biotic_controls_for_env.append(v)\n",
        "\n",
        "def _min_n_for_partial(k_controls: int) -> int:\n",
        "    # keep it permissive given n~22, but still meaningful\n",
        "    return max(6, k_controls + 3)\n",
        "\n",
        "for pred in all_predictors:\n",
        "    if pred not in df.columns:\n",
        "        continue\n",
        "\n",
        "    # decide control set\n",
        "    if pred in env_controls or pred in optional_env_predictors:\n",
        "        controls = biotic_controls_for_env\n",
        "        test_type = \"Environment (Biotic Controlled)\"\n",
        "    else:\n",
        "        controls = env_controls\n",
        "        test_type = \"Biotic/Taxon (Env Controlled)\"\n",
        "\n",
        "    # need at least 2 controls to do a partial\n",
        "    if len(controls) < 2:\n",
        "        part_rows.append({\n",
        "            \"Dataset\": \"Stage-Level Data\",\n",
        "            \"Target\": target,\n",
        "            \"Predictor\": pred,\n",
        "            \"Test_Type\": test_type,\n",
        "            \"Controls\": \",\".join(controls),\n",
        "            \"N\": 0,\n",
        "            \"spearman_partial\": np.nan,\n",
        "            \"spearman_p_partial\": np.nan,\n",
        "            \"Status\": \"SKIP_insufficient_controls\"\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    cols = [pred, target] + controls + ([\"strom_total_occ\"] if \"strom_total_occ\" in df.columns else [])\n",
        "    sub = df[cols].copy()\n",
        "\n",
        "    # numeric coercion\n",
        "    for c in [pred, target] + controls:\n",
        "        sub[c] = pd.to_numeric(sub[c], errors=\"coerce\")\n",
        "\n",
        "    # drop missing for required fields\n",
        "    sub = sub.dropna(subset=[pred, target] + controls)\n",
        "\n",
        "    # global: require strom presence\n",
        "    sub = _filter_no_strom(sub)\n",
        "\n",
        "    n = len(sub)\n",
        "    min_n = _min_n_for_partial(len(controls))\n",
        "    if n < min_n:\n",
        "        part_rows.append({\n",
        "            \"Dataset\": \"Stage-Level Data\",\n",
        "            \"Target\": target,\n",
        "            \"Predictor\": pred,\n",
        "            \"Test_Type\": test_type,\n",
        "            \"Controls\": \",\".join(controls),\n",
        "            \"N\": int(n),\n",
        "            \"spearman_partial\": np.nan,\n",
        "            \"spearman_p_partial\": np.nan,\n",
        "            \"Status\": f\"SKIP_too_few_rows(n<{min_n})\"\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # guard against constant predictor on this subset\n",
        "    if sub[pred].nunique(dropna=True) < 2 or float(np.nanstd(sub[pred].values)) == 0.0:\n",
        "        part_rows.append({\n",
        "            \"Dataset\": \"Stage-Level Data\",\n",
        "            \"Target\": target,\n",
        "            \"Predictor\": pred,\n",
        "            \"Test_Type\": test_type,\n",
        "            \"Controls\": \",\".join(controls),\n",
        "            \"N\": int(n),\n",
        "            \"spearman_partial\": np.nan,\n",
        "            \"spearman_p_partial\": np.nan,\n",
        "            \"Status\": \"SKIP_constant_predictor\"\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        Xc = sm.add_constant(sub[controls], has_constant=\"add\")\n",
        "        # residualize predictor and target vs controls\n",
        "        res_pred = sm.OLS(sub[pred].values.astype(float), Xc.values.astype(float)).fit().resid\n",
        "        res_targ = sm.OLS(sub[target].values.astype(float), Xc.values.astype(float)).fit().resid\n",
        "        r, p = stats.spearmanr(res_pred, res_targ, nan_policy=\"omit\")\n",
        "\n",
        "        part_rows.append({\n",
        "            \"Dataset\": \"Stage-Level Data\",\n",
        "            \"Target\": target,\n",
        "            \"Predictor\": pred,\n",
        "            \"Test_Type\": test_type,\n",
        "            \"Controls\": \",\".join(controls),\n",
        "            \"N\": int(n),\n",
        "            \"spearman_partial\": r,\n",
        "            \"spearman_p_partial\": p,\n",
        "            \"Status\": \"OK\"\n",
        "        })\n",
        "    except Exception as e:\n",
        "        part_rows.append({\n",
        "            \"Dataset\": \"Stage-Level Data\",\n",
        "            \"Target\": target,\n",
        "            \"Predictor\": pred,\n",
        "            \"Test_Type\": test_type,\n",
        "            \"Controls\": \",\".join(controls),\n",
        "            \"N\": int(n),\n",
        "            \"spearman_partial\": np.nan,\n",
        "            \"spearman_p_partial\": np.nan,\n",
        "            \"Status\": f\"FAIL_{type(e).__name__}\"\n",
        "        })\n",
        "\n",
        "part_df = pd.DataFrame(part_rows).sort_values([\"Status\", \"spearman_p_partial\"], na_position=\"last\")\n",
        "part_df.to_csv(OUTPUT_DIR / \"results_partial_correlations.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_partial_correlations.csv\")\n",
        "\n",
        "print(\"\\n[DISPLAY] Partial correlations (OK rows):\")\n",
        "display(part_df[part_df[\"Status\"]==\"OK\"].sort_values(\"spearman_p_partial\", na_position=\"last\"))\n",
        "\n",
        "print(\"\\n[DISPLAY] Partial correlations (all rows incl. skips/fails):\")\n",
        "display(part_df)\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. VARIANCE PARTITIONING (Strom vs Coral vs Env) — uses ONLY derived for strom\n",
        "# ==============================================================================\n",
        "print(\"6. Running Variance Partitioning (groups)...\")\n",
        "\n",
        "def get_adj_r2_group(data: pd.DataFrame, y_col: str, x_cols: list) -> float:\n",
        "    if len(x_cols) == 0:\n",
        "        return np.nan\n",
        "    vv = data[[y_col] + x_cols].copy()\n",
        "    for c in [y_col] + x_cols:\n",
        "        vv[c] = pd.to_numeric(vv[c], errors=\"coerce\")\n",
        "    vv = vv.dropna()\n",
        "    if len(vv) < len(x_cols) + 2:\n",
        "        return np.nan\n",
        "    X = sm.add_constant(vv[x_cols], has_constant=\"add\")\n",
        "    y = vv[y_col].values.astype(float)\n",
        "    try:\n",
        "        return float(sm.OLS(y, X.values.astype(float)).fit(method=\"qr\").rsquared_adj)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "# build common subset for VP (must have all group predictors + target)\n",
        "vp_predictors = strom_vars + coral_vars + env_controls\n",
        "vp_cols = [target] + vp_predictors + ([\"strom_total_occ\"] if \"strom_total_occ\" in df.columns else [])\n",
        "sub_vp = df[vp_cols].copy()\n",
        "\n",
        "for c in [target] + vp_predictors:\n",
        "    sub_vp[c] = pd.to_numeric(sub_vp[c], errors=\"coerce\")\n",
        "\n",
        "sub_vp = sub_vp.dropna(subset=[target] + vp_predictors)\n",
        "sub_vp = _filter_no_strom(sub_vp)\n",
        "\n",
        "A = strom_vars\n",
        "B = coral_vars\n",
        "C = env_controls\n",
        "ABC = A + B + C\n",
        "\n",
        "r2_abc = get_adj_r2_group(sub_vp, target, ABC)\n",
        "r2_bc  = get_adj_r2_group(sub_vp, target, B + C)\n",
        "r2_ac  = get_adj_r2_group(sub_vp, target, A + C)\n",
        "r2_ab  = get_adj_r2_group(sub_vp, target, A + B)\n",
        "\n",
        "unique_a = r2_abc - r2_bc if np.isfinite(r2_abc) and np.isfinite(r2_bc) else np.nan\n",
        "unique_b = r2_abc - r2_ac if np.isfinite(r2_abc) and np.isfinite(r2_ac) else np.nan\n",
        "unique_c = r2_abc - r2_ab if np.isfinite(r2_abc) and np.isfinite(r2_ab) else np.nan\n",
        "shared   = r2_abc - (unique_a + unique_b + unique_c) if np.isfinite(r2_abc) else np.nan\n",
        "\n",
        "var_out = pd.DataFrame([{\n",
        "    \"Dataset\": \"Stage-Level Data\",\n",
        "    \"Target\": target,\n",
        "    \"Strom_Group\": \",\".join(A),\n",
        "    \"Coral_Group\": \",\".join(B),\n",
        "    \"Env_Group\": \",\".join(C),\n",
        "    \"Unique_Strom\": unique_a,\n",
        "    \"Unique_Coral\": unique_b,\n",
        "    \"Unique_Env\": unique_c,\n",
        "    \"Shared\": shared,\n",
        "    \"Residual\": (1 - r2_abc) if np.isfinite(r2_abc) else np.nan,\n",
        "    \"Total_R2_adj\": r2_abc,\n",
        "    \"N\": int(len(sub_vp))\n",
        "}])\n",
        "var_out.to_csv(OUTPUT_DIR / \"results_variance_partition_improved.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_variance_partition_improved.csv\")\n",
        "\n",
        "# ==============================================================================\n",
        "# + Univariate adj-R2 for ALL predictors (robust to singularities)\n",
        "# ==============================================================================\n",
        "print(\"6b. Running Univariate adj-R2 (ALL predictors; robust)...\")\n",
        "uni_rows = []\n",
        "\n",
        "for pred in all_predictors:\n",
        "    sub = get_analysis_data(df, pred, target)\n",
        "    n = len(sub)\n",
        "    if n < 6:\n",
        "        uni_rows.append({\"Predictor\": pred, \"N\": int(n), \"Adj_R2_univariate\": np.nan, \"Status\": \"SKIP_too_few_rows\"})\n",
        "        continue\n",
        "\n",
        "    x = pd.to_numeric(sub[pred], errors=\"coerce\")\n",
        "    y = pd.to_numeric(sub[target], errors=\"coerce\")\n",
        "    m = np.isfinite(x.values) & np.isfinite(y.values)\n",
        "    x = x[m]; y = y[m]\n",
        "    if len(x) < 6:\n",
        "        uni_rows.append({\"Predictor\": pred, \"N\": int(len(x)), \"Adj_R2_univariate\": np.nan, \"Status\": \"SKIP_too_few_rows\"})\n",
        "        continue\n",
        "\n",
        "    if x.nunique(dropna=True) < 2 or float(np.nanstd(x.values)) == 0.0:\n",
        "        uni_rows.append({\"Predictor\": pred, \"N\": int(len(x)), \"Adj_R2_univariate\": np.nan, \"Status\": \"SKIP_constant_predictor\"})\n",
        "        continue\n",
        "\n",
        "    X = sm.add_constant(pd.DataFrame({pred: x.values}), has_constant=\"add\").values.astype(float)\n",
        "    yy = y.values.astype(float)\n",
        "\n",
        "    try:\n",
        "        fit = sm.OLS(yy, X).fit(method=\"qr\")\n",
        "        uni_rows.append({\"Predictor\": pred, \"N\": int(len(x)), \"Adj_R2_univariate\": float(fit.rsquared_adj), \"Status\": \"OK\"})\n",
        "    except Exception as e:\n",
        "        uni_rows.append({\"Predictor\": pred, \"N\": int(len(x)), \"Adj_R2_univariate\": np.nan, \"Status\": f\"FAIL_{type(e).__name__}\"})\n",
        "\n",
        "pd.DataFrame(uni_rows).to_csv(OUTPUT_DIR / \"results_adjR2_univariate_all_predictors.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_adjR2_univariate_all_predictors.csv\")\n",
        "\n",
        "# Flag for downstream cells (13B / 14)\n",
        "CELL13_PART_A_DONE = True\n",
        "print(\"\\n✓ CELL 13 PART A COMPLETE (fixed).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZpjcVv1GLq1V"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 14: MODEL SELECTION + LOWESS + LAG (Robust guards) — NO BASAL\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CELL 14: AICc + LOWESS + LAG (NO BASAL)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def _filter_no_strom(sub):\n",
        "    if 'strom_total_occ' in sub.columns:\n",
        "        sub = sub[sub['strom_total_occ'].notna() & (pd.to_numeric(sub['strom_total_occ'], errors='coerce') > 0)]\n",
        "    return sub\n",
        "\n",
        "# ---------------------------\n",
        "# 7. AICc MODEL SELECTION (COMMON SUBSET; Method 1) — strom/coral/env only\n",
        "# ---------------------------\n",
        "print(\"7. Running AICc Model Selection (COMMON SUBSET; strom/coral/env only)...\")\n",
        "\n",
        "target = 'thickness_mean'\n",
        "\n",
        "# requested groups (NO BASAL)\n",
        "strom_vars = [v for v in ['derived_strom_prop'] if v in df.columns]\n",
        "coral_vars = [v for v in ['rugose_div', 'tabulate_div'] if v in df.columns]\n",
        "env_vars   = [v for v in ['carbonate_area_km2', 'temperature', 'dissolved_O2'] if v in df.columns]\n",
        "\n",
        "needed = [target] + strom_vars + coral_vars + env_vars\n",
        "if 'strom_total_occ' in df.columns:\n",
        "    needed += ['strom_total_occ']\n",
        "\n",
        "sub_aic = df[needed].copy()\n",
        "sub_aic[target] = pd.to_numeric(sub_aic[target], errors='coerce')\n",
        "for c in strom_vars + coral_vars + env_vars:\n",
        "    sub_aic[c] = pd.to_numeric(sub_aic[c], errors='coerce')\n",
        "\n",
        "# COMMON SUBSET (complete for full set)\n",
        "sub_aic = sub_aic.dropna(subset=[target] + strom_vars + coral_vars + env_vars)\n",
        "sub_aic = _filter_no_strom(sub_aic)\n",
        "\n",
        "print(f\"  Common-subset N = {len(sub_aic)} rows\")\n",
        "\n",
        "models = {\n",
        "    'Strom+Coral': strom_vars + coral_vars,\n",
        "    'Strom-only':  strom_vars,\n",
        "    'Strom+Env':   strom_vars + env_vars,\n",
        "    'Full':        strom_vars + coral_vars + env_vars,\n",
        "    'Null':        [],\n",
        "    'Coral-only':  coral_vars,\n",
        "    'Env-only':    env_vars,\n",
        "    'Coral+Env':   coral_vars + env_vars,\n",
        "}\n",
        "\n",
        "aic_rows = []\n",
        "n = len(sub_aic)\n",
        "\n",
        "if n == 0:\n",
        "    print(\"  [SKIP] AICc: common-subset is empty.\")\n",
        "else:\n",
        "    y = sub_aic[target].values.astype(float)\n",
        "\n",
        "    for name, preds in models.items():\n",
        "        if len(preds) == 0:\n",
        "            X = np.ones((n, 1))\n",
        "        else:\n",
        "            X = sm.add_constant(sub_aic[preds], has_constant='add')\n",
        "            X = np.asarray(X, dtype=float)\n",
        "\n",
        "        k = X.shape[1]\n",
        "        if n <= k + 1:\n",
        "            print(f\"  [SKIP] {name}: too few rows for AICc (n={n}, k={k})\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            fit = sm.OLS(y, X).fit(method='qr')\n",
        "            aic = float(fit.aic)\n",
        "            aicc = aic + (2 * k * (k + 1)) / (n - k - 1)\n",
        "            aic_rows.append({\n",
        "                'Model': name,\n",
        "                'Predictors': str(preds),\n",
        "                'R2': float(fit.rsquared),\n",
        "                'Adj_R2': float(fit.rsquared_adj),\n",
        "                'AIC': aic,\n",
        "                'AICc': float(aicc),\n",
        "                'N': int(n),\n",
        "                'K': int(k)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"  [FAIL] {name}: fit failed -> {e}\")\n",
        "\n",
        "aic_df = pd.DataFrame(aic_rows)\n",
        "if aic_df.empty:\n",
        "    print(\"  [SKIP] AICc: no models could be fit.\")\n",
        "else:\n",
        "    aic_df = aic_df.sort_values('AICc')\n",
        "    min_aicc = aic_df['AICc'].min()\n",
        "    aic_df['Delta_AICc'] = aic_df['AICc'] - min_aicc\n",
        "    aic_df['Weight'] = np.exp(-0.5 * aic_df['Delta_AICc'])\n",
        "    aic_df['Weight'] = aic_df['Weight'] / aic_df['Weight'].sum()\n",
        "    aic_df.to_csv(OUTPUT_DIR / 'results_aic_improved.csv', index=False)\n",
        "    print(\"  -> Saved results_aic_improved.csv\")\n",
        "    display(aic_df)\n",
        "\n",
        "# ---------------------------\n",
        "# 8. LOWESS (keep: derived only now that basal is removed)\n",
        "# ---------------------------\n",
        "print(\"8. LOWESS fits (derived only; 1000 boot for plotting stability/speed)...\")\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "\n",
        "def get_lowess_ci(x, y, frac=0.6, n_boot=1000):\n",
        "    sort_idx = np.argsort(x)\n",
        "    x_sorted = x.iloc[sort_idx].values\n",
        "    y_sorted = y.iloc[sort_idx].values\n",
        "\n",
        "    z = lowess(y_sorted, x_sorted, frac=frac)\n",
        "    x_grid = z[:, 0]\n",
        "    y_fit = z[:, 1]\n",
        "\n",
        "    boot_curves = []\n",
        "    idx_all = np.arange(len(x))\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.choice(idx_all, size=len(idx_all), replace=True)\n",
        "        x_s = x.iloc[idx].values\n",
        "        y_s = y.iloc[idx].values\n",
        "        s_idx = np.argsort(x_s)\n",
        "        try:\n",
        "            z_b = lowess(y_s[s_idx], x_s[s_idx], frac=frac)\n",
        "            y_interp = np.interp(x_grid, z_b[:, 0], z_b[:, 1])\n",
        "            boot_curves.append(y_interp)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    boot_curves = np.array(boot_curves)\n",
        "    ci_low = np.nanpercentile(boot_curves, 2.5, axis=0)\n",
        "    ci_high = np.nanpercentile(boot_curves, 97.5, axis=0)\n",
        "    return pd.DataFrame({'x': x_grid, 'y_fit': y_fit, 'ci_low': ci_low, 'ci_high': ci_high})\n",
        "\n",
        "if 'derived_strom_prop' in df.columns:\n",
        "    v = df.dropna(subset=['derived_strom_prop', target]).copy()\n",
        "    v = _filter_no_strom(v)\n",
        "    low = get_lowess_ci(v['derived_strom_prop'], v[target])\n",
        "    low.to_csv(OUTPUT_DIR / 'results_lowess_derived.csv', index=False)\n",
        "    print(\"  -> Saved results_lowess_derived.csv\")\n",
        "\n",
        "# =============================================================================\n",
        "# LAG ANALYSIS (Max-|t| & |Cohen’s d|) — robust + saves outputs\n",
        "#   Outputs:\n",
        "#     - output/results_lag_summary_all_predictors.csv\n",
        "#     - output/results_lag_profile_all_predictors_long.csv\n",
        "#     - output/results_lag_profile_thickness.csv\n",
        "#   Also defines: peak_thick, peak_deriv (for segmented regression cell)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"./output\")\n",
        "OUTPUT_DIR = DATA_DIR\n",
        "\n",
        "# -------------------------\n",
        "# Ensure df exists\n",
        "# -------------------------\n",
        "if \"df\" not in globals() or df is None or not isinstance(df, pd.DataFrame) or df.empty:\n",
        "    df = pd.read_csv(DATA_DIR / \"MASTER_dataset_stage.csv\", encoding=\"utf-8-sig\")\n",
        "    print(f\"[INFO] Loaded df from MASTER_dataset_stage.csv: {len(df)} rows\")\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "target = \"thickness_mean\"\n",
        "AGE_MIN, AGE_MAX = 358.9, 485.4   # Ord–Dev window used previously\n",
        "MIN_BEFORE, MIN_AFTER = 3, 3\n",
        "\n",
        "# -------------------------\n",
        "# Build predictor list (use all_predictors if available; else reconstruct)\n",
        "# -------------------------\n",
        "if \"all_predictors\" in globals() and isinstance(all_predictors, (list, tuple)) and len(all_predictors) > 0:\n",
        "    predictors = [p for p in all_predictors if p in df.columns]\n",
        "else:\n",
        "    predictors = []\n",
        "    for p in [\n",
        "        \"derived_strom_prop\", \"basal_strom_prop\",\n",
        "        \"rugose_div\", \"tabulate_div\",\n",
        "        \"carbonate_area_km2\", \"temperature\", \"dissolved_O2\",\n",
        "        \"log_derived_basal_ratio\",\n",
        "        \"d13C\", \"atm_O2\", \"atm_CO2\", \"pO2\", \"pCO2\",\n",
        "        \"Labechiida_prop\", \"Clathrodictyida_prop\", \"Actinostromatida_prop\",\n",
        "        \"Stromatoporida_prop\", \"Stromatoporellida_prop\", \"Syringostromatida_prop\",\n",
        "        \"Amphiporida_prop\",\n",
        "    ]:\n",
        "        if p in df.columns:\n",
        "            predictors.append(p)\n",
        "\n",
        "# Ensure required columns exist\n",
        "needed_cols = [\"stage\", \"midpoint_ma\", \"strom_total_occ\", target]\n",
        "missing = [c for c in needed_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns for lag analysis: {missing}\")\n",
        "\n",
        "# -------------------------\n",
        "# Filter Ord–Dev + strom presence + required numeric\n",
        "# -------------------------\n",
        "work = df.copy()\n",
        "work[\"midpoint_ma\"] = pd.to_numeric(work[\"midpoint_ma\"], errors=\"coerce\")\n",
        "work[\"strom_total_occ\"] = pd.to_numeric(work[\"strom_total_occ\"], errors=\"coerce\")\n",
        "work[target] = pd.to_numeric(work[target], errors=\"coerce\")\n",
        "\n",
        "work = work.dropna(subset=[\"stage\", \"midpoint_ma\", \"strom_total_occ\", target]).copy()\n",
        "work = work[(work[\"midpoint_ma\"] >= AGE_MIN) & (work[\"midpoint_ma\"] <= AGE_MAX)].copy()\n",
        "work = work[work[\"strom_total_occ\"] > 0].copy()\n",
        "\n",
        "# Sort oldest->youngest (descending Ma)\n",
        "work = work.sort_values(\"midpoint_ma\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(f\"[INFO] Lag dataset after filters: n={len(work)} (Ord–Dev, strom_total_occ>0)\")\n",
        "\n",
        "def _cohen_d(before, after):\n",
        "    before = before[np.isfinite(before)]\n",
        "    after  = after[np.isfinite(after)]\n",
        "    if len(before) < 2 or len(after) < 2:\n",
        "        return np.nan\n",
        "    pooled_std = np.sqrt(((len(before)-1)*np.var(before, ddof=1) + (len(after)-1)*np.var(after, ddof=1)) /\n",
        "                         (len(before) + len(after) - 2))\n",
        "    if not np.isfinite(pooled_std) or pooled_std <= 0:\n",
        "        return np.nan\n",
        "    return (np.mean(after) - np.mean(before)) / pooled_std\n",
        "\n",
        "def lag_profile_for_var(df_sorted, var, min_before=3, min_after=3):\n",
        "    \"\"\"Return break profile rows for a single variable.\"\"\"\n",
        "    y = pd.to_numeric(df_sorted[var], errors=\"coerce\").values.astype(float)\n",
        "    ages = pd.to_numeric(df_sorted[\"midpoint_ma\"], errors=\"coerce\").values.astype(float)\n",
        "    stages = df_sorted[\"stage\"].astype(str).values\n",
        "\n",
        "    rows = []\n",
        "    n_all = len(df_sorted)\n",
        "    for i in range(min_before, n_all - min_after):\n",
        "        before = y[:i]\n",
        "        after  = y[i:]\n",
        "\n",
        "        before = before[np.isfinite(before)]\n",
        "        after  = after[np.isfinite(after)]\n",
        "\n",
        "        if len(before) >= 2 and len(after) >= 2:\n",
        "            t, p = stats.ttest_ind(before, after, equal_var=True, nan_policy=\"omit\")\n",
        "            d = _cohen_d(before, after)\n",
        "        else:\n",
        "            t, p, d = np.nan, np.nan, np.nan\n",
        "\n",
        "        rows.append({\n",
        "            \"Variable\": var,\n",
        "            \"stage\": stages[i],\n",
        "            \"age\": float(ages[i]) if np.isfinite(ages[i]) else np.nan,\n",
        "            \"n_before\": int(i),\n",
        "            \"n_after\": int(n_all - i),\n",
        "            \"t_abs\": float(np.abs(t)) if np.isfinite(t) else 0.0,\n",
        "            \"p\": float(p) if np.isfinite(p) else np.nan,\n",
        "            \"d_abs\": float(np.abs(d)) if np.isfinite(d) else 0.0\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# -------------------------\n",
        "# Compute thickness profile + peak\n",
        "# -------------------------\n",
        "prof_thick = lag_profile_for_var(work, target, MIN_BEFORE, MIN_AFTER)\n",
        "if prof_thick.empty:\n",
        "    raise RuntimeError(\"[ERROR] Thickness lag profile is empty (too few rows?)\")\n",
        "\n",
        "peak_thick = prof_thick.loc[prof_thick[\"t_abs\"].idxmax()].to_dict()\n",
        "\n",
        "# -------------------------\n",
        "# Compute predictor profiles + peaks\n",
        "# -------------------------\n",
        "profiles = [prof_thick]\n",
        "summary_rows = []\n",
        "\n",
        "peak_deriv = None  # composition changepoint\n",
        "\n",
        "for pred in predictors:\n",
        "    # skip if fully missing in this filtered dataset\n",
        "    if pred not in work.columns:\n",
        "        continue\n",
        "\n",
        "    prof = lag_profile_for_var(work, pred, MIN_BEFORE, MIN_AFTER)\n",
        "    if prof.empty:\n",
        "        continue\n",
        "\n",
        "    profiles.append(prof)\n",
        "\n",
        "    peak = prof.loc[prof[\"t_abs\"].idxmax()].to_dict()\n",
        "    lag_vs_thick = peak[\"age\"] - peak_thick[\"age\"] if np.isfinite(peak.get(\"age\", np.nan)) else np.nan\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"Predictor\": pred,\n",
        "        \"Peak_Age\": peak[\"age\"],\n",
        "        \"Peak_Stage\": peak[\"stage\"],\n",
        "        \"Max_t_abs\": peak[\"t_abs\"],\n",
        "        \"Max_d_abs\": peak[\"d_abs\"],\n",
        "        \"p_at_peak\": peak[\"p\"],\n",
        "        \"Lag_vs_Thickness_Myr\": lag_vs_thick,\n",
        "        \"Thickness_Peak_Age\": peak_thick[\"age\"],\n",
        "        \"Thickness_Peak_Stage\": peak_thick[\"stage\"],\n",
        "    })\n",
        "\n",
        "    if pred == \"derived_strom_prop\":\n",
        "        peak_deriv = peak\n",
        "\n",
        "# fallback for peak_deriv (so downstream cells won't break)\n",
        "if peak_deriv is None:\n",
        "    peak_deriv = {\"age\": np.nan, \"stage\": None, \"t_abs\": np.nan, \"d_abs\": np.nan}\n",
        "\n",
        "lag_summary_df = pd.DataFrame(summary_rows).sort_values(\"Max_t_abs\", ascending=False)\n",
        "lag_profiles_long = pd.concat(profiles, ignore_index=True)\n",
        "\n",
        "# -------------------------\n",
        "# Save\n",
        "# -------------------------\n",
        "(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "lag_summary_df.to_csv(OUTPUT_DIR / \"results_lag_summary_all_predictors.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "lag_profiles_long.to_csv(OUTPUT_DIR / \"results_lag_profile_all_predictors_long.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "prof_thick.to_csv(OUTPUT_DIR / \"results_lag_profile_thickness.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"\\n[OK] Saved lag outputs:\")\n",
        "print(\"  - output/results_lag_summary_all_predictors.csv\")\n",
        "print(\"  - output/results_lag_profile_all_predictors_long.csv\")\n",
        "print(\"  - output/results_lag_profile_thickness.csv\")\n",
        "\n",
        "print(f\"\\nChangepoint (thickness):   {peak_thick['age']:.2f} Ma ({peak_thick['stage']})\")\n",
        "if np.isfinite(peak_deriv.get(\"age\", np.nan)):\n",
        "    print(f\"Changepoint (composition): {peak_deriv['age']:.2f} Ma ({peak_deriv['stage']})\")\n",
        "else:\n",
        "    print(\"Changepoint (composition): NA (derived_strom_prop not available/insufficient)\")\n",
        "\n",
        "display(lag_summary_df.head(25))\n",
        "\n",
        "\n",
        "print(\"\\n✓ CELL 14 COMPLETE (NO BASAL).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "289ac04c"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 15: TIME-SERIES-ROBUST TESTS (Segmented regression + sampling control)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.regression.linear_model import OLS, GLSAR\n",
        "from statsmodels.stats.sandwich_covariance import cov_hac\n",
        "\n",
        "def _as_1d_numeric(v):\n",
        "    \"\"\"Force Series/DataFrame column to 1-D numeric float array.\"\"\"\n",
        "    if isinstance(v, pd.DataFrame):\n",
        "        v = v.iloc[:, 0]\n",
        "    v = pd.to_numeric(v, errors='coerce')\n",
        "    return v.values.astype(float)\n",
        "\n",
        "def run_segmented_regression(df_in, target, changepoint):\n",
        "    \"\"\"Segmented (ITS) regression with OLS+HAC, WLS+HAC, and GLSAR\"\"\"\n",
        "    needed = ['midpoint_ma', target, 'derived_strom_prop', 'reef_count', 'strom_total_occ']\n",
        "    for c in needed:\n",
        "        if c not in df_in.columns:\n",
        "            print(f\"[WARN] Missing column: {c}\")\n",
        "            return []\n",
        "\n",
        "    v = df_in[needed].copy()\n",
        "\n",
        "    # Exclude no-strom rows\n",
        "    v = v[v['strom_total_occ'].notna() & (v['strom_total_occ'] > 0)].copy()\n",
        "\n",
        "    # Force numeric\n",
        "    v['midpoint_ma'] = pd.to_numeric(v['midpoint_ma'], errors='coerce')\n",
        "    v[target] = pd.to_numeric(v[target], errors='coerce')\n",
        "    v['derived_strom_prop'] = pd.to_numeric(v['derived_strom_prop'], errors='coerce')\n",
        "    v['strom_total_occ'] = pd.to_numeric(v['strom_total_occ'], errors='coerce')\n",
        "\n",
        "    v = v.dropna(subset=['midpoint_ma', target, 'derived_strom_prop', 'strom_total_occ'])\n",
        "    v = v.sort_values('midpoint_ma', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    if len(v) < 10:\n",
        "        print(\"[WARN] Too few observations after filtering:\", len(v))\n",
        "        return []\n",
        "\n",
        "    t = _as_1d_numeric(v['midpoint_ma'])\n",
        "    y = _as_1d_numeric(v[target])\n",
        "\n",
        "    I_post = (t <= changepoint).astype(float)\n",
        "    t_post = np.maximum(0, changepoint - t)\n",
        "\n",
        "    x_derived = _as_1d_numeric(v['derived_strom_prop'])\n",
        "    s_intensity = np.log1p(_as_1d_numeric(v['strom_total_occ']))\n",
        "\n",
        "    X = np.column_stack([np.ones(len(t)), t, I_post, t_post, x_derived, s_intensity])\n",
        "    col_names = ['const', 'time', 'step', 'post_slope', 'derived_prop', 'sampling']\n",
        "\n",
        "    results = []\n",
        "    n = len(y)\n",
        "    maxlags = max(1, int(np.floor(4 * (n/100)**(2/9))))  # Newey–West\n",
        "\n",
        "    # OLS + HAC\n",
        "    try:\n",
        "        m = OLS(y, X).fit()\n",
        "        hac_cov = cov_hac(m, nlags=maxlags)\n",
        "        hac_se = np.sqrt(np.diag(hac_cov))\n",
        "        hac_t = m.params / hac_se\n",
        "        hac_p = 2 * (1 - stats.t.cdf(np.abs(hac_t), df=n - X.shape[1]))\n",
        "        for i, name in enumerate(col_names):\n",
        "            results.append({'Model': 'OLS+HAC', 'Target': target, 'Changepoint_Ma': changepoint,\n",
        "                            'Term': name, 'Coef': m.params[i], 'SE': hac_se[i], 'P': hac_p[i],\n",
        "                            'N': n, 'R2': m.rsquared})\n",
        "    except Exception as e:\n",
        "        print(\"OLS+HAC failed:\", e)\n",
        "\n",
        "    # WLS + HAC\n",
        "    try:\n",
        "        weights = 1.0 / np.log1p(_as_1d_numeric(v['strom_total_occ']) + 1.0)\n",
        "        m = sm.WLS(y, X, weights=weights).fit()\n",
        "        hac_cov = cov_hac(m, nlags=maxlags)\n",
        "        hac_se = np.sqrt(np.diag(hac_cov))\n",
        "        hac_t = m.params / hac_se\n",
        "        hac_p = 2 * (1 - stats.t.cdf(np.abs(hac_t), df=n - X.shape[1]))\n",
        "        for i, name in enumerate(col_names):\n",
        "            results.append({'Model': 'WLS+HAC', 'Target': target, 'Changepoint_Ma': changepoint,\n",
        "                            'Term': name, 'Coef': m.params[i], 'SE': hac_se[i], 'P': hac_p[i],\n",
        "                            'N': n, 'R2': m.rsquared})\n",
        "    except Exception as e:\n",
        "        print(\"WLS+HAC failed:\", e)\n",
        "\n",
        "    # GLSAR(AR1)\n",
        "    try:\n",
        "        ar_m = GLSAR(y, X, rho=1)\n",
        "        ar_fit = ar_m.iterative_fit(maxiter=20)\n",
        "        for i, name in enumerate(col_names):\n",
        "            results.append({'Model': 'GLSAR', 'Target': target, 'Changepoint_Ma': changepoint,\n",
        "                            'Term': name, 'Coef': ar_fit.params[i], 'SE': ar_fit.bse[i],\n",
        "                            'P': ar_fit.pvalues[i], 'N': n, 'R2': ar_fit.rsquared})\n",
        "    except Exception as e:\n",
        "        print(\"GLSAR failed:\", e)\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------\n",
        "# RUN (use changepoints from Cell 13)\n",
        "# -------------------------\n",
        "CP_thick = np.nan\n",
        "CP_comp  = np.nan\n",
        "\n",
        "if 'peak_thick' in globals():\n",
        "    if isinstance(peak_thick, dict) and 'age' in peak_thick:\n",
        "        CP_thick = float(peak_thick['age'])\n",
        "    elif hasattr(peak_thick, '__getitem__') and 'age' in peak_thick:\n",
        "        CP_thick = float(peak_thick['age'])\n",
        "\n",
        "if 'peak_deriv' in globals():\n",
        "    if isinstance(peak_deriv, dict) and 'age' in peak_deriv:\n",
        "        CP_comp = float(peak_deriv['age'])\n",
        "    elif hasattr(peak_deriv, '__getitem__') and 'age' in peak_deriv:\n",
        "        CP_comp = float(peak_deriv['age'])\n",
        "\n",
        "# fallbacks if missing\n",
        "if not np.isfinite(CP_thick): CP_thick = 426.5\n",
        "if not np.isfinite(CP_comp):  CP_comp  = 431.9\n",
        "\n",
        "print(f\"Running segmented regression at thickness CP = {CP_thick:.1f} Ma...\")\n",
        "seg_thick = pd.DataFrame(run_segmented_regression(df, 'thickness_mean', changepoint=CP_thick))\n",
        "\n",
        "print(f\"Running segmented regression at composition CP = {CP_comp:.1f} Ma...\")\n",
        "seg_comp  = pd.DataFrame(run_segmented_regression(df, 'thickness_mean', changepoint=CP_comp))\n",
        "\n",
        "seg_df = pd.concat([seg_thick.assign(CP_type='Thickness'),\n",
        "                    seg_comp.assign(CP_type='Composition')], ignore_index=True)\n",
        "\n",
        "if 'OUTPUT_DIR' in globals() and not seg_df.empty:\n",
        "    seg_df.to_csv(f'{OUTPUT_DIR}/results_segmented_regression.csv', index=False, encoding='utf-8-sig')\n",
        "    print(f\"Saved: {OUTPUT_DIR}/results_segmented_regression.csv\")\n",
        "\n",
        "display(seg_df if not seg_df.empty else pd.DataFrame())\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}