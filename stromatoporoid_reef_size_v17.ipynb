{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeong-HyunLee/stromatoporoid-reef/blob/main/stromatoporoid_reef_size_v17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiczYjViaSKp",
        "outputId": "47ddcfd3-f43e-400f-854e-47350f2ec390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STROMATOPOROID TURNOVER AND REEF MORPHOLOGY ANALYSIS\n",
            "WITH PEARSON AND SPEARMAN CORRELATIONS\n",
            "STAGE-LEVEL AND 5-MYR BIN ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Libraries loaded successfully!\n",
            "Output directory: ./output\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "#@title CELL 1: SETUP AND IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "# Install required packages (uncomment if needed)\n",
        "# !pip install openpyxl geopandas shapely requests\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib.lines import Line2D\n",
        "from scipy import stats\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import resample\n",
        "import statsmodels.api as sm\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up matplotlib for publication-quality vector figures\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.labelsize'] = 11\n",
        "plt.rcParams['axes.titlesize'] = 12\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['pdf.fonttype'] = 42  # TrueType fonts in PDF\n",
        "plt.rcParams['ps.fonttype'] = 42\n",
        "plt.rcParams['svg.fonttype'] = 'none'  # Text as text in SVG\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STROMATOPOROID TURNOVER AND REEF MORPHOLOGY ANALYSIS\")\n",
        "print(\"WITH PEARSON AND SPEARMAN CORRELATIONS\")\n",
        "print(\"STAGE-LEVEL AND 5-MYR BIN ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nLibraries loaded successfully!\")\n",
        "\n",
        "# Output directory\n",
        "import os\n",
        "OUTPUT_DIR = \"./output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vx2CXdOaivD",
        "outputId": "f5a1a4a8-dd28-4aef-8691-2a59a5ed0515"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "GENERATING MACROSTRAT DATA\n",
            "======================================================================\n",
            "Generating Macrostrat data from API...\n",
            "  Fetching data for Ordovician...\n",
            "    Retrieved 2943 geological units\n",
            "  Fetching data for Silurian...\n",
            "    Retrieved 1715 geological units\n",
            "  Fetching data for Devonian...\n",
            "    Retrieved 2793 geological units\n",
            "  Combined dataset: 7451 geological units\n",
            "  ✓ Saved paleozoic_stage_data.csv\n",
            "  ✓ Saved paleozoic_5myr_data.csv\n",
            "✓ Macrostrat data ready\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "#@title CELL 2: GENERATE MACROSTRAT DATA (paleozoic_stage_data.csv, paleozoic_5myr_data.csv)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"GENERATING MACROSTRAT DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if files already exist\n",
        "macrostrat_stage_file = 'paleozoic_stage_data.csv'\n",
        "macrostrat_5myr_file = 'paleozoic_5myr_data.csv'\n",
        "\n",
        "if os.path.exists(macrostrat_stage_file) and os.path.exists(macrostrat_5myr_file):\n",
        "    print(f\"✓ {macrostrat_stage_file} already exists\")\n",
        "    print(f\"✓ {macrostrat_5myr_file} already exists\")\n",
        "    print(\"Skipping Macrostrat data generation...\")\n",
        "else:\n",
        "    print(\"Generating Macrostrat data from API...\")\n",
        "\n",
        "    import requests\n",
        "    try:\n",
        "        import geopandas as gpd\n",
        "    except ImportError:\n",
        "        print(\"Installing geopandas...\")\n",
        "        import subprocess\n",
        "        subprocess.run(['pip', 'install', 'geopandas', '-q'])\n",
        "        import geopandas as gpd\n",
        "\n",
        "    # Define Paleozoic Period Age Ranges\n",
        "    periods = {\n",
        "        \"Ordovician\": {\"start\": 485.4, \"end\": 443.8, \"color\": \"#00a9ce\"},\n",
        "        \"Silurian\": {\"start\": 443.8, \"end\": 419.2, \"color\": \"#b3e1af\"},\n",
        "        \"Devonian\": {\"start\": 419.2, \"end\": 358.9, \"color\": \"#cb8c37\"}\n",
        "    }\n",
        "\n",
        "    # Define stages\n",
        "    ordovician_stages = {\n",
        "        \"Tremadocian\": (478.6, 485.4), \"Floian\": (470.0, 478.6),\n",
        "        \"Dapingian\": (467.3, 470.0), \"Darriwilian\": (458.4, 467.3),\n",
        "        \"Sandbian\": (453.0, 458.4), \"Katian\": (445.2, 453.0),\n",
        "        \"Hirnantian\": (443.8, 445.2)\n",
        "    }\n",
        "    silurian_stages = {\n",
        "        \"Rhuddanian\": (440.8, 443.8), \"Aeronian\": (438.5, 440.8),\n",
        "        \"Telychian\": (433.4, 438.5), \"Sheinwoodian\": (430.5, 433.4),\n",
        "        \"Homerian\": (427.4, 430.5), \"Gorstian\": (425.6, 427.4),\n",
        "        \"Ludfordian\": (423.0, 425.6), \"Pridolian\": (419.2, 423.0)\n",
        "    }\n",
        "    devonian_stages = {\n",
        "        \"Lochkovian\": (410.8, 419.2), \"Pragian\": (407.6, 410.8),\n",
        "        \"Emsian\": (393.3, 407.6), \"Eifelian\": (387.7, 393.3),\n",
        "        \"Givetian\": (382.7, 387.7), \"Frasnian\": (372.2, 382.7),\n",
        "        \"Famennian\": (358.9, 372.2)\n",
        "    }\n",
        "\n",
        "    # Create stages dataframe\n",
        "    stages_data = []\n",
        "    for stage, (end_age, start_age) in ordovician_stages.items():\n",
        "        stages_data.append({\"stage\": stage, \"start_age\": start_age, \"end_age\": end_age,\n",
        "                           \"mid_age\": (start_age + end_age) / 2, \"period\": \"Ordovician\"})\n",
        "    for stage, (end_age, start_age) in silurian_stages.items():\n",
        "        stages_data.append({\"stage\": stage, \"start_age\": start_age, \"end_age\": end_age,\n",
        "                           \"mid_age\": (start_age + end_age) / 2, \"period\": \"Silurian\"})\n",
        "    for stage, (end_age, start_age) in devonian_stages.items():\n",
        "        stages_data.append({\"stage\": stage, \"start_age\": start_age, \"end_age\": end_age,\n",
        "                           \"mid_age\": (start_age + end_age) / 2, \"period\": \"Devonian\"})\n",
        "    stages_df = pd.DataFrame(stages_data)\n",
        "\n",
        "    # Retrieve Macrostrat Data\n",
        "    periods_to_fetch = [\"Ordovician\", \"Silurian\", \"Devonian\"]\n",
        "    all_units_list = []\n",
        "\n",
        "    for period in periods_to_fetch:\n",
        "        url = f\"https://macrostrat.org/api/units?interval_name={period}&format=geojson&response=long\"\n",
        "        print(f\"  Fetching data for {period}...\")\n",
        "        try:\n",
        "            response = requests.get(url, timeout=60)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                features = data.get(\"success\", {}).get(\"data\", [])\n",
        "                if features:\n",
        "                    period_units = gpd.GeoDataFrame.from_features(features)\n",
        "                    print(f\"    Retrieved {len(period_units)} geological units\")\n",
        "                    period_units['source_period'] = period\n",
        "                    all_units_list.append(period_units)\n",
        "        except Exception as e:\n",
        "            print(f\"    Error fetching {period}: {e}\")\n",
        "\n",
        "    if all_units_list:\n",
        "        units = pd.concat(all_units_list, ignore_index=True)\n",
        "        print(f\"  Combined dataset: {len(units)} geological units\")\n",
        "\n",
        "        # Process units\n",
        "        try:\n",
        "            if units.crs is None:\n",
        "                units.set_crs(epsg=4326, inplace=True)\n",
        "            units = units.to_crs(epsg=3857)\n",
        "            if 'col_area' in units.columns:\n",
        "                units['area_km2'] = pd.to_numeric(units['col_area'], errors='coerce')\n",
        "            else:\n",
        "                units['area_km2'] = units.geometry.area / 1e6\n",
        "        except:\n",
        "            units['area_km2'] = 100  # Default\n",
        "\n",
        "        units['t_age'] = pd.to_numeric(units['t_age'], errors='coerce')\n",
        "        units['b_age'] = pd.to_numeric(units['b_age'], errors='coerce')\n",
        "        units['mid_age'] = (units['t_age'] + units['b_age']) / 2.0\n",
        "        units.dropna(subset=['mid_age'], inplace=True)\n",
        "\n",
        "        # Identify carbonates\n",
        "        def check_if_carbonate(lithologies):\n",
        "            if isinstance(lithologies, list):\n",
        "                for lith in lithologies:\n",
        "                    if isinstance(lith, dict) and 'type' in lith and 'carbonate' in str(lith['type']).lower():\n",
        "                        return True\n",
        "            elif isinstance(lithologies, str):\n",
        "                return 'carbonate' in lithologies.lower()\n",
        "            return False\n",
        "\n",
        "        units['is_carbonate'] = units['lith'].apply(check_if_carbonate)\n",
        "        carbonate_units = units[units['is_carbonate']].copy()\n",
        "\n",
        "        # Assign stages\n",
        "        all_stages = {**ordovician_stages, **silurian_stages, **devonian_stages}\n",
        "        def assign_stage(age):\n",
        "            for stage, (end, start) in all_stages.items():\n",
        "                if start >= age >= end:\n",
        "                    return stage\n",
        "            return None\n",
        "\n",
        "        units['stage'] = units['mid_age'].apply(assign_stage)\n",
        "        carbonate_units['stage'] = carbonate_units['mid_age'].apply(assign_stage)\n",
        "\n",
        "        # Aggregate by stage\n",
        "        stage_totals = units.groupby('stage')['area_km2'].sum().reset_index()\n",
        "        stage_totals.rename(columns={'area_km2': 'total_area_km2'}, inplace=True)\n",
        "        stage_carbonates = carbonate_units.groupby('stage')['area_km2'].sum().reset_index()\n",
        "        stage_carbonates.rename(columns={'area_km2': 'carbonate_area_km2'}, inplace=True)\n",
        "\n",
        "        stage_summary = pd.merge(stage_totals, stage_carbonates, on='stage', how='left')\n",
        "        stage_summary['carbonate_area_km2'] = stage_summary['carbonate_area_km2'].fillna(0)\n",
        "        stage_summary['carbonate_percentage'] = (stage_summary['carbonate_area_km2'] / stage_summary['total_area_km2']) * 100\n",
        "\n",
        "        macrostrat_data = pd.merge(stages_df, stage_summary, on='stage', how='left')\n",
        "        macrostrat_data = macrostrat_data.sort_values('start_age', ascending=False).reset_index(drop=True)\n",
        "        macrostrat_data.to_csv(macrostrat_stage_file, index=False)\n",
        "        print(f\"  ✓ Saved {macrostrat_stage_file}\")\n",
        "\n",
        "        # 5 Myr bins\n",
        "        max_age = 490\n",
        "        min_age = 355\n",
        "        manual_bins = np.arange(min_age, max_age + 5, 5)\n",
        "\n",
        "        units['time_bin'] = pd.cut(units['mid_age'], bins=manual_bins, include_lowest=True, right=False)\n",
        "        carbonate_units['time_bin'] = pd.cut(carbonate_units['mid_age'], bins=manual_bins, include_lowest=True, right=False)\n",
        "\n",
        "        macro_all_5myr = units.groupby('time_bin')['area_km2'].sum().reset_index()\n",
        "        macro_all_5myr.rename(columns={'area_km2': 'total_area_km2'}, inplace=True)\n",
        "        macro_carb_5myr = carbonate_units.groupby('time_bin')['area_km2'].sum().reset_index()\n",
        "        macro_carb_5myr.rename(columns={'area_km2': 'carbonate_area_km2'}, inplace=True)\n",
        "\n",
        "        macrostrat_5myr = pd.merge(macro_all_5myr, macro_carb_5myr, on='time_bin', how='left')\n",
        "        macrostrat_5myr['carbonate_area_km2'] = macrostrat_5myr['carbonate_area_km2'].fillna(0)\n",
        "        macrostrat_5myr['carbonate_percentage'] = (macrostrat_5myr['carbonate_area_km2'] / macrostrat_5myr['total_area_km2']) * 100\n",
        "        macrostrat_5myr['bin_mid'] = macrostrat_5myr['time_bin'].apply(lambda x: (x.left + x.right) / 2 if pd.notna(x) else np.nan)\n",
        "        macrostrat_5myr.to_csv(macrostrat_5myr_file, index=False)\n",
        "        print(f\"  ✓ Saved {macrostrat_5myr_file}\")\n",
        "    else:\n",
        "        print(\"  WARNING: Could not fetch Macrostrat data. Creating placeholder files...\")\n",
        "        # Create placeholder files\n",
        "        pd.DataFrame(columns=['stage', 'total_area_km2', 'carbonate_area_km2', 'carbonate_percentage']).to_csv(macrostrat_stage_file, index=False)\n",
        "        pd.DataFrame(columns=['bin_mid', 'total_area_km2', 'carbonate_area_km2', 'carbonate_percentage']).to_csv(macrostrat_5myr_file, index=False)\n",
        "\n",
        "print(\"✓ Macrostrat data ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk40AhaealAV",
        "outputId": "9008611b-bfa3-4c02-c7eb-9c6221b35fbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "GENERATING PARED REEF DATA\n",
            "======================================================================\n",
            "Source file 'PARED_reef_All_numerical.csv' not found.\n",
            "Please upload it now:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7441913b-6aa8-4a8b-a09b-332102ce534f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7441913b-6aa8-4a8b-a09b-332102ce534f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1342828818.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0muploaded_pared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0muploaded_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded_pared\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muploaded_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpared_source_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    170\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "#@title CELL 3: GENERATE PARED REEF DATA (reef stage and 5myr files)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING PARED REEF DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "reef_stage_file = 'ordovician_devonian_reef_data_stage_for_analysis.csv'\n",
        "reef_5myr_file = 'ordovician_devonian_reef_data_5myr_for_analysis.csv'\n",
        "pared_source_file = 'PARED_reef_All_numerical.csv'\n",
        "\n",
        "# Force regeneration to include new variable\n",
        "if os.path.exists(reef_stage_file) and os.path.exists(reef_5myr_file) and False:\n",
        "    print(f\"✓ {reef_stage_file} already exists\")\n",
        "    print(f\"✓ {reef_5myr_file} already exists\")\n",
        "    print(\"Skipping PARED reef data generation...\")\n",
        "else:\n",
        "    # Check if source file exists\n",
        "    if not os.path.exists(pared_source_file):\n",
        "        print(f\"Source file '{pared_source_file}' not found.\")\n",
        "        print(\"Please upload it now:\")\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            uploaded_pared = files.upload()\n",
        "            uploaded_name = list(uploaded_pared.keys())[0]\n",
        "            if uploaded_name != pared_source_file:\n",
        "                os.rename(uploaded_name, pared_source_file)\n",
        "        except ImportError:\n",
        "            raise FileNotFoundError(f\"Please place '{pared_source_file}' in the current directory.\")\n",
        "\n",
        "    print(f\"Processing {pared_source_file}...\")\n",
        "\n",
        "    # Load PARED data\n",
        "    try:\n",
        "        pared_df = pd.read_csv(pared_source_file, encoding='utf-8')\n",
        "    except UnicodeDecodeError:\n",
        "        try:\n",
        "            pared_df = pd.read_csv(pared_source_file, encoding='latin-1')\n",
        "        except:\n",
        "            pared_df = pd.read_csv(pared_source_file, encoding='cp1252')\n",
        "\n",
        "    # --- NEW CODE: Calculate Paired Ratio (Log Difference) ---\n",
        "    # Ensure numeric\n",
        "    pared_df['thickness'] = pd.to_numeric(pared_df['thickness'], errors='coerce')\n",
        "    pared_df['width'] = pd.to_numeric(pared_df['width'], errors='coerce')\n",
        "\n",
        "    # Calculate difference (Log Thickness - Log Width)\n",
        "    # This automatically becomes NaN if either value is missing\n",
        "    pared_df['t_w_log_ratio'] = pared_df['thickness'] - pared_df['width']\n",
        "    # ---------------------------------------------------------\n",
        "\n",
        "    def analyze_pared_data(dataframe, bin_definitions, analysis_type_label):\n",
        "        \"\"\"Calculate statistics using OVERLAP method\"\"\"\n",
        "        results = []\n",
        "        # Added 't_w_log_ratio' to variables\n",
        "        variables = ['thickness', 'width', 'extension', 't_w_log_ratio']\n",
        "\n",
        "        for bin_def in bin_definitions:\n",
        "            s_start = bin_def['start_ma']\n",
        "            s_end = bin_def['end_ma']\n",
        "            name = bin_def['time_identifier']\n",
        "\n",
        "            # OVERLAP LOGIC\n",
        "            mask = (dataframe['min_ma'] < s_end) & (dataframe['max_ma'] > s_start)\n",
        "            subset = dataframe[mask]\n",
        "\n",
        "            row_data = {\n",
        "                'bin_center': (s_start + s_end) / 2.0,\n",
        "                'start_age': s_start,\n",
        "                'end_age': s_end,\n",
        "                'reef_count': len(subset),\n",
        "            }\n",
        "\n",
        "            for var in variables:\n",
        "                data = subset[var].dropna() if var in subset.columns else pd.Series()\n",
        "                if len(data) > 0:\n",
        "                    row_data[f'{var}_mean'] = data.mean()\n",
        "                    row_data[f'{var}_std'] = data.std()\n",
        "                    row_data[f'{var}_stderr'] = data.sem()\n",
        "                    row_data[f'{var}_median'] = data.median()\n",
        "                    row_data[f'{var}_count'] = len(data)\n",
        "                else:\n",
        "                    for suffix in ['mean', 'std', 'stderr', 'median']:\n",
        "                        row_data[f'{var}_{suffix}'] = np.nan\n",
        "                    row_data[f'{var}_count'] = 0\n",
        "\n",
        "            row_data['analysis_type'] = analysis_type_label\n",
        "            row_data['time_identifier'] = name\n",
        "            row_data['name'] = name\n",
        "            row_data['start_ma'] = s_start\n",
        "            row_data['end_ma'] = s_end\n",
        "            row_data['midpoint_ma'] = (s_start + s_end) / 2.0\n",
        "            row_data['duration_myr'] = round(s_end - s_start, 1)\n",
        "\n",
        "            results.append(row_data)\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    # Stage definitions (PRESERVED FROM ORIGINAL)\n",
        "    stages_data = [\n",
        "        {'time_identifier': 'Famennian', 'start_ma': 358.9, 'end_ma': 372.2},\n",
        "        {'time_identifier': 'Frasnian', 'start_ma': 372.2, 'end_ma': 382.7},\n",
        "        {'time_identifier': 'Givetian', 'start_ma': 382.7, 'end_ma': 387.7},\n",
        "        {'time_identifier': 'Eifelian', 'start_ma': 387.7, 'end_ma': 393.3},\n",
        "        {'time_identifier': 'Emsian', 'start_ma': 393.3, 'end_ma': 407.6},\n",
        "        {'time_identifier': 'Pragian', 'start_ma': 407.6, 'end_ma': 410.8},\n",
        "        {'time_identifier': 'Lochkovian', 'start_ma': 410.8, 'end_ma': 419.2},\n",
        "        {'time_identifier': 'Pridoli', 'start_ma': 419.2, 'end_ma': 423.0},\n",
        "        {'time_identifier': 'Ludfordian', 'start_ma': 423.0, 'end_ma': 425.6},\n",
        "        {'time_identifier': 'Gorstian', 'start_ma': 425.6, 'end_ma': 427.4},\n",
        "        {'time_identifier': 'Homerian', 'start_ma': 427.4, 'end_ma': 430.5},\n",
        "        {'time_identifier': 'Sheinwoodian', 'start_ma': 430.5, 'end_ma': 433.4},\n",
        "        {'time_identifier': 'Telychian', 'start_ma': 433.4, 'end_ma': 438.5},\n",
        "        {'time_identifier': 'Aeronian', 'start_ma': 438.5, 'end_ma': 440.8},\n",
        "        {'time_identifier': 'Rhuddanian', 'start_ma': 440.8, 'end_ma': 443.8},\n",
        "        {'time_identifier': 'Hirnantian', 'start_ma': 443.8, 'end_ma': 445.2},\n",
        "        {'time_identifier': 'Katian', 'start_ma': 445.2, 'end_ma': 453.0},\n",
        "        {'time_identifier': 'Sandbian', 'start_ma': 453.0, 'end_ma': 458.4},\n",
        "        {'time_identifier': 'Darriwilian', 'start_ma': 458.4, 'end_ma': 467.3},\n",
        "        {'time_identifier': 'Dapingian', 'start_ma': 467.3, 'end_ma': 470.0},\n",
        "        {'time_identifier': 'Floian', 'start_ma': 470.0, 'end_ma': 477.7},\n",
        "        {'time_identifier': 'Tremadocian', 'start_ma': 477.7, 'end_ma': 485.4},\n",
        "    ]\n",
        "\n",
        "    # 5-Myr bins\n",
        "    bins_5myr = []\n",
        "    for age in range(355, 490, 5):\n",
        "        bins_5myr.append({\n",
        "            'time_identifier': f\"{age}-{age+5} Ma\",\n",
        "            'start_ma': float(age),\n",
        "            'end_ma': float(age + 5)\n",
        "        })\n",
        "\n",
        "    # Generate stage data\n",
        "    df_stages = analyze_pared_data(pared_df, stages_data, 'Geological_Stages')\n",
        "    df_stages.to_csv(reef_stage_file, index=False)\n",
        "    print(f\"  ✓ Generated {reef_stage_file}\")\n",
        "\n",
        "    # Generate 5myr data\n",
        "    df_5myr = analyze_pared_data(pared_df, bins_5myr, '5_myr_bins')\n",
        "    df_5myr.to_csv(reef_5myr_file, index=False)\n",
        "    print(f\"  ✓ Generated {reef_5myr_file}\")\n",
        "\n",
        "print(\"✓ PARED reef data ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "-CR5FkScbhfW"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 4: GENERATE PBDB DIVERSITY AND OCCURRENCE DATA (GENERIC 485.0 BINS)\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP: Create Time References\n",
        "# ==========================================\n",
        "ics_data = \"\"\"stage,series,period,start_ma,end_ma\n",
        "Tremadocian,Lower Ordovician,Ordovician,485.4,477.7\n",
        "Floian,Lower Ordovician,Ordovician,477.7,470\n",
        "Dapingian,Middle Ordovician,Ordovician,470,467.3\n",
        "Darriwilian,Middle Ordovician,Ordovician,467.3,458.4\n",
        "Sandbian,Upper Ordovician,Ordovician,458.4,453\n",
        "Katian,Upper Ordovician,Ordovician,453,445.2\n",
        "Hirnantian,Upper Ordovician,Ordovician,445.2,443.8\n",
        "Rhuddanian,Llandovery,Silurian,443.8,440.8\n",
        "Aeronian,Llandovery,Silurian,440.8,438.5\n",
        "Telychian,Llandovery,Silurian,438.5,433.4\n",
        "Sheinwoodian,Wenlock,Silurian,433.4,430.5\n",
        "Homerian,Wenlock,Silurian,430.5,427.4\n",
        "Gorstian,Ludlow,Silurian,427.4,425.6\n",
        "Ludfordian,Ludlow,Silurian,425.6,423\n",
        "Pridoli,Pridoli,Silurian,423,419.2\n",
        "Lochkovian,Lower Devonian,Devonian,419.2,410.8\n",
        "Pragian,Lower Devonian,Devonian,410.8,407.6\n",
        "Emsian,Lower Devonian,Devonian,407.6,393.3\n",
        "Eifelian,Middle Devonian,Devonian,393.3,387.7\n",
        "Givetian,Middle Devonian,Devonian,387.7,382.7\n",
        "Frasnian,Upper Devonian,Devonian,382.7,372.2\n",
        "Famennian,Upper Devonian,Devonian,372.2,358.9\"\"\"\n",
        "\n",
        "with open(\"ICS_stage_boundaries.csv\", \"w\") as f:\n",
        "    f.write(ics_data)\n",
        "\n",
        "# MODIFIED: Start at 485.0 to align with generic grid\n",
        "def create_5myr_bins(start_ma=485.0, end_ma=358.9, step=5.0):\n",
        "    bins = []\n",
        "    current = start_ma\n",
        "    # Ensure we cover down to the end_ma\n",
        "    while current > end_ma - step:\n",
        "        # Logic: Stop if the bottom of the bin is way below end_ma?\n",
        "        # Typically we want the bin containing end_ma (358.9).\n",
        "        # Bin 360-355 covers 358.9. 355 is < 358.9? No.\n",
        "        # Let's keep standard logic:\n",
        "        if current < 360 and current < end_ma: break # Safety break\n",
        "\n",
        "        top = current\n",
        "        bottom = current - step\n",
        "\n",
        "        # Check if we are going too far (e.g. into Carboniferous)\n",
        "        # We want to stop after covering Famennian (ends 358.9).\n",
        "        # Bin 365-360: Covers 360+.\n",
        "        # Bin 360-355: Covers 358.9.\n",
        "        # Bin 355-350: Unnecessary.\n",
        "\n",
        "        label = f\"{top:.1f}-{bottom:.1f}\"\n",
        "        bins.append({'bin_label': label, 'bin_top': top, 'bin_bottom': bottom})\n",
        "\n",
        "        # Break if this bin covered the end of the period\n",
        "        if bottom < end_ma:\n",
        "             current = bottom\n",
        "             break\n",
        "\n",
        "        current = bottom\n",
        "    return pd.DataFrame(bins)\n",
        "\n",
        "bins_5myr_df = create_5myr_bins()\n",
        "stages_df = pd.read_csv(\"ICS_stage_boundaries.csv\")\n",
        "\n",
        "print(\"Created Generic 5-Myr bins (Aligned to 485.0):\")\n",
        "print(bins_5myr_df.head())\n",
        "print(bins_5myr_df.tail())\n",
        "\n",
        "# ==========================================\n",
        "# 2. FILE CHECK\n",
        "# ==========================================\n",
        "print(\"\\n--- CHECKING FILE SYSTEM ---\")\n",
        "found_files = []\n",
        "for f in os.listdir('.'):\n",
        "    if f.lower().startswith(\"pbdb_data_\") and f.lower().endswith(\".csv\"):\n",
        "        found_files.append(f)\n",
        "\n",
        "if not found_files:\n",
        "    print(\"No 'pbdb_data_*.csv' files found. Please upload your RAW PBDB files now.\")\n",
        "    files.upload()\n",
        "    found_files = [f for f in os.listdir('.') if f.lower().startswith(\"pbdb_data_\") and f.lower().endswith(\".csv\")]\n",
        "\n",
        "print(f\"Found {len(found_files)} files: {found_files}\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def smart_read_pbdb(file_path):\n",
        "    header_row = None\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "            lines = [f.readline() for _ in range(50)]\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"occurrence_no\" in line:\n",
        "                header_row = i\n",
        "                break\n",
        "        if header_row is None:\n",
        "            print(f\"    CRITICAL: Could not find 'occurrence_no' header in {file_path}\")\n",
        "            return None\n",
        "        return pd.read_csv(file_path, header=header_row)\n",
        "    except Exception as e:\n",
        "        print(f\"    Error reading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_genus(accepted_name):\n",
        "    if pd.isna(accepted_name): return None\n",
        "    return str(accepted_name).split(' ')[0]\n",
        "\n",
        "def get_stage_from_age(age, stages_df):\n",
        "    match = stages_df[(stages_df['start_ma'] >= age) & (stages_df['end_ma'] < age)]\n",
        "    if match.empty:\n",
        "        if abs(age - stages_df['end_ma'].min()) < 0.001: return stages_df.iloc[-1]['stage']\n",
        "    if not match.empty: return match.iloc[0]['stage']\n",
        "    return None\n",
        "\n",
        "def get_bin_from_age(age, bins_df):\n",
        "    # Strict containment\n",
        "    match = bins_df[(bins_df['bin_top'] >= age) & (bins_df['bin_bottom'] < age)]\n",
        "\n",
        "    # MODIFIED: Tolerance/Snap for oldest points\n",
        "    # If age is slightly older than the top bin (e.g. 485.4 vs 485.0), snap it to the first bin\n",
        "    if match.empty:\n",
        "        max_top = bins_df['bin_top'].max()\n",
        "        if age > max_top and (age - max_top) < 1.5: # 1.5 Ma tolerance for Tremadocian start\n",
        "             return bins_df.iloc[0]['bin_label']\n",
        "\n",
        "    if not match.empty:\n",
        "        return match.iloc[0]['bin_label']\n",
        "    return None\n",
        "\n",
        "def process_midpoint(df, group_name, reference_df, ref_type=\"stage\"):\n",
        "    if 'accepted_name' not in df.columns:\n",
        "        print(f\"    Error: 'accepted_name' column missing.\")\n",
        "        return None\n",
        "\n",
        "    df['genus_name'] = df['accepted_name'].apply(extract_genus)\n",
        "    df = df.dropna(subset=['genus_name'])\n",
        "\n",
        "    # Midpoint Logic\n",
        "    df['midpoint'] = (df['max_ma'] + df['min_ma']) / 2\n",
        "\n",
        "    if ref_type == \"stage\":\n",
        "        df['assigned_interval'] = df['midpoint'].apply(lambda x: get_stage_from_age(x, reference_df))\n",
        "        merge_col = 'stage'\n",
        "    else:\n",
        "        df['assigned_interval'] = df['midpoint'].apply(lambda x: get_bin_from_age(x, reference_df))\n",
        "        merge_col = 'bin_label'\n",
        "\n",
        "    df = df.dropna(subset=['assigned_interval'])\n",
        "\n",
        "    # Aggregation\n",
        "    genus_counts = df.groupby('assigned_interval')['genus_name'].nunique()\n",
        "    genus_counts.name = f'{group_name}_genus'\n",
        "    occ_counts = df.groupby('assigned_interval')['occurrence_no'].nunique()\n",
        "    occ_counts.name = f'{group_name}_occ'\n",
        "\n",
        "    # Merging\n",
        "    final_df = reference_df.copy()\n",
        "    final_df = final_df.merge(genus_counts, left_on=merge_col, right_index=True, how='left')\n",
        "    final_df = final_df.merge(occ_counts, left_on=merge_col, right_index=True, how='left')\n",
        "\n",
        "    # Cleanup\n",
        "    cols_to_fix = [f'{group_name}_genus', f'{group_name}_occ']\n",
        "    final_df[cols_to_fix] = final_df[cols_to_fix].fillna(0).astype(int)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# ==========================================\n",
        "# 4. MAIN EXECUTION LOOP\n",
        "# ==========================================\n",
        "print(\"\\n--- STARTING ANALYSIS (Generic 485.0 Bins) ---\")\n",
        "\n",
        "for file_path in found_files:\n",
        "    filename = os.path.basename(file_path)\n",
        "    group_name = filename.replace(\"pbdb_data_\", \"\").replace(\".csv\", \"\")\n",
        "    print(f\"\\nAnalyzing {group_name}...\")\n",
        "\n",
        "    df = smart_read_pbdb(file_path)\n",
        "    if df is not None:\n",
        "        try:\n",
        "            # A. Stages\n",
        "            stage_df = process_midpoint(df.copy(), group_name, stages_df, \"stage\")\n",
        "            if stage_df is not None:\n",
        "                out_stage = f\"pbdb_{group_name}_midpoint_stages.csv\"\n",
        "                stage_df.to_csv(out_stage, index=False)\n",
        "                print(f\"  -> Created: {out_stage}\")\n",
        "\n",
        "            # B. 5-Myr Bins\n",
        "            bin_df = process_midpoint(df.copy(), group_name, bins_5myr_df, \"bin\")\n",
        "            if bin_df is not None:\n",
        "                out_bin = f\"pbdb_{group_name}_midpoint_5myr_bins.csv\"\n",
        "                bin_df.to_csv(out_bin, index=False)\n",
        "                print(f\"  -> Created: {out_bin}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"DONE! New 485.0-aligned bin files created.\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "OHVXM-Zdbn9n"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "#@title CELL 5: UPLOAD ENVIRONMENT DATA FILES (Google Colab) - CONDITIONAL\n",
        "# =============================================================================\n",
        "\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Define required files\n",
        "required_files = [\n",
        "    'temperature.csv',\n",
        "    'DO.csv',\n",
        "    'oxygen.csv',\n",
        "    'sealevel.csv',\n",
        "    'd13C_5Myr_Cam-Dev.csv',\n",
        "    'd13C_stage_binned_Cam-Dev.csv'\n",
        "]\n",
        "# Check which files are missing\n",
        "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "\n",
        "if missing_files:\n",
        "    print(\"The following files are missing and need to be uploaded:\")\n",
        "    for i, f in enumerate(missing_files, 1):\n",
        "        print(f\"  {i}. {f}\")\n",
        "    print(\"\\nClick 'Choose Files' and select the missing files...\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    print(f\"\\n✓ Uploaded {len(uploaded)} files:\")\n",
        "    for fn in uploaded.keys():\n",
        "        print(f\"  - {fn}\")\n",
        "else:\n",
        "    print(\"✓ All required files already exist in the folder\")\n",
        "    uploaded = {}\n",
        "    # Load existing files into uploaded dict for compatibility\n",
        "    for f in required_files:\n",
        "        with open(f, 'rb') as file:\n",
        "            uploaded[f] = file.read()\n",
        "\n",
        "print(f\"\\n✓ {len(required_files)} files ready for analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "_J9_ZRvYbrYT"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 6: LOAD AND PROCESS DATA (FROM CONTENT FOLDER)\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Helper: Find file path by keywords in the current directory\n",
        "def get_file_path_robust(keywords, search_dir='.'):\n",
        "    \"\"\"\n",
        "    Finds a filename in the search_dir that contains ALL keywords (case-insensitive).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        files = os.listdir(search_dir)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  ! Error: Directory '{search_dir}' not found.\")\n",
        "        return None\n",
        "\n",
        "    for filename in files:\n",
        "        if not filename.endswith(('.csv', '.xlsx', '.xls')):\n",
        "            continue\n",
        "        # Check if ALL keywords are present in this filename\n",
        "        if all(str(k).lower() in filename.lower() for k in keywords):\n",
        "            return os.path.join(search_dir, filename)\n",
        "    return None\n",
        "\n",
        "def load_and_merge_from_disk(target_list, merge_cols, dataset_name):\n",
        "    \"\"\"\n",
        "    Iterates through a list of target filenames, finds them on disk, and merges them.\n",
        "    \"\"\"\n",
        "    merged_df = None\n",
        "    print(f\"\\nProcessing {dataset_name}...\")\n",
        "\n",
        "    for target in target_list:\n",
        "        # Extract keywords from the target filename\n",
        "        clean_name = target.replace('.csv', '').replace('.xlsx', '')\n",
        "        keywords = [k for k in clean_name.split('_') if k and k.lower() != 'pbdb']\n",
        "\n",
        "        # Search for the file\n",
        "        filepath = get_file_path_robust(keywords)\n",
        "\n",
        "        if filepath:\n",
        "            print(f\"  ✓ Found: {filepath}\")\n",
        "            try:\n",
        "                df = pd.read_csv(filepath)\n",
        "                df.columns = df.columns.str.strip()\n",
        "\n",
        "                if merged_df is None:\n",
        "                    merged_df = df\n",
        "                else:\n",
        "                    merged_df = pd.merge(merged_df, df, on=merge_cols, how='outer')\n",
        "            except Exception as e:\n",
        "                print(f\"  ! Error reading {filepath}: {e}\")\n",
        "        else:\n",
        "            # Retry with minimal keywords (Taxon + Resolution)\n",
        "            # This handles cases where the user filename might differ slightly from the instruction\n",
        "            taxon = next((k for k in keywords if k.lower() not in ['midpoint', 'stages', '5myr', 'bins']), None)\n",
        "            resolution = '5myr' if '5myr' in target.lower() else 'stages'\n",
        "\n",
        "            if taxon:\n",
        "                filepath_retry = get_file_path_robust([taxon, resolution])\n",
        "                if filepath_retry:\n",
        "                    print(f\"  ✓ Found (fallback): {filepath_retry}\")\n",
        "                    try:\n",
        "                        df = pd.read_csv(filepath_retry)\n",
        "                        df.columns = df.columns.str.strip()\n",
        "                        if merged_df is None: merged_df = df\n",
        "                        else: merged_df = pd.merge(merged_df, df, on=merge_cols, how='outer')\n",
        "                    except Exception as e:\n",
        "                        print(f\"  ! Error reading {filepath_retry}: {e}\")\n",
        "                else:\n",
        "                     print(f\"  x Could not find file for: {taxon} ({resolution})\")\n",
        "            else:\n",
        "                 print(f\"  x Could not find file matching: {keywords}\")\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "# Common merge columns\n",
        "stage_merge_cols = ['stage', 'series', 'period', 'start_ma', 'end_ma']\n",
        "bin_merge_cols = ['bin_label', 'bin_top', 'bin_bottom']\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Substitute Stromatoporoid Data (Stage)\n",
        "# =============================================================================\n",
        "strom_stage_targets = [\n",
        "    \"pbdb_Stromatoporida_midpoint_stages.csv\",\n",
        "    \"pbdb_Labechiida_midpoint_stages.csv\",\n",
        "    \"pbdb_Actinostromatida_midpoint_stages.csv\",\n",
        "    \"pbdb_clathrodictyida_midpoint_stages.csv\",\n",
        "    \"pbdb_Syringostromatida_midpoint_stages.csv\",\n",
        "    \"pbdb_Stromatoporellida_midpoint_stages.csv\",\n",
        "    \"pbdb_Amphiporida_midpoint_stages.csv\"\n",
        "]\n",
        "\n",
        "strom_df = load_and_merge_from_disk(strom_stage_targets, stage_merge_cols, \"Stromatoporoid (Stage)\")\n",
        "\n",
        "if strom_df is not None:\n",
        "    strom_df = strom_df.fillna(0)\n",
        "    # Calculate Totals\n",
        "    genus_cols = [c for c in strom_df.columns if c.endswith('_genus')]\n",
        "    occ_cols = [c for c in strom_df.columns if c.endswith('_occ')]\n",
        "    strom_df['Total_genus'] = strom_df[genus_cols].sum(axis=1)\n",
        "    strom_df['Total_occ'] = strom_df[occ_cols].sum(axis=1)\n",
        "\n",
        "    # === FILTER: Remove empty stages ===\n",
        "    dropped_s = len(strom_df[strom_df['Total_occ'] == 0])\n",
        "    strom_df = strom_df[strom_df['Total_occ'] > 0].copy()\n",
        "    if dropped_s > 0:\n",
        "        print(f\"  -> Dropped {dropped_s} Stromatoporoid stage(s) with zero occurrences.\")\n",
        "    # ===================================\n",
        "\n",
        "    print(f\"  -> Merged Stromatoporoid Stages. Rows: {len(strom_df)}\")\n",
        "else:\n",
        "    print(\"  ! Error: Stromatoporoid dataframe is empty.\")\n",
        "\n",
        "# =============================================================================\n",
        "# 2. Substitute Coral Data (Stage)\n",
        "# =============================================================================\n",
        "coral_stage_targets = [\n",
        "    \"pbdb_tabulata_midpoint_stages.csv\",\n",
        "    \"pbdb_Rugosa_midpoint_stages.csv\"\n",
        "]\n",
        "\n",
        "coral_df = load_and_merge_from_disk(coral_stage_targets, stage_merge_cols, \"Coral (Stage)\")\n",
        "\n",
        "if coral_df is not None:\n",
        "    coral_df = coral_df.fillna(0)\n",
        "\n",
        "    # Calculate Totals (Added for filtering)\n",
        "    c_genus_cols = [c for c in coral_df.columns if c.endswith('_genus')]\n",
        "    c_occ_cols = [c for c in coral_df.columns if c.endswith('_occ')]\n",
        "    coral_df['Total_genus'] = coral_df[c_genus_cols].sum(axis=1)\n",
        "    coral_df['Total_occ'] = coral_df[c_occ_cols].sum(axis=1)\n",
        "\n",
        "    # === FILTER: Remove empty stages ===\n",
        "    dropped_c = len(coral_df[coral_df['Total_occ'] == 0])\n",
        "    coral_df = coral_df[coral_df['Total_occ'] > 0].copy()\n",
        "    if dropped_c > 0:\n",
        "        print(f\"  -> Dropped {dropped_c} Coral stage(s) with zero occurrences.\")\n",
        "    # ===================================\n",
        "\n",
        "    print(f\"  -> Merged Coral Stages. Rows: {len(coral_df)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 3. Create New Dataset for 5-Myr Bins\n",
        "# =============================================================================\n",
        "# Stromatoporoids\n",
        "strom_bin_targets = [\n",
        "    \"pbdb_Stromatoporida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Labechiida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Actinostromatida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_clathrodictyida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Syringostromatida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Stromatoporellida_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Amphiporida_midpoint_5myr_bins.csv\"\n",
        "]\n",
        "\n",
        "strom_5myr_df = load_and_merge_from_disk(strom_bin_targets, bin_merge_cols, \"Stromatoporoid (5-Myr)\")\n",
        "\n",
        "if strom_5myr_df is not None:\n",
        "    strom_5myr_df = strom_5myr_df.fillna(0)\n",
        "    genus_cols = [c for c in strom_5myr_df.columns if c.endswith('_genus')]\n",
        "    occ_cols = [c for c in strom_5myr_df.columns if c.endswith('_occ')]\n",
        "    strom_5myr_df['Total_genus'] = strom_5myr_df[genus_cols].sum(axis=1)\n",
        "    strom_5myr_df['Total_occ'] = strom_5myr_df[occ_cols].sum(axis=1)\n",
        "\n",
        "    # === FILTER: Remove empty bins ===\n",
        "    dropped_s5 = len(strom_5myr_df[strom_5myr_df['Total_occ'] == 0])\n",
        "    strom_5myr_df = strom_5myr_df[strom_5myr_df['Total_occ'] > 0].copy()\n",
        "    if dropped_s5 > 0:\n",
        "        print(f\"  -> Dropped {dropped_s5} Stromatoporoid 5-Myr bin(s) with zero occurrences.\")\n",
        "    # =================================\n",
        "\n",
        "    print(f\"  -> Merged Stromatoporoid 5-Myr Bins. Rows: {len(strom_5myr_df)}\")\n",
        "\n",
        "# Corals\n",
        "coral_bin_targets = [\n",
        "    \"pbdb_tabulata_midpoint_5myr_bins.csv\",\n",
        "    \"pbdb_Rugosa_midpoint_5myr_bins.csv\"\n",
        "]\n",
        "coral_5myr_df = load_and_merge_from_disk(coral_bin_targets, bin_merge_cols, \"Coral (5-Myr)\")\n",
        "\n",
        "if coral_5myr_df is not None:\n",
        "    coral_5myr_df = coral_5myr_df.fillna(0)\n",
        "\n",
        "    # Calculate Totals (Added for filtering)\n",
        "    c5_genus_cols = [c for c in coral_5myr_df.columns if c.endswith('_genus')]\n",
        "    c5_occ_cols = [c for c in coral_5myr_df.columns if c.endswith('_occ')]\n",
        "    coral_5myr_df['Total_genus'] = coral_5myr_df[c5_genus_cols].sum(axis=1)\n",
        "    coral_5myr_df['Total_occ'] = coral_5myr_df[c5_occ_cols].sum(axis=1)\n",
        "\n",
        "    # === FILTER: Remove empty bins ===\n",
        "    dropped_c5 = len(coral_5myr_df[coral_5myr_df['Total_occ'] == 0])\n",
        "    coral_5myr_df = coral_5myr_df[coral_5myr_df['Total_occ'] > 0].copy()\n",
        "    if dropped_c5 > 0:\n",
        "        print(f\"  -> Dropped {dropped_c5} Coral 5-Myr bin(s) with zero occurrences.\")\n",
        "    # =================================\n",
        "\n",
        "    print(f\"  -> Merged Coral 5-Myr Bins. Rows: {len(coral_5myr_df)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 4. Load Remaining Contextual Data\n",
        "# =============================================================================\n",
        "print(\"\\nLoading Contextual Data...\")\n",
        "\n",
        "def quick_load(keywords):\n",
        "    path = get_file_path_robust(keywords)\n",
        "    return pd.read_csv(path) if path else pd.DataFrame()\n",
        "\n",
        "reef_df = quick_load([\"reef_data\", \"stage\"])\n",
        "reef_5myr_df = quick_load([\"reef_data\", \"5myr\"])\n",
        "\n",
        "macro_stage = quick_load([\"paleozoic_stage_data\"])\n",
        "if 'stage' in macro_stage.columns: macro_stage['stage'] = macro_stage['stage'].replace('Pridolian', 'Pridoli')\n",
        "\n",
        "macro_5myr = quick_load([\"paleozoic_5myr_data\"])\n",
        "\n",
        "# Environmental\n",
        "env_files = {\n",
        "    'temperature': 'temperature',\n",
        "    'do': 'DO',\n",
        "    'oxygen': 'oxygen',\n",
        "    'sealevel': 'sealevel',\n",
        "    # δ13C files (already binned; DO NOT re-bin/interpolate these)\n",
        "    'd13c_5myr': 'd13C_5Myr_Cam-Dev',\n",
        "    'd13c_stage': 'd13C_stage_binned_Cam-Dev'\n",
        "}\n",
        "env_dfs = {}\n",
        "for var, key in env_files.items():\n",
        "    df = quick_load([key])\n",
        "    if not df.empty: df.columns = df.columns.str.strip().str.replace('\\ufeff', '')\n",
        "    env_dfs[var] = df\n",
        "\n",
        "temp_df = env_dfs.get('temperature', pd.DataFrame())\n",
        "do_df = env_dfs.get('do', pd.DataFrame())\n",
        "oxygen_df = env_dfs.get('oxygen', pd.DataFrame())\n",
        "sealevel_df = env_dfs.get('sealevel', pd.DataFrame())\n",
        "d13c_5myr_df = env_dfs.get('d13c_5myr', pd.DataFrame())\n",
        "d13c_stage_df = env_dfs.get('d13c_stage', pd.DataFrame())\n",
        "print(\"\\n✓ Data loading complete.\")\n",
        "\n",
        "print(\"\\nSaving intermediate merged datasets...\")\n",
        "\n",
        "if 'strom_df' in locals() and strom_df is not None:\n",
        "    strom_df.to_csv(f\"{OUTPUT_DIR}/intermediate_strom_stage.csv\", index=False)\n",
        "if 'coral_df' in locals() and coral_df is not None:\n",
        "    coral_df.to_csv(f\"{OUTPUT_DIR}/intermediate_coral_stage.csv\", index=False)\n",
        "\n",
        "if 'strom_5myr_df' in locals() and strom_5myr_df is not None:\n",
        "    strom_5myr_df.to_csv(f\"{OUTPUT_DIR}/intermediate_strom_5myr.csv\", index=False)\n",
        "if 'coral_5myr_df' in locals() and coral_5myr_df is not None:\n",
        "    coral_5myr_df.to_csv(f\"{OUTPUT_DIR}/intermediate_coral_5myr.csv\", index=False)\n",
        "\n",
        "print(\"✓ Intermediate files saved to ./output\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "ZqhKwRrKbtoq"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 7: DEFINE CONSTANTS AND STAGE INFORMATION\n",
        "# =============================================================================\n",
        "\n",
        "# 1. Stage definitions (ICS 2023)\n",
        "STAGES = {\n",
        "    'Tremadocian': {'start': 485.4, 'end': 477.7, 'mid': 481.55, 'period': 'Ordovician'},\n",
        "    'Floian': {'start': 477.7, 'end': 470.0, 'mid': 473.85, 'period': 'Ordovician'},\n",
        "    'Dapingian': {'start': 470.0, 'end': 467.3, 'mid': 468.65, 'period': 'Ordovician'},\n",
        "    'Darriwilian': {'start': 467.3, 'end': 458.4, 'mid': 462.85, 'period': 'Ordovician'},\n",
        "    'Sandbian': {'start': 458.4, 'end': 453.0, 'mid': 455.7, 'period': 'Ordovician'},\n",
        "    'Katian': {'start': 453.0, 'end': 445.2, 'mid': 449.1, 'period': 'Ordovician'},\n",
        "    'Hirnantian': {'start': 445.2, 'end': 443.8, 'mid': 444.5, 'period': 'Ordovician'},\n",
        "    'Rhuddanian': {'start': 443.8, 'end': 440.8, 'mid': 442.3, 'period': 'Silurian'},\n",
        "    'Aeronian': {'start': 440.8, 'end': 438.5, 'mid': 439.65, 'period': 'Silurian'},\n",
        "    'Telychian': {'start': 438.5, 'end': 433.4, 'mid': 435.95, 'period': 'Silurian'},\n",
        "    'Sheinwoodian': {'start': 433.4, 'end': 430.5, 'mid': 431.95, 'period': 'Silurian'},\n",
        "    'Homerian': {'start': 430.5, 'end': 427.4, 'mid': 428.95, 'period': 'Silurian'},\n",
        "    'Gorstian': {'start': 427.4, 'end': 425.6, 'mid': 426.5, 'period': 'Silurian'},\n",
        "    'Ludfordian': {'start': 425.6, 'end': 423.0, 'mid': 424.3, 'period': 'Silurian'},\n",
        "    'Pridoli': {'start': 423.0, 'end': 419.2, 'mid': 421.1, 'period': 'Silurian'},\n",
        "    'Lochkovian': {'start': 419.2, 'end': 410.8, 'mid': 415.0, 'period': 'Devonian'},\n",
        "    'Pragian': {'start': 410.8, 'end': 407.6, 'mid': 409.2, 'period': 'Devonian'},\n",
        "    'Emsian': {'start': 407.6, 'end': 393.3, 'mid': 400.45, 'period': 'Devonian'},\n",
        "    'Eifelian': {'start': 393.3, 'end': 387.7, 'mid': 390.5, 'period': 'Devonian'},\n",
        "    'Givetian': {'start': 387.7, 'end': 382.7, 'mid': 385.2, 'period': 'Devonian'},\n",
        "    'Frasnian': {'start': 382.7, 'end': 372.2, 'mid': 377.45, 'period': 'Devonian'},\n",
        "    'Famennian': {'start': 372.2, 'end': 358.9, 'mid': 365.55, 'period': 'Devonian'}\n",
        "}\n",
        "STAGE_ORDER = list(STAGES.keys())\n",
        "\n",
        "# 2. Period colors (ICS standard)\n",
        "PERIOD_COLORS = {\n",
        "    'Ordovician': '#009270',\n",
        "    'Silurian': '#B3E1B6',\n",
        "    'Devonian': '#CB8C37'\n",
        "}\n",
        "\n",
        "# 3. Stromatoporoid order colors (phylogenetically informed)\n",
        "STROM_COLORS = {\n",
        "    'Labechiida': '#8B0000',       # Dark red - basal\n",
        "    'Clathrodictyida': '#CD5C5C',  # Indian red - early-diverging\n",
        "    'Actinostromatida': '#FF8C00', # Dark orange - derived\n",
        "    'Stromatoporida': '#FFD700',   # Gold - derived\n",
        "    'Stromatoporellida': '#32CD32',# Lime green - derived\n",
        "    'Syringostromatida': '#4169E1',# Royal blue - derived reef builders\n",
        "    'Amphiporida': '#9370DB'       # Medium purple - derived\n",
        "}\n",
        "STROM_ORDERS = ['Labechiida', 'Clathrodictyida', 'Actinostromatida',\n",
        "                'Stromatoporida', 'Stromatoporellida', 'Syringostromatida', 'Amphiporida']\n",
        "\n",
        "# 4. Coral colors (New definitions for Rugosa/Tabulata)\n",
        "CORAL_COLORS = {\n",
        "    'Rugosa': '#800080',    # Purple\n",
        "    'Tabulata': '#D2691E'   # Chocolate/Orange-Brown\n",
        "}\n",
        "CORAL_GROUPS = ['Rugosa', 'Tabulata']\n",
        "\n",
        "# =============================================================================\n",
        "# DATA NORMALIZATION: Ensure DataFrame columns match capitalized constants\n",
        "# =============================================================================\n",
        "# Some files were lowercase (e.g., 'clathrodictyida'), but constants are TitleCase.\n",
        "# We fix this here to prevent KeyErrors in future plotting cells.\n",
        "\n",
        "def normalize_columns(df, target_orders):\n",
        "    if df is None: return df\n",
        "\n",
        "    # Get current columns\n",
        "    cols = df.columns.tolist()\n",
        "    rename_map = {}\n",
        "\n",
        "    for order in target_orders:\n",
        "        # Check if TitleCase version exists (e.g., 'Clathrodictyida_genus')\n",
        "        title_genus = f\"{order}_genus\"\n",
        "        title_occ = f\"{order}_occ\"\n",
        "\n",
        "        # Check if LowerCase version exists (e.g., 'clathrodictyida_genus')\n",
        "        lower_genus = f\"{order.lower()}_genus\"\n",
        "        lower_occ = f\"{order.lower()}_occ\"\n",
        "\n",
        "        # If TitleCase missing but LowerCase present, map Lower -> Title\n",
        "        if title_genus not in cols and lower_genus in cols:\n",
        "            rename_map[lower_genus] = title_genus\n",
        "        if title_occ not in cols and lower_occ in cols:\n",
        "            rename_map[lower_occ] = title_occ\n",
        "\n",
        "        # Also handle \"tabulata\" (lowercase t)\n",
        "        if order == 'Tabulata' and 'tabulata_genus' in cols:\n",
        "             rename_map['tabulata_genus'] = 'Tabulata_genus'\n",
        "             rename_map['tabulata_occ'] = 'Tabulata_occ'\n",
        "\n",
        "    if rename_map:\n",
        "        print(f\"  Note: Renaming columns to match standard capitalization: {list(rename_map.keys())}\")\n",
        "        df = df.rename(columns=rename_map)\n",
        "    return df\n",
        "\n",
        "# Apply normalization to the datasets loaded in Cell 5\n",
        "if 'strom_df' in locals(): strom_df = normalize_columns(strom_df, STROM_ORDERS)\n",
        "if 'strom_5myr_df' in locals(): strom_5myr_df = normalize_columns(strom_5myr_df, STROM_ORDERS)\n",
        "if 'coral_df' in locals(): coral_df = normalize_columns(coral_df, CORAL_GROUPS)\n",
        "if 'coral_5myr_df' in locals(): coral_5myr_df = normalize_columns(coral_5myr_df, CORAL_GROUPS)\n",
        "\n",
        "print(\"✓ Constants defined and DataFrames normalized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "2mRadY5BbvJm"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 8: RELOAD ENV DATA & INTERPOLATE (ROBUST FIX)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.interpolate import interp1d\n",
        "import os\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. RELOAD ENVIRONMENTAL DATA (To ensure it is not empty)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"Reloading environmental datasets...\")\n",
        "\n",
        "def load_env_file(filenames, target_cols):\n",
        "    \"\"\"Try to load a file from a list of possible names\"\"\"\n",
        "    for fname in filenames:\n",
        "        if os.path.exists(fname):\n",
        "            try:\n",
        "                df = pd.read_csv(fname)\n",
        "                df.columns = df.columns.str.strip() # clean whitespace\n",
        "                print(f\"  ✓ Loaded {fname} ({len(df)} rows)\")\n",
        "                return df\n",
        "            except Exception as e:\n",
        "                print(f\"  ! Error loading {fname}: {e}\")\n",
        "    print(f\"  ! WARNING: Could not find any of {filenames}\")\n",
        "    return pd.DataFrame() # Return empty if not found\n",
        "\n",
        "# Load with specific fallbacks\n",
        "temp_df     = load_env_file(['temperature.csv', 'Temperature.csv'], ['Age', 'SST'])\n",
        "do_df       = load_env_file(['DO.csv', 'do.csv'], ['Age', 'DO'])\n",
        "oxygen_df   = load_env_file(['oxygen.csv', 'Oxygen.csv'], ['Age', 'O2'])\n",
        "sealevel_df = load_env_file(['sealevel.csv', 'Sealevel.csv'], ['Age', 'Eustatic'])\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. STANDARDIZE COLUMNS\n",
        "# -----------------------------------------------------------------------------\n",
        "def standardize_env_columns(df, name, target_age='Age', target_val=None):\n",
        "    if df.empty: return df\n",
        "\n",
        "    # Fix Age column\n",
        "    if target_age not in df.columns:\n",
        "        for candidate in ['age', 'AGE', 'Time', 'Ma', 'time']:\n",
        "            if candidate in df.columns:\n",
        "                df = df.rename(columns={candidate: target_age})\n",
        "                break\n",
        "\n",
        "    # Fix Value column\n",
        "    if target_val and target_val not in df.columns:\n",
        "        # Check case-insensitive match\n",
        "        for col in df.columns:\n",
        "            if col.lower() == target_val.lower():\n",
        "                df = df.rename(columns={col: target_val})\n",
        "                break\n",
        "\n",
        "    return df\n",
        "\n",
        "do_df = standardize_env_columns(do_df, 'Dissolved Oxygen', target_val='DO')\n",
        "temp_df = standardize_env_columns(temp_df, 'Temperature', target_val='SST')\n",
        "oxygen_df = standardize_env_columns(oxygen_df, 'Atmosphere', target_age='Age')\n",
        "sealevel_df = standardize_env_columns(sealevel_df, 'Sea Level', target_val='Eustatic Sea Level')\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. INTERPOLATION\n",
        "# -----------------------------------------------------------------------------\n",
        "def interpolate_to_ages(source_df, age_col, value_col, target_ages):\n",
        "    \"\"\"Interpolate values to target ages, skipping if columns missing.\"\"\"\n",
        "    # Check if empty or missing columns\n",
        "    if source_df.empty or age_col not in source_df.columns or value_col not in source_df.columns:\n",
        "        return np.full_like(target_ages, np.nan)\n",
        "\n",
        "    # Drop NaNs in source\n",
        "    source_df = source_df.dropna(subset=[age_col, value_col])\n",
        "    # Sort by Age (Crucial for interp1d)\n",
        "    source_df = source_df.sort_values(age_col)\n",
        "\n",
        "    if len(source_df) < 2:\n",
        "        return np.full_like(target_ages, np.nan)\n",
        "\n",
        "    # Interpolate\n",
        "    f = interp1d(source_df[age_col], source_df[value_col],\n",
        "                 kind='linear', fill_value='extrapolate', bounds_error=False)\n",
        "    return f(target_ages)\n",
        "\n",
        "# Get Stage Midpoints from Cell 7 constants\n",
        "stage_midpoints = np.array([STAGES[s]['mid'] for s in STAGE_ORDER])\n",
        "\n",
        "print(\"\\nInterpolating environmental proxies to stage midpoints...\")\n",
        "env_data = pd.DataFrame({'stage': STAGE_ORDER, 'midpoint_ma': stage_midpoints})\n",
        "\n",
        "env_data['temperature']     = interpolate_to_ages(temp_df, 'Age', 'SST', stage_midpoints)\n",
        "env_data['dissolved_O2']    = interpolate_to_ages(do_df, 'Age', 'DO', stage_midpoints)\n",
        "env_data['atm_O2']          = interpolate_to_ages(oxygen_df, 'Age', 'O2', stage_midpoints)\n",
        "env_data['atm_CO2']         = interpolate_to_ages(oxygen_df, 'Age', 'CO2', stage_midpoints)\n",
        "env_data['sea_level']       = interpolate_to_ages(sealevel_df, 'Age', 'Eustatic Sea Level', stage_midpoints)\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# MERGE δ13C (STAGE-BINNED) — prefer Stage-name join; fallback to age-nearest\n",
        "# Uses attached d13C_stage_binned_Cam-Dev.csv WITHOUT re-binning/conversion.\n",
        "# -----------------------------------------------------------------------------\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "def _norm_stage(s):\n",
        "    if pd.isna(s):\n",
        "        return np.nan\n",
        "    s = str(s).strip().lower()\n",
        "    # remove spaces/punct to survive minor naming differences\n",
        "    return re.sub(r'[^a-z0-9]+', '', s)\n",
        "\n",
        "try:\n",
        "    # Load attached stage-binned δ13C table\n",
        "    cand = [\n",
        "        Path(\"d13C_stage_binned_Cam-Dev.csv\"),\n",
        "        Path(\"/mnt/data/d13C_stage_binned_Cam-Dev.csv\")\n",
        "    ]\n",
        "    f = next((p for p in cand if p.exists()), None)\n",
        "    if f is None:\n",
        "        raise FileNotFoundError(\"d13C_stage_binned_Cam-Dev.csv not found in working dir or /mnt/data\")\n",
        "\n",
        "    d13_stage = pd.read_csv(f)\n",
        "    d13_stage.columns = d13_stage.columns.str.strip().str.replace('\\ufeff', '')\n",
        "\n",
        "    # Required columns in attached file\n",
        "    if \"Stage\" not in d13_stage.columns or \"Mid_Ma\" not in d13_stage.columns or \"d13C_mean\" not in d13_stage.columns:\n",
        "        raise ValueError(\"δ13C stage file must have columns: Stage, Mid_Ma, d13C_mean\")\n",
        "\n",
        "    # Clean numeric + stage keys\n",
        "    d13_stage[\"Mid_Ma\"] = pd.to_numeric(d13_stage[\"Mid_Ma\"], errors=\"coerce\")\n",
        "    d13_stage[\"d13C_mean\"] = pd.to_numeric(d13_stage[\"d13C_mean\"], errors=\"coerce\")\n",
        "    d13_stage[\"stage_key\"] = d13_stage[\"Stage\"].apply(_norm_stage)\n",
        "\n",
        "    env_data[\"midpoint_ma\"] = pd.to_numeric(env_data[\"midpoint_ma\"], errors=\"coerce\")\n",
        "\n",
        "    # --- 1) Stage-name merge (best) ---\n",
        "    if \"stage\" in env_data.columns:\n",
        "        env_data[\"stage_key\"] = env_data[\"stage\"].apply(_norm_stage)\n",
        "\n",
        "        _m1 = env_data.merge(\n",
        "            d13_stage[[\"stage_key\", \"d13C_mean\", \"Mid_Ma\"]],\n",
        "            on=\"stage_key\",\n",
        "            how=\"left\",\n",
        "            suffixes=(\"\", \"_d13\")\n",
        "        )\n",
        "\n",
        "        # Keep δ13C as a single downstream name\n",
        "        _m1[\"d13C\"] = _m1[\"d13C_mean\"]\n",
        "\n",
        "        # --- 2) Fallback: for unmatched stages, fill by age-nearest ---\n",
        "        missing = _m1[\"d13C\"].isna() & _m1[\"midpoint_ma\"].notna()\n",
        "        if missing.any():\n",
        "            _left = _m1.loc[missing, [\"midpoint_ma\"]].copy().sort_values(\"midpoint_ma\")\n",
        "            _right = d13_stage[[\"Mid_Ma\", \"d13C_mean\"]].dropna().sort_values(\"Mid_Ma\")\n",
        "\n",
        "            _fill = pd.merge_asof(\n",
        "                _left,\n",
        "                _right,\n",
        "                left_on=\"midpoint_ma\",\n",
        "                right_on=\"Mid_Ma\",\n",
        "                direction=\"nearest\",\n",
        "                tolerance=2.0  # stage midpoints can differ by >0.25; allow reasonable slack\n",
        "            )\n",
        "            _m1.loc[missing, \"d13C\"] = _fill[\"d13C_mean\"].values\n",
        "\n",
        "        # Cleanup\n",
        "        env_data = _m1.drop(columns=[c for c in [\"d13C_mean\", \"Mid_Ma\", \"stage_key\"] if c in _m1.columns])\n",
        "        env_data = env_data.sort_values(\"midpoint_ma\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    else:\n",
        "        # If env_data has no stage column, do age-nearest only (more tolerant)\n",
        "        _left = env_data.dropna(subset=[\"midpoint_ma\"]).sort_values(\"midpoint_ma\")\n",
        "        _right = d13_stage[[\"Mid_Ma\", \"d13C_mean\"]].dropna().sort_values(\"Mid_Ma\")\n",
        "        _m = pd.merge_asof(_left, _right, left_on=\"midpoint_ma\", right_on=\"Mid_Ma\", direction=\"nearest\", tolerance=2.0)\n",
        "        env_data = _m.drop(columns=[\"Mid_Ma\"]).rename(columns={\"d13C_mean\": \"d13C\"})\n",
        "        env_data = env_data.sort_values(\"midpoint_ma\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    n_valid = int(env_data[\"d13C\"].notna().sum()) if \"d13C\" in env_data.columns else 0\n",
        "    print(f\"  -> δ13C (stage-binned) merged: {n_valid}/{len(env_data)} values\")\n",
        "\n",
        "except Exception as e:\n",
        "    env_data[\"d13C\"] = np.nan\n",
        "    print(\"  ! Warning: δ13C stage merge failed:\", e)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. INTERPOLATE TO 5-MYR BINS\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\nInterpolating environmental proxies to 5-Myr bin midpoints...\")\n",
        "\n",
        "# Define bins (Logic from previous step)\n",
        "if 'reef_5myr_df' in locals() and not reef_5myr_df.empty:\n",
        "    bin_midpoints_5myr = reef_5myr_df['midpoint_ma'].values\n",
        "    bin_ids = reef_5myr_df['time_identifier']\n",
        "elif 'strom_5myr_df' in locals() and not strom_5myr_df.empty:\n",
        "    # Recalculate if column missing\n",
        "    if 'midpoint_ma' not in strom_5myr_df.columns:\n",
        "        strom_5myr_df['midpoint_ma'] = (strom_5myr_df['bin_top'] + strom_5myr_df['bin_bottom']) / 2\n",
        "    bin_midpoints_5myr = strom_5myr_df['midpoint_ma'].values\n",
        "    bin_ids = strom_5myr_df['bin_label']\n",
        "else:\n",
        "    # Fallback\n",
        "    bin_midpoints_5myr = np.arange(482.5, 359, -5)\n",
        "    bin_ids = [f\"{x}\" for x in bin_midpoints_5myr]\n",
        "\n",
        "env_data_5myr = pd.DataFrame({'bin_id': bin_ids, 'midpoint_ma': bin_midpoints_5myr})\n",
        "\n",
        "env_data_5myr['temperature']     = interpolate_to_ages(temp_df, 'Age', 'SST', bin_midpoints_5myr)\n",
        "env_data_5myr['dissolved_O2']    = interpolate_to_ages(do_df, 'Age', 'DO', bin_midpoints_5myr)\n",
        "env_data_5myr['atm_O2']          = interpolate_to_ages(oxygen_df, 'Age', 'O2', bin_midpoints_5myr)\n",
        "env_data_5myr['atm_CO2']         = interpolate_to_ages(oxygen_df, 'Age', 'CO2', bin_midpoints_5myr)\n",
        "env_data_5myr['sea_level']       = interpolate_to_ages(sealevel_df, 'Age', 'Eustatic Sea Level', bin_midpoints_5myr)\n",
        "# -----------------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4b. MERGE δ13C (5-MYR BINNED) WITHOUT INTERPOLATION / RE-BINNING\n",
        "#   Float bin midpoints often differ by tiny rounding; if exact merge yields\n",
        "#   few/zero matches, fall back to merge_asof (requires ascending sort).\n",
        "# -----------------------------------------------------------------------------\n",
        "try:\n",
        "    if 'd13c_5myr_df' in globals() and isinstance(d13c_5myr_df, pd.DataFrame) and (not d13c_5myr_df.empty):\n",
        "        _d13_5 = d13c_5myr_df.copy()\n",
        "    else:\n",
        "        cand = [\n",
        "            'd13C_5Myr_Cam-Dev.csv',\n",
        "            './output/d13C_5Myr_Cam-Dev.csv',\n",
        "            'd13C_5Myr.csv',\n",
        "            './output/d13C_5Myr.csv'\n",
        "        ]\n",
        "        _d13_5 = None\n",
        "        for p in cand:\n",
        "            if os.path.exists(p):\n",
        "                _d13_5 = pd.read_csv(p)\n",
        "                break\n",
        "        if _d13_5 is None:\n",
        "            raise FileNotFoundError(\"No 5-Myr δ13C file found (tried: \" + \", \".join(cand) + \").\")\n",
        "\n",
        "    _d13_5.columns = _d13_5.columns.str.strip().str.replace('\\ufeff', '')\n",
        "\n",
        "    # Standardize columns\n",
        "    if 'age_Ma' in _d13_5.columns and 'midpoint_ma' not in _d13_5.columns:\n",
        "        _d13_5 = _d13_5.rename(columns={'age_Ma': 'midpoint_ma'})\n",
        "    if 'Mid_Ma' in _d13_5.columns and 'midpoint_ma' not in _d13_5.columns:\n",
        "        _d13_5 = _d13_5.rename(columns={'Mid_Ma': 'midpoint_ma'})\n",
        "    if 'd13Ccarb_permille' in _d13_5.columns and 'd13C' not in _d13_5.columns:\n",
        "        _d13_5 = _d13_5.rename(columns={'d13Ccarb_permille': 'd13C'})\n",
        "    if 'd13C_mean' in _d13_5.columns and 'd13C' not in _d13_5.columns:\n",
        "        _d13_5 = _d13_5.rename(columns={'d13C_mean': 'd13C'})\n",
        "\n",
        "    _d13_5['midpoint_ma'] = pd.to_numeric(_d13_5['midpoint_ma'], errors='coerce')\n",
        "    _d13_5['d13C'] = pd.to_numeric(_d13_5['d13C'], errors='coerce')\n",
        "    _d13_5 = _d13_5.dropna(subset=['midpoint_ma', 'd13C']).copy()\n",
        "\n",
        "    # --- Attempt exact merge after rounding to reduce floating mismatch\n",
        "    env_data_5myr['midpoint_ma'] = pd.to_numeric(env_data_5myr['midpoint_ma'], errors='coerce')\n",
        "    _left = env_data_5myr.copy()\n",
        "    _left['midpoint_ma_round'] = _left['midpoint_ma'].round(3)\n",
        "    _right = _d13_5[['midpoint_ma', 'd13C']].copy()\n",
        "    _right['midpoint_ma_round'] = _right['midpoint_ma'].round(3)\n",
        "\n",
        "    env_data_5myr = _left.merge(_right[['midpoint_ma_round', 'd13C']], on='midpoint_ma_round', how='left')\n",
        "    env_data_5myr = env_data_5myr.drop(columns=['midpoint_ma_round'])\n",
        "\n",
        "    n_valid = env_data_5myr['d13C'].notna().sum()\n",
        "\n",
        "    # --- Fallback: nearest-age join if exact merge fails\n",
        "    if n_valid == 0 and len(_d13_5) > 0:\n",
        "        _left_sorted = env_data_5myr.drop(columns=['d13C'], errors='ignore').copy()\n",
        "        _left_sorted = _left_sorted.dropna(subset=['midpoint_ma']).sort_values('midpoint_ma', ascending=True).reset_index(drop=True)\n",
        "\n",
        "        _right_sorted = _d13_5[['midpoint_ma', 'd13C']].sort_values('midpoint_ma', ascending=True).reset_index(drop=True)\n",
        "\n",
        "        _m = pd.merge_asof(\n",
        "            _left_sorted,\n",
        "            _right_sorted,\n",
        "            on='midpoint_ma',\n",
        "            direction='nearest',\n",
        "            tolerance=2.6  # ~half of 5-Myr bin width\n",
        "        )\n",
        "        env_data_5myr = _m.sort_values('midpoint_ma', ascending=False).reset_index(drop=True)\n",
        "        n_valid = env_data_5myr['d13C'].notna().sum()\n",
        "\n",
        "    print(f\"  -> δ13C (5-Myr binned) merged: {n_valid}/{len(env_data_5myr)} values\")\n",
        "\n",
        "except Exception as e:\n",
        "    env_data_5myr['d13C'] = np.nan\n",
        "    print(\"  ! Warning: δ13C 5-Myr merge failed:\", e)\n",
        "\n",
        "print(\"✓ Environmental proxies interpolated (Stage & 5-Myr).\")\n",
        "\n",
        "# =============================================================================\n",
        "# [ADDED] SAVE INTERPOLATED ENV DATA\n",
        "# =============================================================================\n",
        "if 'env_data' in locals() and not env_data.empty:\n",
        "    env_data.to_csv(f\"{OUTPUT_DIR}/intermediate_env_data_stage.csv\", index=False)\n",
        "\n",
        "if 'env_data_5myr' in locals() and not env_data_5myr.empty:\n",
        "    env_data_5myr.to_csv(f\"{OUTPUT_DIR}/intermediate_env_data_5myr.csv\", index=False)\n",
        "\n",
        "print(f\"✓ Interpolated environmental data saved to {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "t2BtkuBab2Kp"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 9: CREATE MASTER DATASET (STAGES AND 5-MYR BINS)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. BUILD MASTER STAGE DATASET\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"Building Master Stage Dataset...\")\n",
        "data = []\n",
        "\n",
        "for stage, info in STAGES.items():\n",
        "    row = {\n",
        "        'stage': stage,\n",
        "        'midpoint_ma': info['mid'],\n",
        "        'start_ma': info['start'],\n",
        "        'end_ma': info['end'],\n",
        "        'period': info['period']\n",
        "    }\n",
        "\n",
        "    # A. Reef data (if available)\n",
        "    if 'reef_df' in locals() and reef_df is not None and not reef_df.empty:\n",
        "        reef_row = reef_df[reef_df['name'] == stage]\n",
        "        if len(reef_row) > 0:\n",
        "            for col in [\n",
        "                'thickness_mean', 'thickness_std', 'thickness_stderr', 'thickness_median',\n",
        "                'thickness_min', 'thickness_max', 'thickness_q25', 'thickness_q75', 'thickness_count',\n",
        "                'width_mean', 'width_std', 'width_median', 'width_min', 'width_max',\n",
        "                'reef_count'\n",
        "            ]:\n",
        "                if col in reef_row.columns:\n",
        "                    row[col] = reef_row[col].values[0]\n",
        "\n",
        "    # B. Stromatoporoid data (from strom_df)\n",
        "    if 'strom_df' in locals() and strom_df is not None and not strom_df.empty:\n",
        "        # robust stage matching\n",
        "        s_stage = strom_df['stage'].astype(str).str.lower()\n",
        "        strom_row = strom_df[s_stage == str(stage).lower()]\n",
        "        if len(strom_row) > 0:\n",
        "            for order in STROM_ORDERS:\n",
        "                col_occ = f'{order}_occ'\n",
        "                col_gen = f'{order}_genus'\n",
        "                if col_occ in strom_row.columns:\n",
        "                    row[col_occ] = strom_row[col_occ].values[0]\n",
        "                if col_gen in strom_row.columns:\n",
        "                    row[col_gen] = strom_row[col_gen].values[0]\n",
        "\n",
        "            if 'Total_occ' in strom_row.columns:\n",
        "                row['strom_total_occ'] = strom_row['Total_occ'].values[0]\n",
        "            if 'Total_genus' in strom_row.columns:\n",
        "                row['strom_total_gen'] = strom_row['Total_genus'].values[0]\n",
        "\n",
        "    # C. Coral data (from coral_df)\n",
        "    if 'coral_df' in locals() and coral_df is not None and not coral_df.empty:\n",
        "        c_stage = coral_df['stage'].astype(str).str.lower()\n",
        "        coral_row = coral_df[c_stage == str(stage).lower()]\n",
        "        if len(coral_row) > 0:\n",
        "            # Rugosa\n",
        "            if 'Rugosa_genus' in coral_row.columns: row['rugose_div'] = coral_row['Rugosa_genus'].values[0]\n",
        "            if 'Rugosa_occ' in coral_row.columns:   row['rugose_occ'] = coral_row['Rugosa_occ'].values[0]\n",
        "\n",
        "            # Tabulata\n",
        "            if 'Tabulata_genus' in coral_row.columns: row['tabulate_div'] = coral_row['Tabulata_genus'].values[0]\n",
        "            if 'Tabulata_occ' in coral_row.columns:   row['tabulate_occ'] = coral_row['Tabulata_occ'].values[0]\n",
        "\n",
        "    # D. Macrostrat data\n",
        "    if 'macro_stage' in locals() and macro_stage is not None and not macro_stage.empty:\n",
        "        macro_row = macro_stage[macro_stage['stage'] == stage]\n",
        "        if len(macro_row) > 0:\n",
        "            row['total_area_km2'] = macro_row['total_area_km2'].values[0]\n",
        "            row['carbonate_area_km2'] = macro_row['carbonate_area_km2'].values[0]\n",
        "            row['carbonate_percentage'] = macro_row['carbonate_percentage'].values[0]\n",
        "\n",
        "    # E. Environmental proxies\n",
        "    if 'env_data' in locals() and env_data is not None and not env_data.empty:\n",
        "        env_row = env_data[env_data['stage'] == stage]\n",
        "        if len(env_row) > 0:\n",
        "            row['temperature'] = env_row['temperature'].values[0]\n",
        "            row['dissolved_O2'] = env_row['dissolved_O2'].values[0]\n",
        "            row['atm_O2'] = env_row['atm_O2'].values[0]\n",
        "            row['atm_CO2'] = env_row['atm_CO2'].values[0]\n",
        "            row['sea_level'] = env_row['sea_level'].values[0]\n",
        "            if 'd13C' in env_row.columns:\n",
        "                row['d13C'] = env_row['d13C'].values[0]\n",
        "\n",
        "    data.append(row)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate Stromatoporoid Proportions\n",
        "if 'strom_total_occ' in df.columns:\n",
        "    for order in STROM_ORDERS:\n",
        "        col_occ = f'{order}_occ'\n",
        "        if col_occ in df.columns:\n",
        "            df[f'{order}_prop'] = np.where(\n",
        "                df['strom_total_occ'].fillna(0) > 0,\n",
        "                df[col_occ].fillna(0) / df['strom_total_occ'].fillna(0),\n",
        "                0.0\n",
        "            )\n",
        "\n",
        "    # Derived vs Basal\n",
        "    derived_orders = ['Actinostromatida', 'Stromatoporida', 'Stromatoporellida',\n",
        "                      'Syringostromatida', 'Amphiporida']\n",
        "    basal_orders = ['Labechiida', 'Clathrodictyida']\n",
        "\n",
        "    # Occurrences\n",
        "    df['derived_strom_occ'] = sum(df[f'{o}_occ'].fillna(0) for o in derived_orders if f'{o}_occ' in df.columns)\n",
        "    df['basal_strom_occ'] = sum(df[f'{o}_occ'].fillna(0) for o in basal_orders if f'{o}_occ' in df.columns)\n",
        "\n",
        "    # Diversity\n",
        "    df['derived_strom_div'] = sum(df[f'{o}_genus'].fillna(0) for o in derived_orders if f'{o}_genus' in df.columns)\n",
        "    df['basal_strom_div'] = sum(df[f'{o}_genus'].fillna(0) for o in basal_orders if f'{o}_genus' in df.columns)\n",
        "\n",
        "    # Proportions\n",
        "    df['derived_strom_prop'] = np.where(df['strom_total_occ'].fillna(0) > 0, df['derived_strom_occ'] / df['strom_total_occ'].fillna(0), 0.0)\n",
        "    df['basal_strom_prop'] = np.where(df['strom_total_occ'].fillna(0) > 0, df['basal_strom_occ'] / df['strom_total_occ'].fillna(0), 0.0)\n",
        "\n",
        "# [ADDED] Fill all missing NUMERIC values with 0 (keep text columns untouched)\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[num_cols] = df[num_cols].fillna(0)\n",
        "\n",
        "# Sort\n",
        "df = df.sort_values('midpoint_ma', ascending=False).reset_index(drop=True)\n",
        "print(f\"✓ Master dataset (STAGES) created: {len(df)} stages, {len(df.columns)} variables\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. BUILD MASTER 5-MYR DATASET\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING 5-MYR BIN MASTER DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Determine the primary source for 5-Myr bins\n",
        "# We prefer reef data if available, otherwise we use the biological data\n",
        "if 'reef_5myr_df' in locals() and reef_5myr_df is not None and not reef_5myr_df.empty:\n",
        "    primary_bins = reef_5myr_df\n",
        "    print(\"Using Reef Data as primary 5-Myr bin source.\")\n",
        "elif 'strom_5myr_df' in locals() and strom_5myr_df is not None and not strom_5myr_df.empty:\n",
        "    primary_bins = strom_5myr_df\n",
        "    # Add midpoint if missing\n",
        "    if 'midpoint_ma' not in primary_bins.columns and {'bin_top', 'bin_bottom'}.issubset(primary_bins.columns):\n",
        "        primary_bins = primary_bins.copy()\n",
        "        primary_bins['midpoint_ma'] = (primary_bins['bin_top'] + primary_bins['bin_bottom']) / 2\n",
        "    print(\"Using Stromatoporoid Data as primary 5-Myr bin source.\")\n",
        "else:\n",
        "    # Fallback to env_data_5myr if bio data missing\n",
        "    primary_bins = env_data_5myr\n",
        "    print(\"Using Environmental Data as primary 5-Myr bin source.\")\n",
        "\n",
        "data_5myr = []\n",
        "\n",
        "# Iterate through the chosen primary bins\n",
        "for idx, row_ref in primary_bins.iterrows():\n",
        "    # Identify bin\n",
        "    if 'time_identifier' in row_ref:\n",
        "        bin_id = row_ref['time_identifier']\n",
        "    elif 'bin_label' in row_ref:\n",
        "        bin_id = row_ref['bin_label']\n",
        "    elif 'bin_id' in row_ref:\n",
        "        bin_id = row_ref['bin_id']\n",
        "    else:\n",
        "        bin_id = idx  # fallback\n",
        "\n",
        "    midpoint = row_ref['midpoint_ma']\n",
        "\n",
        "    row = {\n",
        "        'bin_id': bin_id,\n",
        "        'midpoint_ma': midpoint\n",
        "    }\n",
        "\n",
        "    # A. Reef Data (if available)\n",
        "    if 'reef_5myr_df' in locals() and reef_5myr_df is not None and not reef_5myr_df.empty:\n",
        "        # Find matching reef row (if not already iterating it)\n",
        "        if primary_bins is not reef_5myr_df:\n",
        "            reef_match = reef_5myr_df[abs(reef_5myr_df['midpoint_ma'] - midpoint) < 0.1]\n",
        "            row_ref_for_reef = reef_match.iloc[0] if len(reef_match) > 0 else pd.Series(dtype='float64')\n",
        "        else:\n",
        "            row_ref_for_reef = row_ref\n",
        "\n",
        "        for col in [\n",
        "            'thickness_mean', 'thickness_std', 'thickness_stderr', 'thickness_median',\n",
        "            'thickness_count', 'width_mean', 'width_std', 'reef_count'\n",
        "        ]:\n",
        "            if col in row_ref_for_reef.index:\n",
        "                row[col] = row_ref_for_reef[col]\n",
        "\n",
        "    # B. Stromatoporoid Data (5-Myr)\n",
        "    if 'strom_5myr_df' in locals() and strom_5myr_df is not None and not strom_5myr_df.empty:\n",
        "        if {'bin_top', 'bin_bottom'}.issubset(strom_5myr_df.columns):\n",
        "            strom_mid = (strom_5myr_df['bin_top'] + strom_5myr_df['bin_bottom']) / 2\n",
        "            strom_match = strom_5myr_df[abs(strom_mid - midpoint) < 0.1]\n",
        "            if len(strom_match) > 0:\n",
        "                s_row = strom_match.iloc[0]\n",
        "                for order in STROM_ORDERS:\n",
        "                    if f'{order}_occ' in s_row.index: row[f'{order}_occ'] = s_row[f'{order}_occ']\n",
        "                    if f'{order}_genus' in s_row.index: row[f'{order}_genus'] = s_row[f'{order}_genus']\n",
        "                if 'Total_occ' in s_row.index: row['strom_total_occ'] = s_row['Total_occ']\n",
        "                if 'Total_genus' in s_row.index: row['strom_total_gen'] = s_row['Total_genus']\n",
        "\n",
        "    # C. Coral Data (5-Myr)\n",
        "    if 'coral_5myr_df' in locals() and coral_5myr_df is not None and not coral_5myr_df.empty:\n",
        "        if {'bin_top', 'bin_bottom'}.issubset(coral_5myr_df.columns):\n",
        "            coral_mid = (coral_5myr_df['bin_top'] + coral_5myr_df['bin_bottom']) / 2\n",
        "            coral_match = coral_5myr_df[abs(coral_mid - midpoint) < 0.1]\n",
        "            if len(coral_match) > 0:\n",
        "                c_row = coral_match.iloc[0]\n",
        "                if 'Rugosa_occ' in c_row.index: row['rugose_occ'] = c_row['Rugosa_occ']\n",
        "                if 'Rugosa_genus' in c_row.index: row['rugose_div'] = c_row['Rugosa_genus']\n",
        "                if 'Tabulata_occ' in c_row.index: row['tabulate_occ'] = c_row['Tabulata_occ']\n",
        "                if 'Tabulata_genus' in c_row.index: row['tabulate_div'] = c_row['Tabulata_genus']\n",
        "\n",
        "    # D. Macrostrat Data\n",
        "    if 'macro_5myr' in locals() and macro_5myr is not None and not macro_5myr.empty and 'bin_mid' in macro_5myr.columns:\n",
        "        macro_match = macro_5myr[abs(macro_5myr['bin_mid'] - midpoint) < 2.5]\n",
        "        if len(macro_match) > 0:\n",
        "            row['total_area_km2'] = macro_match['total_area_km2'].values[0]\n",
        "            row['carbonate_area_km2'] = macro_match['carbonate_area_km2'].values[0]\n",
        "            row['carbonate_percentage'] = macro_match['carbonate_percentage'].values[0]\n",
        "\n",
        "    # E. Environmental Proxies\n",
        "    if 'env_data_5myr' in locals() and env_data_5myr is not None and not env_data_5myr.empty:\n",
        "        env_match = env_data_5myr[abs(env_data_5myr['midpoint_ma'] - midpoint) < 0.1]\n",
        "        if len(env_match) > 0:\n",
        "            env_val = env_match.iloc[0]\n",
        "            row['temperature'] = env_val['temperature']\n",
        "            row['dissolved_O2'] = env_val['dissolved_O2']\n",
        "            row['atm_O2'] = env_val['atm_O2']\n",
        "            row['atm_CO2'] = env_val['atm_CO2']\n",
        "            row['sea_level'] = env_val['sea_level']\n",
        "            if 'd13C' in env_val.index:\n",
        "                row['d13C'] = env_val['d13C']\n",
        "\n",
        "    data_5myr.append(row)\n",
        "\n",
        "df_5myr = pd.DataFrame(data_5myr)\n",
        "\n",
        "# [ADDED] Fill all missing NUMERIC values with 0 (keep text columns untouched)\n",
        "num_cols_5 = df_5myr.select_dtypes(include=[np.number]).columns\n",
        "df_5myr[num_cols_5] = df_5myr[num_cols_5].fillna(0)\n",
        "\n",
        "df_5myr = df_5myr.sort_values('midpoint_ma', ascending=False).reset_index(drop=True)\n",
        "print(f\"✓ Master dataset (5-MYR BINS) created: {len(df_5myr)} bins, {len(df_5myr.columns)} variables\")\n",
        "\n",
        "# =============================================================================\n",
        "# [ADDED] SAVE MASTER DATASETS\n",
        "# =============================================================================\n",
        "# Save the master datasets immediately after creation\n",
        "if 'df' in locals() and df is not None and not df.empty:\n",
        "    df.to_csv(f\"{OUTPUT_DIR}/MASTER_dataset_stage.csv\", index=False)\n",
        "    print(f\"✓ Saved: {OUTPUT_DIR}/MASTER_dataset_stage.csv\")\n",
        "\n",
        "if 'df_5myr' in locals() and df_5myr is not None and not df_5myr.empty:\n",
        "    df_5myr.to_csv(f\"{OUTPUT_DIR}/MASTER_dataset_5myr.csv\", index=False)\n",
        "    print(f\"✓ Saved: {OUTPUT_DIR}/MASTER_dataset_5myr.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jpj1BjT5d8eO"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 10: CLR COMPOSITIONAL TRANSFORMATION (WITH BASAL/DERIVED GROUPS)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import gmean\n",
        "import scipy.stats as stats\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"PRE-PROCESSING: CLR TRANSFORMATION\")\n",
        "print(\"Transforming closed compositional data (proportions) to open log-ratios\")\n",
        "print(\"=\"*90)\n",
        "# Global list for CLR results\n",
        "global_clr_results = []\n",
        "def calculate_missing_props(input_df):\n",
        "    \"\"\"\n",
        "    Helper: If _prop columns are missing, calculate them from _occ columns.\n",
        "    \"\"\"\n",
        "    df_copy = input_df.copy()\n",
        "    orders = ['Labechiida', 'Clathrodictyida', 'Actinostromatida',\n",
        "              'Stromatoporida', 'Stromatoporellida',\n",
        "              'Syringostromatida', 'Amphiporida']\n",
        "    # Check if we have occurrence columns\n",
        "    occ_cols = [f\"{o}_occ\" for o in orders if f\"{o}_occ\" in df_copy.columns]\n",
        "    if not occ_cols:\n",
        "        return df_copy\n",
        "    # Recalculate Total\n",
        "    if 'strom_total_occ' not in df_copy.columns:\n",
        "        df_copy['strom_total_occ'] = df_copy[occ_cols].sum(axis=1)\n",
        "    # Calculate Proportions\n",
        "    for o in orders:\n",
        "        occ_col = f\"{o}_occ\"\n",
        "        prop_col = f\"{o}_prop\"\n",
        "        if occ_col in df_copy.columns:\n",
        "            total = df_copy['strom_total_occ'].replace(0, np.nan)\n",
        "            df_copy[prop_col] = df_copy[occ_col] / total\n",
        "            df_copy[prop_col] = df_copy[prop_col].fillna(0)\n",
        "    return df_copy\n",
        "def apply_clr(input_df, label):\n",
        "    \"\"\"Apply CLR transformation and calculate group-level metrics.\"\"\"\n",
        "\n",
        "    dataset = input_df.copy()\n",
        "\n",
        "    # 1. AUTO-REPAIR: Ensure Proportion Columns Exist\n",
        "    dataset = calculate_missing_props(dataset)\n",
        "    # 2. Identify Proportion Columns\n",
        "    prop_cols = ['Labechiida_prop', 'Clathrodictyida_prop', 'Actinostromatida_prop',\n",
        "                 'Stromatoporida_prop', 'Stromatoporellida_prop',\n",
        "                 'Syringostromatida_prop', 'Amphiporida_prop']\n",
        "    available_cols = [c for c in prop_cols if c in dataset.columns]\n",
        "    # Define group membership\n",
        "    basal_orders = ['Labechiida_prop', 'Clathrodictyida_prop']\n",
        "    derived_orders = ['Actinostromatida_prop', 'Stromatoporida_prop',\n",
        "                      'Stromatoporellida_prop', 'Syringostromatida_prop',\n",
        "                      'Amphiporida_prop']\n",
        "    if len(available_cols) < 2:\n",
        "        print(f\"  {label}: ! Skipped. Found only {len(available_cols)} proportion columns.\")\n",
        "        return dataset\n",
        "    # 3. Extract and Handle Zeros\n",
        "    comp_data = dataset[available_cols].replace(0, 1e-5)\n",
        "    # 4. Geometric Mean per Row\n",
        "    gmeans = gmean(comp_data, axis=1)\n",
        "    # 5. Transform: ln(x / gmean)\n",
        "    clr_data = np.log(comp_data.div(gmeans, axis=0))\n",
        "\n",
        "    # Use CLR_ prefix (avoiding duplicates)\n",
        "    new_col_names = []\n",
        "    for c in available_cols:\n",
        "        new_name = f\"CLR_{c}\"\n",
        "        # Drop existing column if present to avoid duplicates\n",
        "        if new_name in dataset.columns:\n",
        "            dataset = dataset.drop(columns=[new_name])\n",
        "        new_col_names.append(new_name)\n",
        "    clr_data.columns = new_col_names\n",
        "    # 6. Create 'Derived vs Basal' Log-Ratio\n",
        "    basal_in = [c for c in basal_orders if c in dataset.columns]\n",
        "    derived_in = [c for c in derived_orders if c in dataset.columns]\n",
        "    if basal_in and derived_in:\n",
        "        b_sum = dataset[basal_in].sum(axis=1).replace(0, 1e-5)\n",
        "        d_sum = dataset[derived_in].sum(axis=1).replace(0, 1e-5)\n",
        "        dataset['log_derived_basal_ratio'] = np.log(d_sum / b_sum)\n",
        "        print(f\"  {label}: Created 'log_derived_basal_ratio'\")\n",
        "    # 7. Merge CLR columns back to dataset\n",
        "    dataset = pd.concat([dataset.reset_index(drop=True), clr_data.reset_index(drop=True)], axis=1)\n",
        "    print(f\"  {label}: Generated {len(clr_data.columns)} CLR variables.\")\n",
        "\n",
        "    # 8. Calculate GROUP-LEVEL CLR means\n",
        "    clr_basal_cols = [f\"CLR_{c}\" for c in basal_in]\n",
        "    clr_derived_cols = [f\"CLR_{c}\" for c in derived_in]\n",
        "\n",
        "    if clr_basal_cols:\n",
        "        dataset['CLR_basal_mean'] = dataset[clr_basal_cols].mean(axis=1)\n",
        "    if clr_derived_cols:\n",
        "        dataset['CLR_derived_mean'] = dataset[clr_derived_cols].mean(axis=1)\n",
        "\n",
        "    return dataset\n",
        "# Apply to both master datasets\n",
        "df = apply_clr(df, \"Stage-Level\")\n",
        "df_5myr = apply_clr(df_5myr, \"5-Myr Bins\")\n",
        "# =============================================================================\n",
        "# CLR CORRELATION ANALYSIS (Individual Taxa + Basal/Derived Groups)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"-\"*90)\n",
        "print(\"CLR CORRELATION ANALYSIS\")\n",
        "print(\"-\"*90)\n",
        "prop_cols = ['Labechiida_prop', 'Clathrodictyida_prop', 'Actinostromatida_prop',\n",
        "             'Stromatoporida_prop', 'Stromatoporellida_prop',\n",
        "             'Syringostromatida_prop', 'Amphiporida_prop']\n",
        "targets = ['thickness_mean', 'width_mean']\n",
        "def safe_spearman(x, y):\n",
        "    \"\"\"Calculate Spearman correlation safely, returning scalars.\"\"\"\n",
        "    try:\n",
        "        # Ensure 1D numpy arrays\n",
        "        x_arr = np.array(x).flatten()\n",
        "        y_arr = np.array(y).flatten()\n",
        "\n",
        "        # Remove NaN pairs\n",
        "        mask = ~(np.isnan(x_arr) | np.isnan(y_arr))\n",
        "        x_clean = x_arr[mask]\n",
        "        y_clean = y_arr[mask]\n",
        "\n",
        "        if len(x_clean) < 3:\n",
        "            return np.nan, np.nan\n",
        "\n",
        "        result = stats.spearmanr(x_clean, y_clean)\n",
        "        # Handle both old and new scipy return types\n",
        "        if hasattr(result, 'correlation'):\n",
        "            return float(result.correlation), float(result.pvalue)\n",
        "        else:\n",
        "            return float(result[0]), float(result[1])\n",
        "    except:\n",
        "        return np.nan, np.nan\n",
        "for label, data in [('Stage-Level', df), ('5-Myr Bins', df_5myr)]:\n",
        "    print(f\"\\n--- {label} ---\")\n",
        "    print(f\"  {'Taxon/Group':<28s} | {'Orig ρ':>8s} | {'CLR ρ':>8s} | {'Diff':>7s} | {'CLR p':>10s}\")\n",
        "    print(\"  \" + \"-\"*75)\n",
        "\n",
        "    # A. INDIVIDUAL TAXA\n",
        "    for prop in prop_cols:\n",
        "        clr_col = f\"CLR_{prop}\"\n",
        "        if prop not in data.columns or clr_col not in data.columns:\n",
        "            continue\n",
        "\n",
        "        for target in targets:\n",
        "            if target not in data.columns:\n",
        "                continue\n",
        "\n",
        "            # Get data as 1D arrays\n",
        "            x_orig = data[prop].values\n",
        "            x_clr = data[clr_col].values\n",
        "            y = data[target].values\n",
        "\n",
        "            # Calculate correlations\n",
        "            r_orig, p_orig = safe_spearman(x_orig, y)\n",
        "            r_clr, p_clr = safe_spearman(x_clr, y)\n",
        "\n",
        "            if np.isnan(r_orig) or np.isnan(r_clr):\n",
        "                continue\n",
        "\n",
        "            diff = r_clr - r_orig\n",
        "\n",
        "            # Interpretation\n",
        "            if abs(diff) < 0.1:\n",
        "                interp = \"Stable\"\n",
        "            elif diff < -0.2:\n",
        "                interp = \"SUPPRESSED\"\n",
        "            elif diff > 0.2:\n",
        "                interp = \"INFLATED\"\n",
        "            else:\n",
        "                interp = \"Moderate\"\n",
        "\n",
        "            global_clr_results.append({\n",
        "                'Dataset': label, 'Level': 'Individual',\n",
        "                'Predictor': prop.replace('_prop', ''),\n",
        "                'Target': target, 'Original_Rho': r_orig, 'CLR_Rho': r_clr,\n",
        "                'Difference': diff, 'CLR_P': p_clr, 'Interpretation': interp\n",
        "            })\n",
        "\n",
        "            if target == 'thickness_mean':\n",
        "                sig = \"*\" if p_clr < 0.05 else \"\"\n",
        "                print(f\"  {prop.replace('_prop',''):<28s} | {r_orig:>+8.3f} | {r_clr:>+8.3f} | {diff:>+7.3f} | {p_clr:>10.4f} {sig}\")\n",
        "\n",
        "    # B. BASAL vs DERIVED GROUPS\n",
        "    print(\"  \" + \"-\"*75)\n",
        "    print(\"  GROUP COMPARISONS:\")\n",
        "\n",
        "    group_vars = [\n",
        "        ('CLR_basal_mean', 'Basal (Labech+Clathr) CLR'),\n",
        "        ('CLR_derived_mean', 'Derived (5 taxa) CLR'),\n",
        "        ('log_derived_basal_ratio', 'Log(Derived/Basal) Ratio')\n",
        "    ]\n",
        "\n",
        "    for col, name in group_vars:\n",
        "        if col not in data.columns:\n",
        "            continue\n",
        "\n",
        "        for target in targets:\n",
        "            if target not in data.columns:\n",
        "                continue\n",
        "\n",
        "            x = data[col].values\n",
        "            y = data[target].values\n",
        "\n",
        "            r, p = safe_spearman(x, y)\n",
        "\n",
        "            if np.isnan(r):\n",
        "                continue\n",
        "\n",
        "            global_clr_results.append({\n",
        "                'Dataset': label, 'Level': 'Group',\n",
        "                'Predictor': name, 'Target': target,\n",
        "                'Original_Rho': np.nan, 'CLR_Rho': r,\n",
        "                'Difference': np.nan, 'CLR_P': p, 'Interpretation': 'Group-level'\n",
        "            })\n",
        "\n",
        "            if target == 'thickness_mean':\n",
        "                sig = \"*\" if p < 0.05 else \"\"\n",
        "                print(f\"  {name:<28s} |      N/A | {r:>+8.3f} |     N/A | {p:>10.4f} {sig}\")\n",
        "# =============================================================================\n",
        "# SAVE CLR RESULTS\n",
        "# =============================================================================\n",
        "pd.DataFrame(global_clr_results).to_csv(f\"{OUTPUT_DIR}/results_clr.csv\", index=False)\n",
        "print(f\"\\nSaved: {OUTPUT_DIR}/results_clr.csv ({len(global_clr_results)} rows)\")\n",
        "# Update predictor lists\n",
        "if 'all_test_vars' in locals():\n",
        "    new_vars = [\n",
        "        ('log_derived_basal_ratio', 'Log(Derived/Basal)'),\n",
        "        ('CLR_basal_mean', 'CLR Basal Mean'),\n",
        "        ('CLR_derived_mean', 'CLR Derived Mean')\n",
        "    ]\n",
        "    for var in new_vars:\n",
        "        if not any(v[0] == var[0] for v in all_test_vars):\n",
        "            all_test_vars.append(var)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OhFo2l0de0YP"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 11: CORRELATION FUNCTIONS - SPEARMAN AND PEARSON\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "\"\"\"\n",
        "CORRELATION ANALYSIS METHODS\n",
        "============================\n",
        "This cell defines functions for both Spearman and Pearson correlations.\n",
        "\n",
        "SPEARMAN'S RHO (ρ):\n",
        "- Non-parametric rank correlation\n",
        "- Measures monotonic relationships (not just linear)\n",
        "- Robust to outliers and non-normal distributions\n",
        "- Appropriate for ordinal data or data with outliers\n",
        "- Reference: Spearman, C. (1904). American Journal of Psychology, 15(1), 72-101.\n",
        "\n",
        "PEARSON'S r:\n",
        "- Parametric correlation coefficient\n",
        "- Measures linear relationships specifically\n",
        "- Assumes normally distributed variables\n",
        "- More powerful when assumptions are met\n",
        "- Reference: Pearson, K. (1895). Philosophical Transactions of the Royal Society A, 186, 343-414.\n",
        "\n",
        "For geological time series:\n",
        "- Spearman is often preferred due to non-normal distributions\n",
        "- Pearson provides information on linear relationships\n",
        "- Presenting both allows comparison and assessment of relationship type\n",
        "\"\"\"\n",
        "\n",
        "def calc_spearman(data, v1, v2):\n",
        "    \"\"\"\n",
        "    Calculate Spearman rank correlation with significance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : DataFrame\n",
        "        Data containing the variables\n",
        "    v1, v2 : str\n",
        "        Column names to correlate\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    rho : float\n",
        "        Spearman correlation coefficient\n",
        "    p : float\n",
        "        Two-tailed p-value\n",
        "    n : int\n",
        "        Number of valid pairs\n",
        "    \"\"\"\n",
        "    # Check if columns exist\n",
        "    if v1 not in data.columns or v2 not in data.columns:\n",
        "        return np.nan, np.nan, 0\n",
        "\n",
        "    valid = data[[v1, v2]].dropna()\n",
        "\n",
        "    if len(valid) >= 5:\n",
        "        # Spearmanr returns a Result object or tuple depending on version, generic unpacking is safer\n",
        "        result = stats.spearmanr(valid[v1], valid[v2])\n",
        "        # Handle cases where result might be a struct or tuple\n",
        "        try:\n",
        "            r, p = result.correlation, result.pvalue\n",
        "        except AttributeError:\n",
        "            r, p = result[0], result[1]\n",
        "\n",
        "        return r, p, len(valid)\n",
        "\n",
        "    return np.nan, np.nan, 0\n",
        "\n",
        "def calc_pearson(data, v1, v2):\n",
        "    \"\"\"\n",
        "    Calculate Pearson correlation with significance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : DataFrame\n",
        "        Data containing the variables\n",
        "    v1, v2 : str\n",
        "        Column names to correlate\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    r : float\n",
        "        Pearson correlation coefficient\n",
        "    p : float\n",
        "        Two-tailed p-value\n",
        "    n : int\n",
        "        Number of valid pairs\n",
        "    \"\"\"\n",
        "    if v1 not in data.columns or v2 not in data.columns:\n",
        "        return np.nan, np.nan, 0\n",
        "\n",
        "    valid = data[[v1, v2]].dropna()\n",
        "\n",
        "    if len(valid) >= 5:\n",
        "        r, p = stats.pearsonr(valid[v1], valid[v2])\n",
        "        return r, p, len(valid)\n",
        "\n",
        "    return np.nan, np.nan, 0\n",
        "\n",
        "def calc_both_correlations(data, v1, v2):\n",
        "    \"\"\"\n",
        "    Calculate both Spearman and Pearson correlations\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict with both correlation results\n",
        "    \"\"\"\n",
        "    spearman_r, spearman_p, n = calc_spearman(data, v1, v2)\n",
        "    pearson_r, pearson_p, _ = calc_pearson(data, v1, v2)\n",
        "\n",
        "    return {\n",
        "        'spearman_rho': spearman_r,\n",
        "        'spearman_p': spearman_p,\n",
        "        'pearson_r': pearson_r,\n",
        "        'pearson_p': pearson_p,\n",
        "        'n': n\n",
        "    }\n",
        "\n",
        "def get_significance_stars(p):\n",
        "    \"\"\"Convert p-value to significance stars\"\"\"\n",
        "    if p is None or np.isnan(p):\n",
        "        return ''\n",
        "    if p < 0.001:\n",
        "        return '***'\n",
        "    elif p < 0.01:\n",
        "        return '**'\n",
        "    elif p < 0.05:\n",
        "        return '*'\n",
        "    else:\n",
        "        return 'ns'\n",
        "\n",
        "print(\"✓ Correlation functions defined (Spearman and Pearson)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WIsgnZGe2v-"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 12: COMPREHENSIVE CORRELATION ANALYSIS (MULTI-METRIC & 5-MYR BIOLOGY)\n",
        "#   - NO T/W ratio computed or compared\n",
        "#   - ORIGINAL behavior restored:\n",
        "#       * strom presence view: strom_total_occ > 0\n",
        "#       * coral presence view: coral_total_occ > 0\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import re\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\"COMPREHENSIVE CORRELATION ANALYSIS\")\n",
        "print(\"Metrics: Thickness, Width\")\n",
        "print(\"Scopes:  Stage-Level AND 5-Myr Bins\")\n",
        "print(\"Notes:   NO T/W ratio; ORIGINAL presence filters restored\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# SETTINGS\n",
        "# -----------------------------------------------------------------------------#\n",
        "MIN_N = 5\n",
        "VERBOSE_SKIPS = False\n",
        "TREAT_CORAL_ZEROS_AS_MISSING = False  # keep as in your original logic\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# 0. SAFETY: define STROM_ORDERS if missing\n",
        "# -----------------------------------------------------------------------------#\n",
        "if 'STROM_ORDERS' not in globals():\n",
        "    STROM_ORDERS = [\n",
        "        'Labechiida', 'Clathrodictyida', 'Actinostromatida',\n",
        "        'Stromatoporida', 'Stromatoporellida', 'Syringostromatida', 'Amphiporida'\n",
        "    ]\n",
        "    print(\"[WARN] STROM_ORDERS not found in globals(); using default list.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# 1. PRE-PROCESSING: CALCULATE DERIVED VARIABLES (NO T/W)\n",
        "# -----------------------------------------------------------------------------#\n",
        "print(\"Pre-processing data...\")\n",
        "\n",
        "def calculate_derived_metrics(dataset, label):\n",
        "    if dataset is None or dataset.empty:\n",
        "        print(f\"  - [{label}] Empty dataset; skip derived metrics.\")\n",
        "        return dataset\n",
        "\n",
        "    dataset = dataset.copy()\n",
        "\n",
        "    # Remove T/W ratio if it exists from older runs (do NOT compute it)\n",
        "    if 'thickness_width_ratio' in dataset.columns:\n",
        "        dataset = dataset.drop(columns=['thickness_width_ratio'])\n",
        "\n",
        "    # Stromatoporoid Proportions & Groups\n",
        "    strom_cols = [c for c in dataset.columns if c.endswith('_occ') and c != 'strom_total_occ']\n",
        "    if strom_cols:\n",
        "        if 'strom_total_occ' not in dataset.columns:\n",
        "            dataset['strom_total_occ'] = dataset[strom_cols].sum(axis=1)\n",
        "\n",
        "        dataset['strom_total_occ'] = pd.to_numeric(dataset['strom_total_occ'], errors='coerce')\n",
        "\n",
        "        for order in STROM_ORDERS:\n",
        "            col_occ = f'{order}_occ'\n",
        "            if col_occ in dataset.columns:\n",
        "                dataset[col_occ] = pd.to_numeric(dataset[col_occ], errors='coerce')\n",
        "                dataset[f'{order}_prop'] = np.where(\n",
        "                    dataset['strom_total_occ'] > 0,\n",
        "                    dataset[col_occ] / dataset['strom_total_occ'],\n",
        "                    np.nan\n",
        "                )\n",
        "\n",
        "        derived_orders = ['Actinostromatida', 'Stromatoporida', 'Stromatoporellida',\n",
        "                          'Syringostromatida', 'Amphiporida']\n",
        "        basal_orders   = ['Labechiida', 'Clathrodictyida']\n",
        "\n",
        "        derived_occ_cols = [f'{o}_occ' for o in derived_orders if f'{o}_occ' in dataset.columns]\n",
        "        basal_occ_cols   = [f'{o}_occ' for o in basal_orders   if f'{o}_occ' in dataset.columns]\n",
        "\n",
        "        dataset['derived_strom_occ'] = (\n",
        "            dataset[derived_occ_cols].apply(pd.to_numeric, errors='coerce').sum(axis=1, min_count=1)\n",
        "            if derived_occ_cols else np.nan\n",
        "        )\n",
        "        dataset['basal_strom_occ'] = (\n",
        "            dataset[basal_occ_cols].apply(pd.to_numeric, errors='coerce').sum(axis=1, min_count=1)\n",
        "            if basal_occ_cols else np.nan\n",
        "        )\n",
        "\n",
        "        derived_div_cols = [f'{o}_genus' for o in derived_orders if f'{o}_genus' in dataset.columns]\n",
        "        basal_div_cols   = [f'{o}_genus' for o in basal_orders   if f'{o}_genus' in dataset.columns]\n",
        "\n",
        "        dataset['derived_strom_div'] = (\n",
        "            dataset[derived_div_cols].apply(pd.to_numeric, errors='coerce').sum(axis=1, min_count=1)\n",
        "            if derived_div_cols else np.nan\n",
        "        )\n",
        "        dataset['basal_strom_div'] = (\n",
        "            dataset[basal_div_cols].apply(pd.to_numeric, errors='coerce').sum(axis=1, min_count=1)\n",
        "            if basal_div_cols else np.nan\n",
        "        )\n",
        "\n",
        "        dataset['derived_strom_prop'] = np.where(\n",
        "            dataset['strom_total_occ'] > 0,\n",
        "            dataset['derived_strom_occ'] / dataset['strom_total_occ'],\n",
        "            np.nan\n",
        "        )\n",
        "        dataset['basal_strom_prop'] = np.where(\n",
        "            dataset['strom_total_occ'] > 0,\n",
        "            dataset['basal_strom_occ'] / dataset['strom_total_occ'],\n",
        "            np.nan\n",
        "        )\n",
        "\n",
        "        print(f\"  [OK] [{label}] Calculated strom proportions and groupings\")\n",
        "    else:\n",
        "        print(f\"  - [{label}] No strom occurrence columns found.\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "df = calculate_derived_metrics(df, \"Stage\")\n",
        "df_5myr = calculate_derived_metrics(df_5myr, \"5-Myr\")\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# 1A. STANDARDIZE ATMOSPHERIC O2/CO2 COLUMN NAMES (Stage + 5-Myr)\n",
        "# -----------------------------------------------------------------------------#\n",
        "def standardize_atm_cols(dataset, label):\n",
        "    if dataset is None or dataset.empty:\n",
        "        return dataset\n",
        "    dataset = dataset.copy()\n",
        "\n",
        "    def _find_col(candidates, regex_pat=None):\n",
        "        for c in candidates:\n",
        "            if c in dataset.columns:\n",
        "                return c\n",
        "        if regex_pat is not None:\n",
        "            hits = [c for c in dataset.columns if re.search(regex_pat, c, flags=re.IGNORECASE)]\n",
        "            return hits[0] if hits else None\n",
        "        return None\n",
        "\n",
        "    if 'atmospheric_O2' not in dataset.columns:\n",
        "        o2_src = _find_col(\n",
        "            candidates=['atm_O2','atmospheric_o2','oxygen','pO2','PO2','O2_atm','oxygen_atm'],\n",
        "            regex_pat=r'(atm|atmos).*o2|po2'\n",
        "        )\n",
        "        if o2_src is not None:\n",
        "            dataset['atmospheric_O2'] = pd.to_numeric(dataset[o2_src], errors='coerce')\n",
        "            print(f\"  [OK] [{label}] atmospheric_O2 <- {o2_src}\")\n",
        "        else:\n",
        "            print(f\"  [WARN] [{label}] No atmospheric O2 column found (atmospheric_O2 missing).\")\n",
        "\n",
        "    if 'atmospheric_CO2' not in dataset.columns:\n",
        "        co2_src = _find_col(\n",
        "            candidates=['atm_CO2','atmospheric_co2','co2','pCO2','PCO2','CO2_atm','co2_atm'],\n",
        "            regex_pat=r'(atm|atmos).*co2|pco2'\n",
        "        )\n",
        "        if co2_src is not None:\n",
        "            dataset['atmospheric_CO2'] = pd.to_numeric(dataset[co2_src], errors='coerce')\n",
        "            print(f\"  [OK] [{label}] atmospheric_CO2 <- {co2_src}\")\n",
        "        else:\n",
        "            print(f\"  [WARN] [{label}] No atmospheric CO2 column found (atmospheric_CO2 missing).\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "df = standardize_atm_cols(df, \"Stage\")\n",
        "df_5myr = standardize_atm_cols(df_5myr, \"5-Myr\")\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# 1B. CORAL TOTALS (used for original coral presence filtering)\n",
        "# -----------------------------------------------------------------------------#\n",
        "def add_coral_totals(dataset, label):\n",
        "    if dataset is None or dataset.empty:\n",
        "        return dataset\n",
        "    dataset = dataset.copy()\n",
        "\n",
        "    for c in ['rugose_occ','tabulate_occ','rugose_div','tabulate_div']:\n",
        "        if c in dataset.columns:\n",
        "            dataset[c] = pd.to_numeric(dataset[c], errors='coerce')\n",
        "\n",
        "    occ_cols = [c for c in ['rugose_occ','tabulate_occ'] if c in dataset.columns]\n",
        "    div_cols = [c for c in ['rugose_div','tabulate_div'] if c in dataset.columns]\n",
        "\n",
        "    if occ_cols:\n",
        "        dataset['coral_total_occ'] = dataset[occ_cols].sum(axis=1, min_count=1)\n",
        "    if div_cols:\n",
        "        dataset['coral_total_div'] = dataset[div_cols].sum(axis=1, min_count=1)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "df = add_coral_totals(df, \"Stage\")\n",
        "df_5myr = add_coral_totals(df_5myr, \"5-Myr\")\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# 1C. BUILD DATASET VIEWS (ORIGINAL PRESENCE FILTERS RESTORED)\n",
        "# -----------------------------------------------------------------------------#\n",
        "df_all = df\n",
        "df_5myr_all = df_5myr\n",
        "\n",
        "def _filter_has_strom(dataset):\n",
        "    if dataset is None or dataset.empty or 'strom_total_occ' not in dataset.columns:\n",
        "        return dataset\n",
        "    s = pd.to_numeric(dataset['strom_total_occ'], errors='coerce')\n",
        "    return dataset.loc[s > 0].copy()\n",
        "\n",
        "def _filter_has_coral(dataset):\n",
        "    if dataset is None or dataset.empty:\n",
        "        return dataset\n",
        "    if 'coral_total_occ' in dataset.columns:\n",
        "        c = pd.to_numeric(dataset['coral_total_occ'], errors='coerce')\n",
        "        return dataset.loc[c > 0].copy()\n",
        "    # fallback if totals missing: use rugose/tabulate occ if present\n",
        "    cols = [cc for cc in ['rugose_occ','tabulate_occ'] if cc in dataset.columns]\n",
        "    if cols:\n",
        "        tmp = dataset[cols].apply(pd.to_numeric, errors='coerce').sum(axis=1, min_count=1)\n",
        "        return dataset.loc[tmp > 0].copy()\n",
        "    return dataset\n",
        "\n",
        "df_strom = _filter_has_strom(df_all)\n",
        "df_5myr_strom = _filter_has_strom(df_5myr_all)\n",
        "\n",
        "df_coral = _filter_has_coral(df_all)\n",
        "df_5myr_coral = _filter_has_coral(df_5myr_all)\n",
        "\n",
        "print(f\"Views: stage all={len(df_all)}, strom={len(df_strom)}, coral={len(df_coral)}\")\n",
        "print(f\"Views: 5myr  all={len(df_5myr_all)}, strom={len(df_5myr_strom)}, coral={len(df_5myr_coral)}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# 2. DEFINE VARIABLE GROUPS\n",
        "# -----------------------------------------------------------------------------#\n",
        "reef_targets = [\n",
        "    ('thickness_mean', 'Thickness (Log)'),\n",
        "    ('width_mean', 'Width (Log)')\n",
        "]\n",
        "\n",
        "strom_prop_vars = [('derived_strom_prop', 'Derived Proportion'), ('basal_strom_prop', 'Basal Proportion')] + \\\n",
        "                  [(f'{order}_prop', f'{order} Prop') for order in STROM_ORDERS]\n",
        "\n",
        "strom_occ_vars = [('strom_total_occ', 'Total Occurrence'), ('derived_strom_occ', 'Derived Occurrence'), ('basal_strom_occ', 'Basal Occurrence')] + \\\n",
        "                 [(f'{order}_occ', f'{order} Occ') for order in STROM_ORDERS]\n",
        "\n",
        "strom_div_vars = [('strom_total_gen', 'Total Diversity'), ('derived_strom_div', 'Derived Diversity'), ('basal_strom_div', 'Basal Diversity')] + \\\n",
        "                 [(f'{order}_genus', f'{order} Div') for order in STROM_ORDERS]\n",
        "\n",
        "coral_div_vars = [('rugose_div', 'Rugose Diversity'), ('tabulate_div', 'Tabulate Diversity')]\n",
        "coral_occ_vars = [('rugose_occ', 'Rugose Occurrence'), ('tabulate_occ', 'Tabulate Occurrence')]\n",
        "\n",
        "env_vars_macrostrat = [\n",
        "    ('total_area_km2', 'Total Area'),\n",
        "    ('carbonate_area_km2', 'Carb Area'),\n",
        "    ('carbonate_percentage', 'Carb %')\n",
        "]\n",
        "\n",
        "env_vars_proxies = [\n",
        "    ('temperature', 'SST'),\n",
        "    ('sea_level', 'Sea Level'),\n",
        "    ('atmospheric_O2', 'Atm O2'),\n",
        "    ('atmospheric_CO2', 'Atm CO2'),\n",
        "    ('dissolved_O2', 'Dissolved O2'),\n",
        "    ('d13C', 'δ¹³C')\n",
        "]\n",
        "\n",
        "var_groups = [\n",
        "    ('Strom Props', strom_prop_vars),\n",
        "    ('Strom Occ', strom_occ_vars),\n",
        "    ('Strom Div', strom_div_vars),\n",
        "    ('Coral Div', coral_div_vars),\n",
        "    ('Coral Occ', coral_occ_vars),\n",
        "    ('Macrostrat', env_vars_macrostrat),\n",
        "    ('Proxies', env_vars_proxies),\n",
        "]\n",
        "\n",
        "STROM_GROUPS = {'Strom Props', 'Strom Occ', 'Strom Div'}\n",
        "CORAL_GROUPS = {'Coral Div', 'Coral Occ'}\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# 3. ANALYSIS FUNCTIONS\n",
        "# -----------------------------------------------------------------------------#\n",
        "def get_significance_stars(p):\n",
        "    if p is None or np.isnan(p):\n",
        "        return \"\"\n",
        "    if p < 0.001: return \"***\"\n",
        "    if p < 0.01:  return \"**\"\n",
        "    if p < 0.05:  return \"*\"\n",
        "    return \"\"\n",
        "\n",
        "def calc_stats_pairwise(x, y, min_n=5):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    y = np.asarray(y, dtype=float)\n",
        "\n",
        "    mask = np.isfinite(x) & np.isfinite(y)\n",
        "    x = x[mask]\n",
        "    y = y[mask]\n",
        "    n = int(len(x))\n",
        "\n",
        "    out = dict(\n",
        "        n=n,\n",
        "        spearman_rho=np.nan, spearman_p=np.nan,\n",
        "        pearson_r=np.nan,  pearson_p=np.nan,\n",
        "        status=\"too_few\"\n",
        "    )\n",
        "    if n < min_n:\n",
        "        return out\n",
        "\n",
        "    x_const = (np.unique(x).size <= 1)\n",
        "    y_const = (np.unique(y).size <= 1)\n",
        "    if x_const and y_const:\n",
        "        out[\"status\"] = \"constant_both\"; return out\n",
        "    if x_const:\n",
        "        out[\"status\"] = \"constant_x\"; return out\n",
        "    if y_const:\n",
        "        out[\"status\"] = \"constant_y\"; return out\n",
        "\n",
        "    sr = stats.spearmanr(x, y)\n",
        "    pr = stats.pearsonr(x, y)\n",
        "\n",
        "    out.update(\n",
        "        spearman_rho=float(sr.correlation),\n",
        "        spearman_p=float(sr.pvalue),\n",
        "        pearson_r=float(pr.statistic),\n",
        "        pearson_p=float(pr.pvalue),\n",
        "        status=\"ok\"\n",
        "    )\n",
        "    return out\n",
        "\n",
        "def choose_view(group_name, df_all_in, df_strom_in, df_coral_in):\n",
        "    if group_name in STROM_GROUPS:\n",
        "        return df_strom_in\n",
        "    if group_name in CORAL_GROUPS:\n",
        "        return df_coral_in\n",
        "    return df_all_in\n",
        "\n",
        "def run_correlation_suite(scope_label, df_all_in, df_strom_in, df_coral_in, target_col, target_name, min_n=5):\n",
        "    if df_all_in is None or df_all_in.empty or target_col not in df_all_in.columns:\n",
        "        print(f\"[WARN] ({scope_label}) Missing target {target_col} or dataset empty; skipping {target_name}.\")\n",
        "        return []\n",
        "\n",
        "    print(\"\\n\" + \"-\"*90)\n",
        "    print(f\"{scope_label} | TARGET: {target_name.upper()}\")\n",
        "    print(\"-\"*90)\n",
        "    print(\"{:<12} {:<35} {:>8} {:>10} {:>8} {:>10} {:>5}\".format(\"Group\", \"Variable\", \"rho\", \"p(rho)\", \"r\", \"p(r)\", \"n\"))\n",
        "\n",
        "    results = []\n",
        "    for group_name, group_vars in var_groups:\n",
        "        use_df = choose_view(group_name, df_all_in, df_strom_in, df_coral_in)\n",
        "        if use_df is None or use_df.empty or target_col not in use_df.columns:\n",
        "            continue\n",
        "\n",
        "        y = pd.to_numeric(use_df[target_col], errors='coerce').to_numpy(dtype=float)\n",
        "\n",
        "        for var, label in group_vars:\n",
        "            if var not in use_df.columns:\n",
        "                continue\n",
        "\n",
        "            x = pd.to_numeric(use_df[var], errors='coerce').to_numpy(dtype=float)\n",
        "\n",
        "            if TREAT_CORAL_ZEROS_AS_MISSING and (group_name in CORAL_GROUPS):\n",
        "                x = x.copy()\n",
        "                x[x <= 0] = np.nan\n",
        "\n",
        "            s = calc_stats_pairwise(x, y, min_n=min_n)\n",
        "\n",
        "            if s[\"status\"] == \"ok\":\n",
        "                s_sig = get_significance_stars(s['spearman_p'])\n",
        "                p_sig = get_significance_stars(s['pearson_p'])\n",
        "                print(f\"{group_name[:11]:<12} {label:35s} {s['spearman_rho']:8.2f}{s_sig:3s} {s['spearman_p']:10.3g} \"\n",
        "                      f\"{s['pearson_r']:8.2f}{p_sig:3s} {s['pearson_p']:10.3g} {s['n']:5d}\")\n",
        "            else:\n",
        "                if VERBOSE_SKIPS and s[\"n\"] > 0:\n",
        "                    print(f\"{group_name[:11]:<12} {label:35s} {'':>8} {'':>10} {'':>8} {'':>10} {s['n']:5d}  [{s['status']}]\")\n",
        "\n",
        "            results.append({\n",
        "                'Scope': scope_label,\n",
        "                'Target': target_col,\n",
        "                'Predictor': var,\n",
        "                'Label': label,\n",
        "                'Group': group_name,\n",
        "                'Spearman_Rho': s['spearman_rho'],\n",
        "                'Spearman_P': s['spearman_p'],\n",
        "                'Pearson_R': s['pearson_r'],\n",
        "                'Pearson_P': s['pearson_p'],\n",
        "                'N': int(s['n']),\n",
        "                'Status': s['status']\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# 4. RUN: Stage + 5-Myr\n",
        "# -----------------------------------------------------------------------------#\n",
        "all_results_stage = []\n",
        "for target_col, target_name in reef_targets:\n",
        "    all_results_stage.extend(\n",
        "        run_correlation_suite(\"Stage\", df_all, df_strom, df_coral, target_col, target_name, min_n=MIN_N)\n",
        "    )\n",
        "\n",
        "all_results_5myr = []\n",
        "for target_col, target_name in reef_targets:\n",
        "    all_results_5myr.extend(\n",
        "        run_correlation_suite(\"5-Myr\", df_5myr_all, df_5myr_strom, df_5myr_coral, target_col, target_name, min_n=MIN_N)\n",
        "    )\n",
        "\n",
        "stage_results_df = pd.DataFrame(all_results_stage)\n",
        "myr_results_df = pd.DataFrame(all_results_5myr)\n",
        "\n",
        "stage_ok = stage_results_df[stage_results_df['Status'] == 'ok'].copy() if not stage_results_df.empty else stage_results_df\n",
        "myr_ok   = myr_results_df[myr_results_df['Status'] == 'ok'].copy() if not myr_results_df.empty else myr_results_df\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# 5. SAVE + DISPLAY\n",
        "# -----------------------------------------------------------------------------#\n",
        "if 'OUTPUT_DIR' in globals():\n",
        "    if not stage_ok.empty:\n",
        "        stage_ok.to_csv(f\"{OUTPUT_DIR}/results_correlations_stage.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"\\nSaved: {OUTPUT_DIR}/results_correlations_stage.csv\")\n",
        "    else:\n",
        "        print(\"\\n[WARN] Stage correlations empty (nothing met criteria).\")\n",
        "\n",
        "    if not myr_ok.empty:\n",
        "        myr_ok.to_csv(f\"{OUTPUT_DIR}/results_correlations_5myr.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"Saved: {OUTPUT_DIR}/results_correlations_5myr.csv\")\n",
        "    else:\n",
        "        print(\"[WARN] 5-Myr correlations empty (nothing met criteria).\")\n",
        "\n",
        "    if not stage_results_df.empty:\n",
        "        stage_results_df.to_csv(f\"{OUTPUT_DIR}/results_correlations_stage_FULL_with_status.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"Saved: {OUTPUT_DIR}/results_correlations_stage_FULL_with_status.csv\")\n",
        "    if not myr_results_df.empty:\n",
        "        myr_results_df.to_csv(f\"{OUTPUT_DIR}/results_correlations_5myr_FULL_with_status.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"Saved: {OUTPUT_DIR}/results_correlations_5myr_FULL_with_status.csv\")\n",
        "\n",
        "display(stage_ok if not stage_ok.empty else pd.DataFrame())\n",
        "display(myr_ok if not myr_ok.empty else pd.DataFrame())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oRd7FSq_Rab"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 13: PERFORM ADVANCED STATISTICAL ANALYSES (Centralized) — PART A (REWRITE v3 + LOWESS)\n",
        "#   Updated to enforce the SAME missingness/presence logic for stromatoporoids and corals:\n",
        "#   - KEEP NaNs in MASTER; each analysis uses pairwise complete-case only for its variables\n",
        "#   - Apply strom_total_occ > 0 for ALL strom/coral predictors (your existing rule)\n",
        "#   - Apply coral_total_occ > 0 (or rugose+tabulate fallback) for coral predictors\n",
        "#   - occ/div rule:\n",
        "#       * For group-summary occ/div (derived/basal + rugose/tabulate), DO NOT require pred>0\n",
        "#         (zeros are allowed once the group is present)\n",
        "#       * For other occ/div (taxon-specific occ/div), require pred>0\n",
        "#   - prop rule: allow zeros but require enough non-zero overall (signal gate)\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import re\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "DATA_DIR = Path(\"./output\")\n",
        "\n",
        "if \"OUTPUT_DIR\" in globals():\n",
        "    try:\n",
        "        OUTPUT_DIR = Path(OUTPUT_DIR)\n",
        "    except Exception:\n",
        "        OUTPUT_DIR = DATA_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = DATA_DIR\n",
        "\n",
        "N_BOOT = 10000\n",
        "N_PERM = 10000\n",
        "RNG_SEED = 0\n",
        "\n",
        "MIN_N = 5\n",
        "MIN_POSITIVE = 5          # only used for presence-only occ/div variables\n",
        "MIN_NONZERO_PROP = 5\n",
        "VERBOSE_SKIP_COUNTS = True\n",
        "\n",
        "# LOWESS config\n",
        "DO_LOWESS = True\n",
        "LOWESS_FRAC = 0.4\n",
        "LOWESS_IT = 0\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PERFORMING ADVANCED STATISTICAL ANALYSES (CENTRALIZED) — PART A (REWRITE v3 + LOWESS)\")\n",
        "print(f\"Iterations: Bootstrap={N_BOOT}, Permutation={N_PERM}\")\n",
        "print(f\"LOWESS: {DO_LOWESS} (frac={LOWESS_FRAC}, it={LOWESS_IT})\")\n",
        "print(f\"Output dir: {OUTPUT_DIR}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# -------------------------\n",
        "# Load dataset\n",
        "# -------------------------\n",
        "master_path = DATA_DIR / \"MASTER_dataset_stage.csv\"\n",
        "df = pd.read_csv(master_path, encoding=\"utf-8-sig\")\n",
        "print(f\"Loaded: {master_path.name} ({len(df)} rows)\")\n",
        "\n",
        "if df.empty:\n",
        "    raise SystemExit(\"ERROR: Dataset is empty.\")\n",
        "\n",
        "target = \"thickness_mean\"\n",
        "if target not in df.columns:\n",
        "    raise SystemExit(f\"ERROR: target column '{target}' not found.\")\n",
        "\n",
        "# numeric core\n",
        "for c in [\"midpoint_ma\", target]:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# -------------------------\n",
        "# Standardize atmospheric columns -> atm_O2 / atm_CO2\n",
        "# -------------------------\n",
        "def _find_col(frame, candidates, regex_pat=None):\n",
        "    for c in candidates:\n",
        "        if c in frame.columns:\n",
        "            return c\n",
        "    if regex_pat:\n",
        "        hits = [c for c in frame.columns if re.search(regex_pat, c, flags=re.IGNORECASE)]\n",
        "        return hits[0] if hits else None\n",
        "    return None\n",
        "\n",
        "if \"atm_O2\" not in df.columns:\n",
        "    o2_src = _find_col(\n",
        "        df,\n",
        "        [\"atmospheric_O2\",\"atmospheric_o2\",\"atm_o2\",\"pO2\",\"PO2\",\"oxygen\",\"O2_atm\",\"oxygen_atm\"],\n",
        "        r\"(atm|atmos).*o2|po2\"\n",
        "    )\n",
        "    if o2_src is not None:\n",
        "        df[\"atm_O2\"] = pd.to_numeric(df[o2_src], errors=\"coerce\")\n",
        "        print(f\"[INFO] atm_O2 <- {o2_src}\")\n",
        "\n",
        "if \"atm_CO2\" not in df.columns:\n",
        "    co2_src = _find_col(\n",
        "        df,\n",
        "        [\"atmospheric_CO2\",\"atmospheric_co2\",\"atm_co2\",\"pCO2\",\"PCO2\",\"co2\",\"CO2_atm\",\"co2_atm\"],\n",
        "        r\"(atm|atmos).*co2|pco2\"\n",
        "    )\n",
        "    if co2_src is not None:\n",
        "        df[\"atm_CO2\"] = pd.to_numeric(df[co2_src], errors=\"coerce\")\n",
        "        print(f\"[INFO] atm_CO2 <- {co2_src}\")\n",
        "\n",
        "# -------------------------\n",
        "# Global strom filter (kept)\n",
        "# -------------------------\n",
        "def _filter_no_strom(sub: pd.DataFrame) -> pd.DataFrame:\n",
        "    if sub is None or sub.empty:\n",
        "        return sub\n",
        "    if \"strom_total_occ\" in sub.columns:\n",
        "        v = pd.to_numeric(sub[\"strom_total_occ\"], errors=\"coerce\").fillna(0)\n",
        "        sub = sub.loc[v > 0].copy()\n",
        "    return sub\n",
        "\n",
        "# -------------------------\n",
        "# Global coral filter (added)\n",
        "# -------------------------\n",
        "def _filter_no_coral(sub: pd.DataFrame) -> pd.DataFrame:\n",
        "    if sub is None or sub.empty:\n",
        "        return sub\n",
        "    if \"coral_total_occ\" in sub.columns:\n",
        "        c = pd.to_numeric(sub[\"coral_total_occ\"], errors=\"coerce\").fillna(0)\n",
        "        return sub.loc[c > 0].copy()\n",
        "    # fallback if totals missing\n",
        "    cols = [cc for cc in [\"rugose_occ\", \"tabulate_occ\"] if cc in sub.columns]\n",
        "    if cols:\n",
        "        tmp = sub[cols].apply(pd.to_numeric, errors=\"coerce\").sum(axis=1, min_count=1).fillna(0)\n",
        "        return sub.loc[tmp > 0].copy()\n",
        "    return sub\n",
        "\n",
        "# -------------------------\n",
        "# Predictor typing + presence rules\n",
        "# -------------------------\n",
        "ENV_VARS = set([v for v in [\n",
        "    \"carbonate_area_km2\",\"temperature\",\"dissolved_O2\",\n",
        "    \"d13C\",\"atm_O2\",\"atm_CO2\",\n",
        "    \"total_area_km2\",\"carbonate_percentage\",\"sea_level\"\n",
        "] if v in df.columns])\n",
        "\n",
        "COUNTLIKE_EXTRA = set([v for v in [\"reef_count\",\"strom_total_occ\"] if v in df.columns])\n",
        "\n",
        "STROM_GROUPS = set([\n",
        "    \"derived_strom_prop\",\"basal_strom_prop\",\n",
        "    \"derived_strom_occ\",\"basal_strom_occ\",\n",
        "    \"derived_strom_div\",\"basal_strom_div\",\n",
        "    \"strom_total_occ\",\"strom_total_gen\",\n",
        "    \"Labechiida_prop\",\"Clathrodictyida_prop\",\"Actinostromatida_prop\",\"Stromatoporida_prop\",\n",
        "    \"Stromatoporellida_prop\",\"Syringostromatida_prop\",\"Amphiporida_prop\",\n",
        "    \"Labechiida_occ\",\"Clathrodictyida_occ\",\"Actinostromatida_occ\",\"Stromatoporida_occ\",\n",
        "    \"Stromatoporellida_occ\",\"Syringostromatida_occ\",\"Amphiporida_occ\",\n",
        "    \"Labechiida_genus\",\"Clathrodictyida_genus\",\"Actinostromatida_genus\",\"Stromatoporida_genus\",\n",
        "    \"Stromatoporellida_genus\",\"Syringostromatida_genus\",\"Amphiporida_genus\",\n",
        "])\n",
        "\n",
        "CORAL_GROUPS = set([\"rugose_occ\",\"tabulate_occ\",\"rugose_div\",\"tabulate_div\"])\n",
        "\n",
        "# IMPORTANT: these occ/div variables are \"group summaries\" where zeros are valid within a present group\n",
        "KEEP_ZERO_OCCDIV = set([\n",
        "    \"derived_strom_occ\",\"basal_strom_occ\",\n",
        "    \"derived_strom_div\",\"basal_strom_div\",\n",
        "    \"rugose_occ\",\"tabulate_occ\",\n",
        "    \"rugose_div\",\"tabulate_div\",\n",
        "    \"strom_total_gen\",\n",
        "])\n",
        "\n",
        "def _pred_kind(pred: str) -> str:\n",
        "    if pred in ENV_VARS:\n",
        "        return \"env\"\n",
        "    if pred.endswith(\"_prop\"):\n",
        "        return \"prop\"\n",
        "    if pred.endswith(\"_occ\") or pred.endswith(\"_div\") or pred in COUNTLIKE_EXTRA:\n",
        "        return \"occdiv\"\n",
        "    return \"other\"\n",
        "\n",
        "def _passes_prop_signal(sub: pd.DataFrame, pred: str) -> bool:\n",
        "    if sub is None or sub.empty or len(sub) < MIN_N:\n",
        "        return False\n",
        "    vals = pd.to_numeric(sub[pred], errors=\"coerce\").values\n",
        "    nonzero = int(np.sum(np.isfinite(vals) & (vals != 0)))\n",
        "    return nonzero >= MIN_NONZERO_PROP\n",
        "\n",
        "def _apply_occdiv_presence_rule(sub: pd.DataFrame, pred: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    occ/div rule:\n",
        "      - if pred in KEEP_ZERO_OCCDIV: do NOT require pred>0\n",
        "      - else: require pred>0\n",
        "    \"\"\"\n",
        "    if sub is None or sub.empty:\n",
        "        return sub\n",
        "    if pred in KEEP_ZERO_OCCDIV:\n",
        "        return sub\n",
        "    return sub.loc[pd.to_numeric(sub[pred], errors=\"coerce\") > 0].copy()\n",
        "\n",
        "def get_analysis_data(d: pd.DataFrame, pred: str, targ: str) -> pd.DataFrame:\n",
        "    cols = [pred, targ]\n",
        "    if \"stage\" in d.columns: cols.append(\"stage\")\n",
        "    if \"midpoint_ma\" in d.columns: cols.append(\"midpoint_ma\")\n",
        "    if \"strom_total_occ\" in d.columns: cols.append(\"strom_total_occ\")\n",
        "    if \"coral_total_occ\" in d.columns: cols.append(\"coral_total_occ\")\n",
        "    for cc in [\"rugose_occ\",\"tabulate_occ\"]:\n",
        "        if cc in d.columns and cc not in cols:\n",
        "            cols.append(cc)\n",
        "\n",
        "    sub = d[cols].copy()\n",
        "    sub[pred] = pd.to_numeric(sub[pred], errors=\"coerce\")\n",
        "    sub[targ] = pd.to_numeric(sub[targ], errors=\"coerce\")\n",
        "\n",
        "    # Pairwise complete-case\n",
        "    sub = sub[np.isfinite(sub[pred].values) & np.isfinite(sub[targ].values)].copy()\n",
        "\n",
        "    # Apply group presence filters\n",
        "    if pred in STROM_GROUPS or pred in CORAL_GROUPS:\n",
        "        sub = _filter_no_strom(sub)\n",
        "    if pred in CORAL_GROUPS:\n",
        "        sub = _filter_no_coral(sub)\n",
        "\n",
        "    # Apply predictor-type presence\n",
        "    kind = _pred_kind(pred)\n",
        "    if kind == \"occdiv\":\n",
        "        sub = _apply_occdiv_presence_rule(sub, pred)\n",
        "\n",
        "    return sub\n",
        "\n",
        "def get_lowess_data(d: pd.DataFrame, pred: str) -> pd.DataFrame:\n",
        "    if \"midpoint_ma\" not in d.columns:\n",
        "        return d.iloc[0:0].copy()\n",
        "\n",
        "    cols = [\"midpoint_ma\", pred]\n",
        "    if \"stage\" in d.columns: cols.append(\"stage\")\n",
        "    if \"strom_total_occ\" in d.columns: cols.append(\"strom_total_occ\")\n",
        "    if \"coral_total_occ\" in d.columns: cols.append(\"coral_total_occ\")\n",
        "    for cc in [\"rugose_occ\",\"tabulate_occ\"]:\n",
        "        if cc in d.columns and cc not in cols:\n",
        "            cols.append(cc)\n",
        "\n",
        "    sub = d[cols].copy()\n",
        "    sub[\"midpoint_ma\"] = pd.to_numeric(sub[\"midpoint_ma\"], errors=\"coerce\")\n",
        "    sub[pred] = pd.to_numeric(sub[pred], errors=\"coerce\")\n",
        "\n",
        "    sub = sub[np.isfinite(sub[\"midpoint_ma\"].values) & np.isfinite(sub[pred].values)].copy()\n",
        "\n",
        "    if pred in STROM_GROUPS or pred in CORAL_GROUPS:\n",
        "        sub = _filter_no_strom(sub)\n",
        "    if pred in CORAL_GROUPS:\n",
        "        sub = _filter_no_coral(sub)\n",
        "\n",
        "    kind = _pred_kind(pred)\n",
        "    if kind == \"occdiv\":\n",
        "        sub = _apply_occdiv_presence_rule(sub, pred)\n",
        "\n",
        "    return sub\n",
        "\n",
        "def _safe_spearman(x, y):\n",
        "    x = pd.to_numeric(pd.Series(x), errors=\"coerce\")\n",
        "    y = pd.to_numeric(pd.Series(y), errors=\"coerce\")\n",
        "    m = np.isfinite(x.values) & np.isfinite(y.values)\n",
        "    if m.sum() < MIN_N:\n",
        "        return np.nan, np.nan\n",
        "    res = stats.spearmanr(x.values[m], y.values[m])\n",
        "    return float(res.correlation), float(res.pvalue)\n",
        "\n",
        "def _min_n_for_partial(k_controls: int) -> int:\n",
        "    return max(6, k_controls + 3)\n",
        "\n",
        "rng = np.random.default_rng(RNG_SEED)\n",
        "\n",
        "# -------------------------\n",
        "# Predictor list\n",
        "# -------------------------\n",
        "all_predictors = []\n",
        "\n",
        "for v in [\"derived_strom_prop\", \"basal_strom_prop\", \"derived_strom_occ\", \"basal_strom_occ\", \"derived_strom_div\", \"basal_strom_div\"]:\n",
        "    if v in df.columns:\n",
        "        all_predictors.append(v)\n",
        "\n",
        "for v in [\"rugose_occ\",\"tabulate_occ\",\"rugose_div\",\"tabulate_div\"]:\n",
        "    if v in df.columns:\n",
        "        all_predictors.append(v)\n",
        "\n",
        "for v in [\"carbonate_area_km2\",\"temperature\",\"dissolved_O2\",\n",
        "          \"d13C\",\"atm_O2\",\"atm_CO2\",\"total_area_km2\",\"carbonate_percentage\",\"sea_level\"]:\n",
        "    if v in df.columns and v not in all_predictors:\n",
        "        all_predictors.append(v)\n",
        "\n",
        "for v in [\n",
        "    \"Labechiida_prop\",\"Clathrodictyida_prop\",\"Actinostromatida_prop\",\n",
        "    \"Stromatoporida_prop\",\"Stromatoporellida_prop\",\"Syringostromatida_prop\",\"Amphiporida_prop\"\n",
        "]:\n",
        "    if v in df.columns:\n",
        "        all_predictors.append(v)\n",
        "\n",
        "all_predictors = list(dict.fromkeys(all_predictors))\n",
        "print(\"[INFO] Predictors included:\", all_predictors)\n",
        "\n",
        "# ==============================================================================\n",
        "# LOWESS (pred vs time)\n",
        "# ==============================================================================\n",
        "if DO_LOWESS:\n",
        "    print(\"\\n[LOWESS] Computing LOWESS smooths vs time (midpoint_ma)...\")\n",
        "\n",
        "    if \"midpoint_ma\" not in df.columns:\n",
        "        print(\"  [WARN] midpoint_ma missing; LOWESS skipped.\")\n",
        "    else:\n",
        "        lowess_predictors = []\n",
        "        for v in [\n",
        "            \"basal_strom_prop\",\"derived_strom_prop\",\n",
        "            \"basal_strom_occ\",\"derived_strom_occ\",\n",
        "            \"basal_strom_div\",\"derived_strom_div\"\n",
        "        ]:\n",
        "            if v in df.columns:\n",
        "                lowess_predictors.append(v)\n",
        "\n",
        "        lowess_predictors = list(dict.fromkeys(lowess_predictors))\n",
        "        print(\"  LOWESS predictors:\", lowess_predictors)\n",
        "\n",
        "        lowess_rows = []\n",
        "        for pred in lowess_predictors:\n",
        "            sub = get_lowess_data(df, pred)\n",
        "            kind = _pred_kind(pred)\n",
        "\n",
        "            if kind == \"prop\" and len(sub) >= MIN_N and (not _passes_prop_signal(sub, pred)):\n",
        "                continue\n",
        "            if len(sub) < MIN_N:\n",
        "                continue\n",
        "\n",
        "            sub2 = sub.sort_values(\"midpoint_ma\").reset_index(drop=True)\n",
        "            x = sub2[\"midpoint_ma\"].values.astype(float)\n",
        "            y = sub2[pred].values.astype(float)\n",
        "\n",
        "            try:\n",
        "                fit = lowess(endog=y, exog=x, frac=LOWESS_FRAC, it=LOWESS_IT, return_sorted=True)\n",
        "                for xi, yhat in fit:\n",
        "                    lowess_rows.append({\n",
        "                        \"Predictor\": pred,\n",
        "                        \"midpoint_ma\": float(xi),\n",
        "                        \"lowess_y\": float(yhat),\n",
        "                        \"N_used\": int(len(sub2)),\n",
        "                        \"frac\": float(LOWESS_FRAC),\n",
        "                        \"it\": int(LOWESS_IT)\n",
        "                    })\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        lowess_df = pd.DataFrame(lowess_rows)\n",
        "        lowess_df.to_csv(OUTPUT_DIR / \"results_lowess_predictors_vs_time_long.csv\",\n",
        "                         index=False, encoding=\"utf-8-sig\")\n",
        "        print(\"  -> Saved results_lowess_predictors_vs_time_long.csv\")\n",
        "        display(lowess_df.head(20))\n",
        "\n",
        "# ==============================================================================\n",
        "# From here down: advanced stats blocks.\n",
        "# basal_strom_prop excluded for multivariate blocks to avoid singularity.\n",
        "# ==============================================================================\n",
        "stats_predictors = [p for p in all_predictors if p != \"basal_strom_prop\"]\n",
        "\n",
        "print(\"\\n[INFO] Predictors used for correlation/bootstrap/permutation/LOO/detrend/partial/univariate:\",\n",
        "      stats_predictors)\n",
        "\n",
        "# ==============================================================================\n",
        "# 0. ORIGINAL SPEARMAN CORRELATIONS\n",
        "# ==============================================================================\n",
        "print(\"\\n0. Computing original Spearman correlations...\")\n",
        "corr_rows = []\n",
        "skip_counts = {\"too_few\":0, \"prop_low_signal\":0, \"ok\":0}\n",
        "\n",
        "for pred in stats_predictors:\n",
        "    sub = get_analysis_data(df, pred, target)\n",
        "    kind = _pred_kind(pred)\n",
        "\n",
        "    if kind == \"prop\" and len(sub) >= MIN_N and (not _passes_prop_signal(sub, pred)):\n",
        "        corr_rows.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"N\":int(len(sub)),\n",
        "                          \"spearman_rho\":np.nan,\"spearman_p\":np.nan,\"Status\":\"SKIP_prop_low_signal\"})\n",
        "        skip_counts[\"prop_low_signal\"] += 1\n",
        "        continue\n",
        "\n",
        "    r, p = _safe_spearman(sub[pred], sub[target]) if len(sub) else (np.nan, np.nan)\n",
        "    status = \"OK\" if (len(sub) >= MIN_N and np.isfinite(r)) else \"SKIP_too_few_rows\"\n",
        "    corr_rows.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"N\":int(len(sub)),\n",
        "                      \"spearman_rho\":r,\"spearman_p\":p,\"Status\":status})\n",
        "    skip_counts[\"ok\" if status==\"OK\" else \"too_few\"] += 1\n",
        "\n",
        "corr_df = pd.DataFrame(corr_rows)\n",
        "corr_df.to_csv(OUTPUT_DIR / \"results_spearman_original_all_predictors.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_spearman_original_all_predictors.csv\")\n",
        "if VERBOSE_SKIP_COUNTS:\n",
        "    print(\"[INFO] Spearman status counts:\", skip_counts)\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. BOOTSTRAP\n",
        "# ==============================================================================\n",
        "print(f\"\\n1. Running Bootstrap ({N_BOOT} iter)...\")\n",
        "boot_summary = []\n",
        "boot_dist_long = []\n",
        "\n",
        "for pred in stats_predictors:\n",
        "    sub = get_analysis_data(df, pred, target)\n",
        "    kind = _pred_kind(pred)\n",
        "    n = len(sub)\n",
        "\n",
        "    if kind == \"prop\" and n >= MIN_N and (not _passes_prop_signal(sub, pred)):\n",
        "        boot_summary.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"N\":int(n),\n",
        "                             \"spearman_observed\":np.nan,\"spearman_ci_low\":np.nan,\"spearman_ci_high\":np.nan,\n",
        "                             \"Status\":\"SKIP_prop_low_signal\"})\n",
        "        continue\n",
        "\n",
        "    if n < MIN_N:\n",
        "        boot_summary.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"N\":int(n),\n",
        "                             \"spearman_observed\":np.nan,\"spearman_ci_low\":np.nan,\"spearman_ci_high\":np.nan,\n",
        "                             \"Status\":\"SKIP_too_few_rows\"})\n",
        "        continue\n",
        "\n",
        "    rho_obs, _ = stats.spearmanr(sub[pred], sub[target])\n",
        "\n",
        "    idx = np.arange(n)\n",
        "    rhos = np.empty(N_BOOT, dtype=float)\n",
        "    for b in range(N_BOOT):\n",
        "        s = rng.choice(idx, size=n, replace=True)\n",
        "        r, _ = stats.spearmanr(sub[pred].iloc[s], sub[target].iloc[s])\n",
        "        rhos[b] = r\n",
        "\n",
        "    ci_low = np.nanpercentile(rhos, 2.5)\n",
        "    ci_high = np.nanpercentile(rhos, 97.5)\n",
        "\n",
        "    boot_summary.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"N\":int(n),\n",
        "                         \"spearman_observed\":float(rho_obs) if np.isfinite(rho_obs) else np.nan,\n",
        "                         \"spearman_ci_low\":float(ci_low),\"spearman_ci_high\":float(ci_high),\"Status\":\"OK\"})\n",
        "    boot_dist_long.extend([{\"Predictor\":pred,\"iter\":i+1,\"rho\":float(rhos[i])} for i in range(N_BOOT)])\n",
        "\n",
        "pd.DataFrame(boot_summary).to_csv(OUTPUT_DIR / \"results_bootstrap.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "pd.DataFrame(boot_dist_long).to_csv(OUTPUT_DIR / \"results_bootstrap_dist_all_predictors_long.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_bootstrap.csv\")\n",
        "print(\"  -> Saved results_bootstrap_dist_all_predictors_long.csv\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. PERMUTATION\n",
        "# ==============================================================================\n",
        "print(f\"2. Running Permutation ({N_PERM} iter)...\")\n",
        "perm_summary = []\n",
        "perm_dist_long = []\n",
        "\n",
        "for pred in stats_predictors:\n",
        "    sub = get_analysis_data(df, pred, target)\n",
        "    kind = _pred_kind(pred)\n",
        "    n = len(sub)\n",
        "\n",
        "    if kind == \"prop\" and n >= MIN_N and (not _passes_prop_signal(sub, pred)):\n",
        "        perm_summary.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"N\":int(n),\n",
        "                             \"spearman_observed\":np.nan,\"spearman_p_permutation\":np.nan,\n",
        "                             \"Status\":\"SKIP_prop_low_signal\"})\n",
        "        continue\n",
        "\n",
        "    if n < MIN_N:\n",
        "        perm_summary.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"N\":int(n),\n",
        "                             \"spearman_observed\":np.nan,\"spearman_p_permutation\":np.nan,\n",
        "                             \"Status\":\"SKIP_too_few_rows\"})\n",
        "        continue\n",
        "\n",
        "    rho_obs, _ = stats.spearmanr(sub[pred], sub[target])\n",
        "    y = sub[target].values\n",
        "\n",
        "    perm_rhos = np.empty(N_PERM, dtype=float)\n",
        "    extreme = 0\n",
        "    for i in range(N_PERM):\n",
        "        y_perm = rng.permutation(y)\n",
        "        r_perm, _ = stats.spearmanr(sub[pred].values, y_perm)\n",
        "        perm_rhos[i] = r_perm\n",
        "        if np.isfinite(r_perm) and np.isfinite(rho_obs) and (abs(r_perm) >= abs(rho_obs)):\n",
        "            extreme += 1\n",
        "\n",
        "    p_val = (extreme + 1) / (N_PERM + 1)\n",
        "    perm_summary.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"N\":int(n),\n",
        "                         \"spearman_observed\":float(rho_obs) if np.isfinite(rho_obs) else np.nan,\n",
        "                         \"spearman_p_permutation\":float(p_val),\"Status\":\"OK\"})\n",
        "    perm_dist_long.extend([{\"Predictor\":pred,\"iter\":i+1,\"rho\":float(perm_rhos[i])} for i in range(N_PERM)])\n",
        "\n",
        "pd.DataFrame(perm_summary).to_csv(OUTPUT_DIR / \"results_permutation.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "pd.DataFrame(perm_dist_long).to_csv(OUTPUT_DIR / \"results_permutation_dist_all_predictors_long.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_permutation.csv\")\n",
        "print(\"  -> Saved results_permutation_dist_all_predictors_long.csv\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. LEAVE-ONE-OUT\n",
        "# ==============================================================================\n",
        "print(\"3. Running Leave-One-Out...\")\n",
        "loo_detailed_long = []\n",
        "loo_summary = []\n",
        "\n",
        "for pred in stats_predictors:\n",
        "    cols = [\"stage\", pred, target]\n",
        "    if \"strom_total_occ\" in df.columns: cols.append(\"strom_total_occ\")\n",
        "    if \"coral_total_occ\" in df.columns: cols.append(\"coral_total_occ\")\n",
        "    for cc in [\"rugose_occ\",\"tabulate_occ\"]:\n",
        "        if cc in df.columns and cc not in cols:\n",
        "            cols.append(cc)\n",
        "    if \"midpoint_ma\" in df.columns: cols.append(\"midpoint_ma\")\n",
        "\n",
        "    v = df[cols].copy()\n",
        "    for c in [pred, target]:\n",
        "        v[c] = pd.to_numeric(v[c], errors=\"coerce\")\n",
        "    v = v.dropna(subset=[\"stage\", pred, target])\n",
        "\n",
        "    if pred in STROM_GROUPS or pred in CORAL_GROUPS:\n",
        "        v = _filter_no_strom(v)\n",
        "    if pred in CORAL_GROUPS:\n",
        "        v = _filter_no_coral(v)\n",
        "\n",
        "    kind = _pred_kind(pred)\n",
        "    if kind == \"occdiv\":\n",
        "        v = _apply_occdiv_presence_rule(v, pred)\n",
        "\n",
        "    if kind == \"prop\" and len(v) >= MIN_N and (not _passes_prop_signal(v, pred)):\n",
        "        loo_summary.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"N\":int(len(v)),\n",
        "                            \"LOO_Mean_Rho\":np.nan,\"LOO_Max_P\":np.nan,\"LOO_Min_Rho\":np.nan,\"LOO_Max_Rho\":np.nan,\n",
        "                            \"Status\":\"SKIP_prop_low_signal\"})\n",
        "        continue\n",
        "\n",
        "    v = v.reset_index(drop=True)\n",
        "    n = len(v)\n",
        "    if n < MIN_N:\n",
        "        loo_summary.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"N\":int(n),\n",
        "                            \"LOO_Mean_Rho\":np.nan,\"LOO_Max_P\":np.nan,\"LOO_Min_Rho\":np.nan,\"LOO_Max_Rho\":np.nan,\n",
        "                            \"Status\":\"SKIP_too_few_rows\"})\n",
        "        continue\n",
        "\n",
        "    rho_full, _ = stats.spearmanr(v[pred], v[target])\n",
        "\n",
        "    rhos, ps = [], []\n",
        "    for i in range(n):\n",
        "        vv = v.drop(index=i)\n",
        "        r, p = _safe_spearman(vv[pred], vv[target])\n",
        "        rhos.append(r); ps.append(p)\n",
        "        loo_detailed_long.append({\n",
        "            \"Predictor\": pred,\n",
        "            \"Stage_Dropped\": v.loc[i, \"stage\"],\n",
        "            \"LOO_Rho\": r,\n",
        "            \"Diff_from_Full\": (r - rho_full) if (np.isfinite(r) and np.isfinite(rho_full)) else np.nan\n",
        "        })\n",
        "\n",
        "    loo_summary.append({\n",
        "        \"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"N\":int(n),\n",
        "        \"LOO_Mean_Rho\":float(np.nanmean(rhos)),\n",
        "        \"LOO_Max_P\":float(np.nanmax(ps)),\n",
        "        \"LOO_Min_Rho\":float(np.nanmin(rhos)),\n",
        "        \"LOO_Max_Rho\":float(np.nanmax(rhos)),\n",
        "        \"Status\":\"OK\"\n",
        "    })\n",
        "\n",
        "pd.DataFrame(loo_detailed_long).to_csv(OUTPUT_DIR / \"results_loo_detailed_all_predictors_long.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "pd.DataFrame(loo_summary).to_csv(OUTPUT_DIR / \"results_loo_summary.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_loo_detailed_all_predictors_long.csv\")\n",
        "print(\"  -> Saved results_loo_summary.csv\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. DETRENDING\n",
        "# ==============================================================================\n",
        "print(\"4. Running Detrending (status logged)...\")\n",
        "det_results = []\n",
        "\n",
        "if \"midpoint_ma\" not in df.columns:\n",
        "    print(\"  [ERROR] midpoint_ma missing -> detrending cannot run.\")\n",
        "else:\n",
        "    for pred in stats_predictors:\n",
        "        row = {\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"N\":0,\n",
        "               \"Detrend_Rho\":np.nan,\"Detrend_P\":np.nan,\"Status\":\"INIT\"}\n",
        "\n",
        "        cols = [\"midpoint_ma\", pred, target]\n",
        "        if \"strom_total_occ\" in df.columns: cols.append(\"strom_total_occ\")\n",
        "        if \"coral_total_occ\" in df.columns: cols.append(\"coral_total_occ\")\n",
        "        for cc in [\"rugose_occ\",\"tabulate_occ\"]:\n",
        "            if cc in df.columns and cc not in cols:\n",
        "                cols.append(cc)\n",
        "\n",
        "        v = df[cols].copy()\n",
        "        v[\"midpoint_ma\"] = pd.to_numeric(v[\"midpoint_ma\"], errors=\"coerce\")\n",
        "        v[pred] = pd.to_numeric(v[pred], errors=\"coerce\")\n",
        "        v[target] = pd.to_numeric(v[target], errors=\"coerce\")\n",
        "\n",
        "        v = v.dropna(subset=[\"midpoint_ma\", pred, target])\n",
        "\n",
        "        if pred in STROM_GROUPS or pred in CORAL_GROUPS:\n",
        "            v = _filter_no_strom(v)\n",
        "        if pred in CORAL_GROUPS:\n",
        "            v = _filter_no_coral(v)\n",
        "\n",
        "        kind = _pred_kind(pred)\n",
        "        if kind == \"occdiv\":\n",
        "            v = _apply_occdiv_presence_rule(v, pred)\n",
        "        elif kind == \"prop\":\n",
        "            if len(v) >= MIN_N and (not _passes_prop_signal(v, pred)):\n",
        "                row[\"N\"] = int(len(v))\n",
        "                row[\"Status\"] = \"SKIP_prop_low_signal\"\n",
        "                det_results.append(row)\n",
        "                continue\n",
        "\n",
        "        n = len(v)\n",
        "        row[\"N\"] = int(n)\n",
        "        if n < MIN_N:\n",
        "            row[\"Status\"] = \"SKIP_too_few_rows\"\n",
        "            det_results.append(row)\n",
        "            continue\n",
        "        if v[pred].nunique(dropna=True) < 2 or float(np.nanstd(v[pred].values)) == 0.0:\n",
        "            row[\"Status\"] = \"SKIP_constant_predictor\"\n",
        "            det_results.append(row)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            sx, ix, *_ = stats.linregress(v[\"midpoint_ma\"], v[pred])\n",
        "            sy, iy, *_ = stats.linregress(v[\"midpoint_ma\"], v[target])\n",
        "            resid_x = v[pred] - (sx * v[\"midpoint_ma\"] + ix)\n",
        "            resid_y = v[target] - (sy * v[\"midpoint_ma\"] + iy)\n",
        "            r, p = stats.spearmanr(resid_x, resid_y, nan_policy=\"omit\")\n",
        "            row[\"Detrend_Rho\"] = float(r) if np.isfinite(r) else np.nan\n",
        "            row[\"Detrend_P\"] = float(p) if np.isfinite(p) else np.nan\n",
        "            row[\"Status\"] = \"OK\"\n",
        "        except Exception as e:\n",
        "            row[\"Status\"] = f\"FAIL_{type(e).__name__}\"\n",
        "\n",
        "        det_results.append(row)\n",
        "\n",
        "pd.DataFrame(det_results).to_csv(OUTPUT_DIR / \"results_detrending_improved.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_detrending_improved.csv\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. PARTIAL CORRELATIONS\n",
        "# ==============================================================================\n",
        "print(\"\\n5. Running Partial Correlations (predictor-type aware)...\")\n",
        "\n",
        "env_controls = [v for v in [\"carbonate_area_km2\",\"temperature\",\"dissolved_O2\"] if v in df.columns]\n",
        "biotic_controls_for_env = [v for v in [\"derived_strom_prop\"] if v in df.columns]\n",
        "\n",
        "def _partial_spearman(sub: pd.DataFrame, pred: str, targ: str, controls: list):\n",
        "    Xc = sm.add_constant(sub[controls], has_constant=\"add\")\n",
        "    res_pred = sm.OLS(sub[pred].values.astype(float), Xc.values.astype(float)).fit().resid\n",
        "    res_targ = sm.OLS(sub[targ].values.astype(float), Xc.values.astype(float)).fit().resid\n",
        "    r, p = stats.spearmanr(res_pred, res_targ, nan_policy=\"omit\")\n",
        "    return float(r) if np.isfinite(r) else np.nan, float(p) if np.isfinite(p) else np.nan\n",
        "\n",
        "part_rows = []\n",
        "\n",
        "for pred in stats_predictors:\n",
        "    kind = _pred_kind(pred)\n",
        "    if kind == \"env\":\n",
        "        controls = biotic_controls_for_env[:]\n",
        "        test_type = \"Environment (Biotic Controlled)\"\n",
        "    else:\n",
        "        controls = env_controls[:]\n",
        "        test_type = \"Biotic/Taxon (Env Controlled)\"\n",
        "\n",
        "    controls = [c for c in controls if c in df.columns]\n",
        "    if len(controls) < 1:\n",
        "        part_rows.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"Test_Type\":test_type,\n",
        "                          \"Controls\":\",\".join(controls),\"N\":0,\"spearman_partial\":np.nan,\"spearman_p_partial\":np.nan,\n",
        "                          \"Status\":\"SKIP_insufficient_controls\"})\n",
        "        continue\n",
        "\n",
        "    cols = [pred, target] + controls\n",
        "    if \"strom_total_occ\" in df.columns: cols.append(\"strom_total_occ\")\n",
        "    if \"coral_total_occ\" in df.columns: cols.append(\"coral_total_occ\")\n",
        "    for cc in [\"rugose_occ\",\"tabulate_occ\"]:\n",
        "        if cc in df.columns and cc not in cols:\n",
        "            cols.append(cc)\n",
        "\n",
        "    sub = df[cols].copy()\n",
        "    for c in [pred, target] + controls:\n",
        "        sub[c] = pd.to_numeric(sub[c], errors=\"coerce\")\n",
        "\n",
        "    sub = sub.dropna(subset=[pred, target] + controls)\n",
        "\n",
        "    if pred in STROM_GROUPS or pred in CORAL_GROUPS:\n",
        "        sub = _filter_no_strom(sub)\n",
        "    if pred in CORAL_GROUPS:\n",
        "        sub = _filter_no_coral(sub)\n",
        "\n",
        "    if kind == \"occdiv\":\n",
        "        sub = _apply_occdiv_presence_rule(sub, pred)\n",
        "        # Only enforce MIN_POSITIVE for presence-only occ/div variables\n",
        "        if pred not in KEEP_ZERO_OCCDIV and len(sub) < MIN_POSITIVE:\n",
        "            part_rows.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"Test_Type\":test_type,\n",
        "                              \"Controls\":\",\".join(controls),\"N\":int(len(sub)),\n",
        "                              \"spearman_partial\":np.nan,\"spearman_p_partial\":np.nan,\n",
        "                              \"Status\":\"SKIP_too_few_positive\"})\n",
        "            continue\n",
        "    elif kind == \"prop\":\n",
        "        if len(sub) >= MIN_N and (not _passes_prop_signal(sub, pred)):\n",
        "            part_rows.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"Test_Type\":test_type,\n",
        "                              \"Controls\":\",\".join(controls),\"N\":int(len(sub)),\n",
        "                              \"spearman_partial\":np.nan,\"spearman_p_partial\":np.nan,\n",
        "                              \"Status\":\"SKIP_prop_low_signal\"})\n",
        "            continue\n",
        "\n",
        "    n = len(sub)\n",
        "    min_n = _min_n_for_partial(len(controls))\n",
        "    if n < min_n:\n",
        "        part_rows.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"Test_Type\":test_type,\n",
        "                          \"Controls\":\",\".join(controls),\"N\":int(n),\n",
        "                          \"spearman_partial\":np.nan,\"spearman_p_partial\":np.nan,\n",
        "                          \"Status\":f\"SKIP_too_few_rows(n<{min_n})\"})\n",
        "        continue\n",
        "\n",
        "    if sub[pred].nunique(dropna=True) < 2 or float(np.nanstd(sub[pred].values)) == 0.0:\n",
        "        part_rows.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"Test_Type\":test_type,\n",
        "                          \"Controls\":\",\".join(controls),\"N\":int(n),\n",
        "                          \"spearman_partial\":np.nan,\"spearman_p_partial\":np.nan,\n",
        "                          \"Status\":\"SKIP_constant_predictor\"})\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        r, p = _partial_spearman(sub, pred, target, controls)\n",
        "        part_rows.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"Test_Type\":test_type,\n",
        "                          \"Controls\":\",\".join(controls),\"N\":int(n),\n",
        "                          \"spearman_partial\":r,\"spearman_p_partial\":p,\"Status\":\"OK\"})\n",
        "    except Exception as e:\n",
        "        part_rows.append({\"Dataset\":\"Stage-Level Data\",\"Target\":target,\"Predictor\":pred,\"Test_Type\":test_type,\n",
        "                          \"Controls\":\",\".join(controls),\"N\":int(n),\n",
        "                          \"spearman_partial\":np.nan,\"spearman_p_partial\":np.nan,\n",
        "                          \"Status\":f\"FAIL_{type(e).__name__}\"})\n",
        "\n",
        "part_df = pd.DataFrame(part_rows).sort_values([\"Status\",\"spearman_p_partial\"], na_position=\"last\")\n",
        "part_df.to_csv(OUTPUT_DIR / \"results_partial_correlations.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_partial_correlations.csv\")\n",
        "\n",
        "print(\"\\n[DISPLAY] Partial correlations (OK rows):\")\n",
        "display(part_df[part_df[\"Status\"] == \"OK\"].sort_values(\"spearman_p_partial\", na_position=\"last\"))\n",
        "\n",
        "print(\"\\n[DISPLAY] Partial correlations (all rows incl. skips/fails):\")\n",
        "display(part_df)\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. VARIANCE PARTITIONING (strict)\n",
        "# ==============================================================================\n",
        "print(\"6. Running Variance Partitioning (groups; strict subset)...\")\n",
        "\n",
        "def get_adj_r2_group(data: pd.DataFrame, y_col: str, x_cols: list) -> float:\n",
        "    if len(x_cols) == 0:\n",
        "        return np.nan\n",
        "    vv = data[[y_col] + x_cols].copy()\n",
        "    for c in [y_col] + x_cols:\n",
        "        vv[c] = pd.to_numeric(vv[c], errors=\"coerce\")\n",
        "    vv = vv.dropna()\n",
        "    if len(vv) < len(x_cols) + 2:\n",
        "        return np.nan\n",
        "    # drop constant predictors to avoid singular designs\n",
        "    keep = []\n",
        "    for xc in x_cols:\n",
        "        if vv[xc].nunique(dropna=True) >= 2 and float(np.nanstd(vv[xc].values)) > 0.0:\n",
        "            keep.append(xc)\n",
        "    if len(keep) == 0:\n",
        "        return np.nan\n",
        "    X = sm.add_constant(vv[keep], has_constant=\"add\")\n",
        "    y = vv[y_col].values.astype(float)\n",
        "    try:\n",
        "        return float(sm.OLS(y, X.values.astype(float)).fit(method=\"qr\").rsquared_adj)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "A = [v for v in [\"derived_strom_prop\"] if v in df.columns]  # derived only\n",
        "B = [v for v in [\"rugose_occ\",\"tabulate_occ\"] if v in df.columns]\n",
        "C = env_controls[:]\n",
        "ABC = A + B + C\n",
        "\n",
        "vp_cols = [target] + ABC\n",
        "if \"strom_total_occ\" in df.columns: vp_cols.append(\"strom_total_occ\")\n",
        "if \"coral_total_occ\" in df.columns: vp_cols.append(\"coral_total_occ\")\n",
        "for cc in [\"rugose_occ\",\"tabulate_occ\"]:\n",
        "    if cc in df.columns and cc not in vp_cols:\n",
        "        vp_cols.append(cc)\n",
        "\n",
        "sub_vp = df[vp_cols].copy()\n",
        "for c in [target] + ABC:\n",
        "    sub_vp[c] = pd.to_numeric(sub_vp[c], errors=\"coerce\")\n",
        "sub_vp = sub_vp.dropna(subset=[target] + ABC)\n",
        "\n",
        "# Apply group presence (do NOT force rugose/tabulate >0; zeros allowed once corals present)\n",
        "sub_vp = _filter_no_strom(sub_vp)\n",
        "sub_vp = _filter_no_coral(sub_vp)\n",
        "\n",
        "# Prop signal gate for A\n",
        "for pred in A:\n",
        "    if not _passes_prop_signal(sub_vp, pred):\n",
        "        sub_vp = sub_vp.iloc[0:0].copy()\n",
        "        break\n",
        "\n",
        "r2_abc = get_adj_r2_group(sub_vp, target, ABC)\n",
        "r2_bc  = get_adj_r2_group(sub_vp, target, B + C)\n",
        "r2_ac  = get_adj_r2_group(sub_vp, target, A + C)\n",
        "r2_ab  = get_adj_r2_group(sub_vp, target, A + B)\n",
        "\n",
        "unique_a = r2_abc - r2_bc if np.isfinite(r2_abc) and np.isfinite(r2_bc) else np.nan\n",
        "unique_b = r2_abc - r2_ac if np.isfinite(r2_abc) and np.isfinite(r2_ac) else np.nan\n",
        "unique_c = r2_abc - r2_ab if np.isfinite(r2_abc) and np.isfinite(r2_ab) else np.nan\n",
        "shared   = r2_abc - (unique_a + unique_b + unique_c) if np.isfinite(r2_abc) else np.nan\n",
        "\n",
        "var_out = pd.DataFrame([{\n",
        "    \"Dataset\":\"Stage-Level Data\",\n",
        "    \"Target\":target,\n",
        "    \"Strom_Group\":\",\".join(A),\n",
        "    \"Coral_Group\":\",\".join(B),\n",
        "    \"Env_Group\":\",\".join(C),\n",
        "    \"Unique_Strom\":unique_a,\n",
        "    \"Unique_Coral\":unique_b,\n",
        "    \"Unique_Env\":unique_c,\n",
        "    \"Shared\":shared,\n",
        "    \"Residual\":(1 - r2_abc) if np.isfinite(r2_abc) else np.nan,\n",
        "    \"Total_R2_adj\":r2_abc,\n",
        "    \"N\":int(len(sub_vp))\n",
        "}])\n",
        "var_out.to_csv(OUTPUT_DIR / \"results_variance_partition_improved.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_variance_partition_improved.csv\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 6b. Univariate adj-R2\n",
        "# ==============================================================================\n",
        "print(\"6b. Running Univariate adj-R2 (robust)...\")\n",
        "uni_rows = []\n",
        "\n",
        "for pred in stats_predictors:\n",
        "    sub = get_analysis_data(df, pred, target)\n",
        "    kind = _pred_kind(pred)\n",
        "\n",
        "    if kind == \"prop\" and len(sub) >= MIN_N and (not _passes_prop_signal(sub, pred)):\n",
        "        uni_rows.append({\"Predictor\":pred,\"N\":int(len(sub)),\"Adj_R2_univariate\":np.nan,\"Status\":\"SKIP_prop_low_signal\"})\n",
        "        continue\n",
        "\n",
        "    n = len(sub)\n",
        "    if n < 6:\n",
        "        uni_rows.append({\"Predictor\":pred,\"N\":int(n),\"Adj_R2_univariate\":np.nan,\"Status\":\"SKIP_too_few_rows\"})\n",
        "        continue\n",
        "\n",
        "    x = pd.to_numeric(sub[pred], errors=\"coerce\")\n",
        "    y = pd.to_numeric(sub[target], errors=\"coerce\")\n",
        "    m = np.isfinite(x.values) & np.isfinite(y.values)\n",
        "    x = x[m]; y = y[m]\n",
        "\n",
        "    if len(x) < 6:\n",
        "        uni_rows.append({\"Predictor\":pred,\"N\":int(len(x)),\"Adj_R2_univariate\":np.nan,\"Status\":\"SKIP_too_few_rows\"})\n",
        "        continue\n",
        "    if x.nunique(dropna=True) < 2 or float(np.nanstd(x.values)) == 0.0:\n",
        "        uni_rows.append({\"Predictor\":pred,\"N\":int(len(x)),\"Adj_R2_univariate\":np.nan,\"Status\":\"SKIP_constant_predictor\"})\n",
        "        continue\n",
        "\n",
        "    X = sm.add_constant(pd.DataFrame({pred: x.values}), has_constant=\"add\").values.astype(float)\n",
        "    yy = y.values.astype(float)\n",
        "\n",
        "    try:\n",
        "        fit = sm.OLS(yy, X).fit(method=\"qr\")\n",
        "        uni_rows.append({\"Predictor\":pred,\"N\":int(len(x)),\"Adj_R2_univariate\":float(fit.rsquared_adj),\"Status\":\"OK\"})\n",
        "    except Exception as e:\n",
        "        uni_rows.append({\"Predictor\":pred,\"N\":int(len(x)),\"Adj_R2_univariate\":np.nan,\"Status\":f\"FAIL_{type(e).__name__}\"})\n",
        "\n",
        "pd.DataFrame(uni_rows).to_csv(OUTPUT_DIR / \"results_adjR2_univariate_all_predictors.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"  -> Saved results_adjR2_univariate_all_predictors.csv\")\n",
        "\n",
        "CELL13_PART_A_DONE = True\n",
        "print(\"\\n✓ CELL 13 PART A COMPLETE (v3 + LOWESS; strom+coral summary occ/div allow zeros once present).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZpjcVv1GLq1V"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 14: MODEL SELECTION + LOWESS + LAG (Robust guards) — NO BASAL\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CELL 14: AICc + LOWESS + LAG (NO BASAL)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def _filter_no_strom(sub):\n",
        "    if 'strom_total_occ' in sub.columns:\n",
        "        sub = sub[sub['strom_total_occ'].notna() & (pd.to_numeric(sub['strom_total_occ'], errors='coerce') > 0)]\n",
        "    return sub\n",
        "\n",
        "# ---------------------------\n",
        "# 7. AICc MODEL SELECTION (COMMON SUBSET; Method 1) — strom/coral/env only\n",
        "# ---------------------------\n",
        "print(\"7. Running AICc Model Selection (COMMON SUBSET; strom/coral/env only)...\")\n",
        "\n",
        "target = 'thickness_mean'\n",
        "\n",
        "# requested groups (NO BASAL)\n",
        "strom_vars = [v for v in ['derived_strom_prop'] if v in df.columns]\n",
        "coral_vars = [v for v in ['rugose_div', 'tabulate_div'] if v in df.columns]\n",
        "env_vars   = [v for v in ['carbonate_area_km2', 'temperature', 'dissolved_O2'] if v in df.columns]\n",
        "\n",
        "needed = [target] + strom_vars + coral_vars + env_vars\n",
        "if 'strom_total_occ' in df.columns:\n",
        "    needed += ['strom_total_occ']\n",
        "\n",
        "sub_aic = df[needed].copy()\n",
        "sub_aic[target] = pd.to_numeric(sub_aic[target], errors='coerce')\n",
        "for c in strom_vars + coral_vars + env_vars:\n",
        "    sub_aic[c] = pd.to_numeric(sub_aic[c], errors='coerce')\n",
        "\n",
        "# COMMON SUBSET (complete for full set)\n",
        "sub_aic = sub_aic.dropna(subset=[target] + strom_vars + coral_vars + env_vars)\n",
        "sub_aic = _filter_no_strom(sub_aic)\n",
        "\n",
        "print(f\"  Common-subset N = {len(sub_aic)} rows\")\n",
        "\n",
        "models = {\n",
        "    'Strom+Coral': strom_vars + coral_vars,\n",
        "    'Strom-only':  strom_vars,\n",
        "    'Strom+Env':   strom_vars + env_vars,\n",
        "    'Full':        strom_vars + coral_vars + env_vars,\n",
        "    'Null':        [],\n",
        "    'Coral-only':  coral_vars,\n",
        "    'Env-only':    env_vars,\n",
        "    'Coral+Env':   coral_vars + env_vars,\n",
        "}\n",
        "\n",
        "aic_rows = []\n",
        "n = len(sub_aic)\n",
        "\n",
        "if n == 0:\n",
        "    print(\"  [SKIP] AICc: common-subset is empty.\")\n",
        "else:\n",
        "    y = sub_aic[target].values.astype(float)\n",
        "\n",
        "    for name, preds in models.items():\n",
        "        if len(preds) == 0:\n",
        "            X = np.ones((n, 1))\n",
        "        else:\n",
        "            X = sm.add_constant(sub_aic[preds], has_constant='add')\n",
        "            X = np.asarray(X, dtype=float)\n",
        "\n",
        "        k = X.shape[1]\n",
        "        if n <= k + 1:\n",
        "            print(f\"  [SKIP] {name}: too few rows for AICc (n={n}, k={k})\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            fit = sm.OLS(y, X).fit(method='qr')\n",
        "            aic = float(fit.aic)\n",
        "            aicc = aic + (2 * k * (k + 1)) / (n - k - 1)\n",
        "            aic_rows.append({\n",
        "                'Model': name,\n",
        "                'Predictors': str(preds),\n",
        "                'R2': float(fit.rsquared),\n",
        "                'Adj_R2': float(fit.rsquared_adj),\n",
        "                'AIC': aic,\n",
        "                'AICc': float(aicc),\n",
        "                'N': int(n),\n",
        "                'K': int(k)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"  [FAIL] {name}: fit failed -> {e}\")\n",
        "\n",
        "aic_df = pd.DataFrame(aic_rows)\n",
        "if aic_df.empty:\n",
        "    print(\"  [SKIP] AICc: no models could be fit.\")\n",
        "else:\n",
        "    aic_df = aic_df.sort_values('AICc')\n",
        "    min_aicc = aic_df['AICc'].min()\n",
        "    aic_df['Delta_AICc'] = aic_df['AICc'] - min_aicc\n",
        "    aic_df['Weight'] = np.exp(-0.5 * aic_df['Delta_AICc'])\n",
        "    aic_df['Weight'] = aic_df['Weight'] / aic_df['Weight'].sum()\n",
        "    aic_df.to_csv(OUTPUT_DIR / 'results_aic_improved.csv', index=False)\n",
        "    print(\"  -> Saved results_aic_improved.csv\")\n",
        "    display(aic_df)\n",
        "\n",
        "# ---------------------------\n",
        "# 8. LOWESS (keep: derived only now that basal is removed)\n",
        "# ---------------------------\n",
        "print(\"8. LOWESS fits (derived only; 1000 boot for plotting stability/speed)...\")\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "\n",
        "def get_lowess_ci(x, y, frac=0.6, n_boot=1000):\n",
        "    sort_idx = np.argsort(x)\n",
        "    x_sorted = x.iloc[sort_idx].values\n",
        "    y_sorted = y.iloc[sort_idx].values\n",
        "\n",
        "    z = lowess(y_sorted, x_sorted, frac=frac)\n",
        "    x_grid = z[:, 0]\n",
        "    y_fit = z[:, 1]\n",
        "\n",
        "    boot_curves = []\n",
        "    idx_all = np.arange(len(x))\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.choice(idx_all, size=len(idx_all), replace=True)\n",
        "        x_s = x.iloc[idx].values\n",
        "        y_s = y.iloc[idx].values\n",
        "        s_idx = np.argsort(x_s)\n",
        "        try:\n",
        "            z_b = lowess(y_s[s_idx], x_s[s_idx], frac=frac)\n",
        "            y_interp = np.interp(x_grid, z_b[:, 0], z_b[:, 1])\n",
        "            boot_curves.append(y_interp)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    boot_curves = np.array(boot_curves)\n",
        "    ci_low = np.nanpercentile(boot_curves, 2.5, axis=0)\n",
        "    ci_high = np.nanpercentile(boot_curves, 97.5, axis=0)\n",
        "    return pd.DataFrame({'x': x_grid, 'y_fit': y_fit, 'ci_low': ci_low, 'ci_high': ci_high})\n",
        "\n",
        "if 'derived_strom_prop' in df.columns:\n",
        "    v = df.dropna(subset=['derived_strom_prop', target]).copy()\n",
        "    v = _filter_no_strom(v)\n",
        "    low = get_lowess_ci(v['derived_strom_prop'], v[target])\n",
        "    low.to_csv(OUTPUT_DIR / 'results_lowess_derived.csv', index=False)\n",
        "    print(\"  -> Saved results_lowess_derived.csv\")\n",
        "\n",
        "# =============================================================================\n",
        "# LAG ANALYSIS (Max-|t| & |Cohen’s d|) — robust + saves outputs\n",
        "#   Outputs:\n",
        "#     - output/results_lag_summary_all_predictors.csv\n",
        "#     - output/results_lag_profile_all_predictors_long.csv\n",
        "#     - output/results_lag_profile_thickness.csv\n",
        "#   Also defines: peak_thick, peak_deriv (for segmented regression cell)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"./output\")\n",
        "OUTPUT_DIR = DATA_DIR\n",
        "\n",
        "# -------------------------\n",
        "# Ensure df exists\n",
        "# -------------------------\n",
        "if \"df\" not in globals() or df is None or not isinstance(df, pd.DataFrame) or df.empty:\n",
        "    df = pd.read_csv(DATA_DIR / \"MASTER_dataset_stage.csv\", encoding=\"utf-8-sig\")\n",
        "    print(f\"[INFO] Loaded df from MASTER_dataset_stage.csv: {len(df)} rows\")\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "target = \"thickness_mean\"\n",
        "AGE_MIN, AGE_MAX = 358.9, 485.4   # Ord–Dev window used previously\n",
        "MIN_BEFORE, MIN_AFTER = 3, 3\n",
        "\n",
        "# -------------------------\n",
        "# Build predictor list (use all_predictors if available; else reconstruct)\n",
        "# -------------------------\n",
        "if \"all_predictors\" in globals() and isinstance(all_predictors, (list, tuple)) and len(all_predictors) > 0:\n",
        "    predictors = [p for p in all_predictors if p in df.columns]\n",
        "else:\n",
        "    predictors = []\n",
        "    for p in [\n",
        "        \"derived_strom_prop\", \"basal_strom_prop\",\n",
        "        \"rugose_div\", \"tabulate_div\",\n",
        "        \"carbonate_area_km2\", \"temperature\", \"dissolved_O2\",\n",
        "        \"log_derived_basal_ratio\",\n",
        "        \"d13C\", \"atm_O2\", \"atm_CO2\", \"pO2\", \"pCO2\",\n",
        "        \"Labechiida_prop\", \"Clathrodictyida_prop\", \"Actinostromatida_prop\",\n",
        "        \"Stromatoporida_prop\", \"Stromatoporellida_prop\", \"Syringostromatida_prop\",\n",
        "        \"Amphiporida_prop\",\n",
        "    ]:\n",
        "        if p in df.columns:\n",
        "            predictors.append(p)\n",
        "\n",
        "# Ensure required columns exist\n",
        "needed_cols = [\"stage\", \"midpoint_ma\", \"strom_total_occ\", target]\n",
        "missing = [c for c in needed_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns for lag analysis: {missing}\")\n",
        "\n",
        "# -------------------------\n",
        "# Filter Ord–Dev + strom presence + required numeric\n",
        "# -------------------------\n",
        "work = df.copy()\n",
        "work[\"midpoint_ma\"] = pd.to_numeric(work[\"midpoint_ma\"], errors=\"coerce\")\n",
        "work[\"strom_total_occ\"] = pd.to_numeric(work[\"strom_total_occ\"], errors=\"coerce\")\n",
        "work[target] = pd.to_numeric(work[target], errors=\"coerce\")\n",
        "\n",
        "work = work.dropna(subset=[\"stage\", \"midpoint_ma\", \"strom_total_occ\", target]).copy()\n",
        "work = work[(work[\"midpoint_ma\"] >= AGE_MIN) & (work[\"midpoint_ma\"] <= AGE_MAX)].copy()\n",
        "work = work[work[\"strom_total_occ\"] > 0].copy()\n",
        "\n",
        "# Sort oldest->youngest (descending Ma)\n",
        "work = work.sort_values(\"midpoint_ma\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(f\"[INFO] Lag dataset after filters: n={len(work)} (Ord–Dev, strom_total_occ>0)\")\n",
        "\n",
        "def _cohen_d(before, after):\n",
        "    before = before[np.isfinite(before)]\n",
        "    after  = after[np.isfinite(after)]\n",
        "    if len(before) < 2 or len(after) < 2:\n",
        "        return np.nan\n",
        "    pooled_std = np.sqrt(((len(before)-1)*np.var(before, ddof=1) + (len(after)-1)*np.var(after, ddof=1)) /\n",
        "                         (len(before) + len(after) - 2))\n",
        "    if not np.isfinite(pooled_std) or pooled_std <= 0:\n",
        "        return np.nan\n",
        "    return (np.mean(after) - np.mean(before)) / pooled_std\n",
        "\n",
        "def lag_profile_for_var(df_sorted, var, min_before=3, min_after=3):\n",
        "    \"\"\"Return break profile rows for a single variable.\"\"\"\n",
        "    y = pd.to_numeric(df_sorted[var], errors=\"coerce\").values.astype(float)\n",
        "    ages = pd.to_numeric(df_sorted[\"midpoint_ma\"], errors=\"coerce\").values.astype(float)\n",
        "    stages = df_sorted[\"stage\"].astype(str).values\n",
        "\n",
        "    rows = []\n",
        "    n_all = len(df_sorted)\n",
        "    for i in range(min_before, n_all - min_after):\n",
        "        before = y[:i]\n",
        "        after  = y[i:]\n",
        "\n",
        "        before = before[np.isfinite(before)]\n",
        "        after  = after[np.isfinite(after)]\n",
        "\n",
        "        if len(before) >= 2 and len(after) >= 2:\n",
        "            t, p = stats.ttest_ind(before, after, equal_var=True, nan_policy=\"omit\")\n",
        "            d = _cohen_d(before, after)\n",
        "        else:\n",
        "            t, p, d = np.nan, np.nan, np.nan\n",
        "\n",
        "        rows.append({\n",
        "            \"Variable\": var,\n",
        "            \"stage\": stages[i],\n",
        "            \"age\": float(ages[i]) if np.isfinite(ages[i]) else np.nan,\n",
        "            \"n_before\": int(i),\n",
        "            \"n_after\": int(n_all - i),\n",
        "            \"t_abs\": float(np.abs(t)) if np.isfinite(t) else 0.0,\n",
        "            \"p\": float(p) if np.isfinite(p) else np.nan,\n",
        "            \"d_abs\": float(np.abs(d)) if np.isfinite(d) else 0.0\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# -------------------------\n",
        "# Compute thickness profile + peak\n",
        "# -------------------------\n",
        "prof_thick = lag_profile_for_var(work, target, MIN_BEFORE, MIN_AFTER)\n",
        "if prof_thick.empty:\n",
        "    raise RuntimeError(\"[ERROR] Thickness lag profile is empty (too few rows?)\")\n",
        "\n",
        "peak_thick = prof_thick.loc[prof_thick[\"t_abs\"].idxmax()].to_dict()\n",
        "\n",
        "# -------------------------\n",
        "# Compute predictor profiles + peaks\n",
        "# -------------------------\n",
        "profiles = [prof_thick]\n",
        "summary_rows = []\n",
        "\n",
        "peak_deriv = None  # composition changepoint\n",
        "\n",
        "for pred in predictors:\n",
        "    # skip if fully missing in this filtered dataset\n",
        "    if pred not in work.columns:\n",
        "        continue\n",
        "\n",
        "    prof = lag_profile_for_var(work, pred, MIN_BEFORE, MIN_AFTER)\n",
        "    if prof.empty:\n",
        "        continue\n",
        "\n",
        "    profiles.append(prof)\n",
        "\n",
        "    peak = prof.loc[prof[\"t_abs\"].idxmax()].to_dict()\n",
        "    lag_vs_thick = peak[\"age\"] - peak_thick[\"age\"] if np.isfinite(peak.get(\"age\", np.nan)) else np.nan\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"Predictor\": pred,\n",
        "        \"Peak_Age\": peak[\"age\"],\n",
        "        \"Peak_Stage\": peak[\"stage\"],\n",
        "        \"Max_t_abs\": peak[\"t_abs\"],\n",
        "        \"Max_d_abs\": peak[\"d_abs\"],\n",
        "        \"p_at_peak\": peak[\"p\"],\n",
        "        \"Lag_vs_Thickness_Myr\": lag_vs_thick,\n",
        "        \"Thickness_Peak_Age\": peak_thick[\"age\"],\n",
        "        \"Thickness_Peak_Stage\": peak_thick[\"stage\"],\n",
        "    })\n",
        "\n",
        "    if pred == \"derived_strom_prop\":\n",
        "        peak_deriv = peak\n",
        "\n",
        "# fallback for peak_deriv (so downstream cells won't break)\n",
        "if peak_deriv is None:\n",
        "    peak_deriv = {\"age\": np.nan, \"stage\": None, \"t_abs\": np.nan, \"d_abs\": np.nan}\n",
        "\n",
        "lag_summary_df = pd.DataFrame(summary_rows).sort_values(\"Max_t_abs\", ascending=False)\n",
        "lag_profiles_long = pd.concat(profiles, ignore_index=True)\n",
        "\n",
        "# -------------------------\n",
        "# Save\n",
        "# -------------------------\n",
        "(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "lag_summary_df.to_csv(OUTPUT_DIR / \"results_lag_summary_all_predictors.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "lag_profiles_long.to_csv(OUTPUT_DIR / \"results_lag_profile_all_predictors_long.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "prof_thick.to_csv(OUTPUT_DIR / \"results_lag_profile_thickness.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"\\n[OK] Saved lag outputs:\")\n",
        "print(\"  - output/results_lag_summary_all_predictors.csv\")\n",
        "print(\"  - output/results_lag_profile_all_predictors_long.csv\")\n",
        "print(\"  - output/results_lag_profile_thickness.csv\")\n",
        "\n",
        "print(f\"\\nChangepoint (thickness):   {peak_thick['age']:.2f} Ma ({peak_thick['stage']})\")\n",
        "if np.isfinite(peak_deriv.get(\"age\", np.nan)):\n",
        "    print(f\"Changepoint (composition): {peak_deriv['age']:.2f} Ma ({peak_deriv['stage']})\")\n",
        "else:\n",
        "    print(\"Changepoint (composition): NA (derived_strom_prop not available/insufficient)\")\n",
        "\n",
        "display(lag_summary_df.head(25))\n",
        "\n",
        "\n",
        "print(\"\\n✓ CELL 14 COMPLETE (NO BASAL).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "289ac04c"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# @title CELL 15: TIME-SERIES-ROBUST TESTS (Segmented regression + sampling control)\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.regression.linear_model import OLS, GLSAR\n",
        "from statsmodels.stats.sandwich_covariance import cov_hac\n",
        "\n",
        "def _as_1d_numeric(v):\n",
        "    \"\"\"Force Series/DataFrame column to 1-D numeric float array.\"\"\"\n",
        "    if isinstance(v, pd.DataFrame):\n",
        "        v = v.iloc[:, 0]\n",
        "    v = pd.to_numeric(v, errors='coerce')\n",
        "    return v.values.astype(float)\n",
        "\n",
        "def run_segmented_regression(df_in, target, changepoint):\n",
        "    \"\"\"Segmented (ITS) regression with OLS+HAC, WLS+HAC, and GLSAR\"\"\"\n",
        "    needed = ['midpoint_ma', target, 'derived_strom_prop', 'reef_count', 'strom_total_occ']\n",
        "    for c in needed:\n",
        "        if c not in df_in.columns:\n",
        "            print(f\"[WARN] Missing column: {c}\")\n",
        "            return []\n",
        "\n",
        "    v = df_in[needed].copy()\n",
        "\n",
        "    # Exclude no-strom rows\n",
        "    v = v[v['strom_total_occ'].notna() & (v['strom_total_occ'] > 0)].copy()\n",
        "\n",
        "    # Force numeric\n",
        "    v['midpoint_ma'] = pd.to_numeric(v['midpoint_ma'], errors='coerce')\n",
        "    v[target] = pd.to_numeric(v[target], errors='coerce')\n",
        "    v['derived_strom_prop'] = pd.to_numeric(v['derived_strom_prop'], errors='coerce')\n",
        "    v['strom_total_occ'] = pd.to_numeric(v['strom_total_occ'], errors='coerce')\n",
        "\n",
        "    v = v.dropna(subset=['midpoint_ma', target, 'derived_strom_prop', 'strom_total_occ'])\n",
        "    v = v.sort_values('midpoint_ma', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    if len(v) < 10:\n",
        "        print(\"[WARN] Too few observations after filtering:\", len(v))\n",
        "        return []\n",
        "\n",
        "    t = _as_1d_numeric(v['midpoint_ma'])\n",
        "    y = _as_1d_numeric(v[target])\n",
        "\n",
        "    I_post = (t <= changepoint).astype(float)\n",
        "    t_post = np.maximum(0, changepoint - t)\n",
        "\n",
        "    x_derived = _as_1d_numeric(v['derived_strom_prop'])\n",
        "    s_intensity = np.log1p(_as_1d_numeric(v['strom_total_occ']))\n",
        "\n",
        "    X = np.column_stack([np.ones(len(t)), t, I_post, t_post, x_derived, s_intensity])\n",
        "    col_names = ['const', 'time', 'step', 'post_slope', 'derived_prop', 'sampling']\n",
        "\n",
        "    results = []\n",
        "    n = len(y)\n",
        "    maxlags = max(1, int(np.floor(4 * (n/100)**(2/9))))  # Newey–West\n",
        "\n",
        "    # OLS + HAC\n",
        "    try:\n",
        "        m = OLS(y, X).fit()\n",
        "        hac_cov = cov_hac(m, nlags=maxlags)\n",
        "        hac_se = np.sqrt(np.diag(hac_cov))\n",
        "        hac_t = m.params / hac_se\n",
        "        hac_p = 2 * (1 - stats.t.cdf(np.abs(hac_t), df=n - X.shape[1]))\n",
        "        for i, name in enumerate(col_names):\n",
        "            results.append({'Model': 'OLS+HAC', 'Target': target, 'Changepoint_Ma': changepoint,\n",
        "                            'Term': name, 'Coef': m.params[i], 'SE': hac_se[i], 'P': hac_p[i],\n",
        "                            'N': n, 'R2': m.rsquared})\n",
        "    except Exception as e:\n",
        "        print(\"OLS+HAC failed:\", e)\n",
        "\n",
        "    # WLS + HAC\n",
        "    try:\n",
        "        weights = 1.0 / np.log1p(_as_1d_numeric(v['strom_total_occ']) + 1.0)\n",
        "        m = sm.WLS(y, X, weights=weights).fit()\n",
        "        hac_cov = cov_hac(m, nlags=maxlags)\n",
        "        hac_se = np.sqrt(np.diag(hac_cov))\n",
        "        hac_t = m.params / hac_se\n",
        "        hac_p = 2 * (1 - stats.t.cdf(np.abs(hac_t), df=n - X.shape[1]))\n",
        "        for i, name in enumerate(col_names):\n",
        "            results.append({'Model': 'WLS+HAC', 'Target': target, 'Changepoint_Ma': changepoint,\n",
        "                            'Term': name, 'Coef': m.params[i], 'SE': hac_se[i], 'P': hac_p[i],\n",
        "                            'N': n, 'R2': m.rsquared})\n",
        "    except Exception as e:\n",
        "        print(\"WLS+HAC failed:\", e)\n",
        "\n",
        "    # GLSAR(AR1)\n",
        "    try:\n",
        "        ar_m = GLSAR(y, X, rho=1)\n",
        "        ar_fit = ar_m.iterative_fit(maxiter=20)\n",
        "        for i, name in enumerate(col_names):\n",
        "            results.append({'Model': 'GLSAR', 'Target': target, 'Changepoint_Ma': changepoint,\n",
        "                            'Term': name, 'Coef': ar_fit.params[i], 'SE': ar_fit.bse[i],\n",
        "                            'P': ar_fit.pvalues[i], 'N': n, 'R2': ar_fit.rsquared})\n",
        "    except Exception as e:\n",
        "        print(\"GLSAR failed:\", e)\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------\n",
        "# RUN (use changepoints from Cell 13)\n",
        "# -------------------------\n",
        "CP_thick = np.nan\n",
        "CP_comp  = np.nan\n",
        "\n",
        "if 'peak_thick' in globals():\n",
        "    if isinstance(peak_thick, dict) and 'age' in peak_thick:\n",
        "        CP_thick = float(peak_thick['age'])\n",
        "    elif hasattr(peak_thick, '__getitem__') and 'age' in peak_thick:\n",
        "        CP_thick = float(peak_thick['age'])\n",
        "\n",
        "if 'peak_deriv' in globals():\n",
        "    if isinstance(peak_deriv, dict) and 'age' in peak_deriv:\n",
        "        CP_comp = float(peak_deriv['age'])\n",
        "    elif hasattr(peak_deriv, '__getitem__') and 'age' in peak_deriv:\n",
        "        CP_comp = float(peak_deriv['age'])\n",
        "\n",
        "# fallbacks if missing\n",
        "if not np.isfinite(CP_thick): CP_thick = 426.5\n",
        "if not np.isfinite(CP_comp):  CP_comp  = 431.9\n",
        "\n",
        "print(f\"Running segmented regression at thickness CP = {CP_thick:.1f} Ma...\")\n",
        "seg_thick = pd.DataFrame(run_segmented_regression(df, 'thickness_mean', changepoint=CP_thick))\n",
        "\n",
        "print(f\"Running segmented regression at composition CP = {CP_comp:.1f} Ma...\")\n",
        "seg_comp  = pd.DataFrame(run_segmented_regression(df, 'thickness_mean', changepoint=CP_comp))\n",
        "\n",
        "seg_df = pd.concat([seg_thick.assign(CP_type='Thickness'),\n",
        "                    seg_comp.assign(CP_type='Composition')], ignore_index=True)\n",
        "\n",
        "if 'OUTPUT_DIR' in globals() and not seg_df.empty:\n",
        "    seg_df.to_csv(f'{OUTPUT_DIR}/results_segmented_regression.csv', index=False, encoding='utf-8-sig')\n",
        "    print(f\"Saved: {OUTPUT_DIR}/results_segmented_regression.csv\")\n",
        "\n",
        "display(seg_df if not seg_df.empty else pd.DataFrame())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vAOSwErWMeuP"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "#@title CELL 16: ZIP everything in ./output and download the zip (Colab-safe)\n",
        "# ============================================================\n",
        "from pathlib import Path\n",
        "import zipfile, datetime, os\n",
        "\n",
        "OUTPUT_DIR = Path(\"./output\")\n",
        "if not OUTPUT_DIR.exists():\n",
        "    raise FileNotFoundError(f\"OUTPUT_DIR not found: {OUTPUT_DIR.resolve()}\")\n",
        "\n",
        "# Make a timestamped zip name\n",
        "ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "zip_path = Path(f\"output_{ts}.zip\")\n",
        "\n",
        "# Collect files\n",
        "files_to_zip = [p for p in OUTPUT_DIR.rglob(\"*\") if p.is_file()]\n",
        "if len(files_to_zip) == 0:\n",
        "    print(f\"[WARN] No files found under: {OUTPUT_DIR.resolve()}\")\n",
        "else:\n",
        "    # Create zip\n",
        "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "        for fp in files_to_zip:\n",
        "            # store relative to OUTPUT_DIR (so zip has a clean structure)\n",
        "            zf.write(fp, arcname=fp.relative_to(OUTPUT_DIR))\n",
        "    print(f\"[OK] Created zip: {zip_path.resolve()}  ({len(files_to_zip)} files)\")\n",
        "\n",
        "    # Download (Colab) or show link (Jupyter)\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(str(zip_path))\n",
        "    except Exception:\n",
        "        try:\n",
        "            from IPython.display import FileLink, display\n",
        "            display(FileLink(str(zip_path)))\n",
        "        except Exception:\n",
        "            print(f\"Download not auto-supported here. Zip is at: {zip_path.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bHOenpJCS-_"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}